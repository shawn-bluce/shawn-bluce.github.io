[{"categories":null,"content":"0X00 Intro 现在是 2025 年末，所有人都在聊 AI，不管是一线科研人员还是开发者甚至是楼下卖瓜的大叔。但是在我的观察中，即便是很多开发者也其实对 AI 的了解也比较肤浅，仅限于使用成品的豆包、Deepseek 等产品，对时下热门的技术关键词并不了解。所以我自己重新整理了一下，假设目标用户是不懂什么是 LLM、Agent、MCP、Function Call 的我自己，写了一篇介绍文章。远远算不上由浅入深，但勉强可以做一个纯粹的介绍。 0X01 LLM 首先 LLM 的意思是大语言模型（Large Language Model），重点是语言和模型。所以我们搞清楚一个问题就是很多玩家自己部署的 Stable Diffusion 并非 LLM，因为它并不是语言模型。另外我们常用的 Deepseek 和豆包 App 也并非 LLM，因为它们并非语言模型。 最常被人们接触到的其实都不是模型，而是在模型上构建的产品，例如：ChatGPT 和豆包。模型指的是给这些产品提供 AI 能力的诸如 GPT-5.2、Claude Sonnet 4.5、GLM 4.7 等。以 OpenAI 为例，我们在使用 ChatGPT 的 App 时能调整的参数极其有限，但是如果通过 API 来直接调用模型能力，那可以修改的参数会多达几十上百个，并且可以区分 System Prompts 和 User Prompts。 总的来说 LLM 是本次 AI 浪潮中最重要也是最基石的存在。 0X02 Function Call Function Call 的概念其实非常直观，本质上就是一个函数调用接口。我们在写代码时定义的函数通常是给程序内部逻辑调用的，而 Function Call 允许我们将这些函数注册给 LLM，让模型根据对话上下文自主决定是否触发某个函数以及传什么参数。 不过在早期的工程实践中，虽然各家模型都支持 Function Call，但痛点非常明显：定义格式不统一。比如 OpenAI 用的是特定的 JSON Schema，其他模型可能又有各自的规范。这就导致你想写一个通用的“查天气工具”，需要针对每个模型单独适配一套接口，开发成本高，工具难以在不同模型间复用。 0X03 MCP MCP (Model Context Protocol) 的出现就是为了终结这种“方言”乱象。它是由 Anthropic（开发 Claude 系列模型的公司） 提出的一个开放标准协议，统一规定了 AI 应用与外部数据源、工具之间如何交互。 只要模型端和工具端都遵循 MCP 协议，它们就能无缝对接。简单类比，以前各家是各自搞一套私有 RPC 协议，现在 MCP 就像是 AI 界的 HTTP 或者 RESTful API 标准一样。它解决了连接器的标准化问题，让一个工具可以轻松地在不同的 AI 客户端（如 Claude Desktop、IDE 插件）中即插即用。 0X04 Agent Agent 就是现在比较流行的智能体，它的本质是一个程序。我自己听过的说法包括但不限于： “Agent 和大模型不一样，Agent 能直接操作你的电脑” “Agent 比大模型厉害多了，肯定是大语言模型的替代品” “Agent 就是个骗人的噱头，没什么用，泡沫迟早要爆炸” 其实都不太对劲，因为 Agent 是一个至少集成了 LLM 和 MCP/Function Call 的集合体程序。如果逐一反驳的话，Agent 确实和大模型不一样毕竟你不能说汽车和发动机一样，但 Agent 能否操作电脑也是看它具体集成了哪些 MCP/Function Call；其次 Agent 比大模型厉害就更是无稽之谈了，因为 LLM 是 Agent 的一部分，总不能说汽车比发动机强吧；说 Agent 是骗人的噱头的其实也不至于，Agent 只是 LLM 发展的一个方向而已。 最近我正在使用的 Claude Code 其实就是一个 Agent，给没有用过的朋友简单介绍一下 Claude Code 的使用流程 在目录中打开 Claude Code（是一个 CLI 工具），发送：“检查这个代码仓库，并向我介绍它”。 此时 Claude Code 会理解我的意图，然后调用系统的 find、cat、tree 等命令尽可能的获取这个项目的信息，比如目录结构、入口位置甚至是 README.md。在它获取到信息之后会继续传输给 LLM，让 LLM 来理解并输出内容。如果我继续下发指令：“为这个项目新增一个 XX 功能”，那 Claude Code 会继续先把我的内容发送给 LLM 然后列出一个 TODO List，再根据 todo 的内容一项一项执行，中间就会涉及到创建编辑文件、执行测试代码、甚至是安装依赖的三方库等。 ","date":"2025-12-27","objectID":"/posts/2025-ai-primer/:0:0","tags":["LLM","AI","Agent","MCP"],"title":"2025 开发者 AI 扫盲：从 LLM 到 Agent","uri":"/posts/2025-ai-primer/"},{"categories":null,"content":"0X00 Intro 今年年初的时候，写过一篇名为如何高质量地接入AI的文章，现在十个月过去了，经过这一年的探索和总结，现在我已经找到了一个非常适合开发者的 AI 接入方案（至少非常适合我自己），现在打算在这里跟大家做一个简单的分享。 我个人使用这些 LLM 的经历主要是分成三个阶段： 最开始是 2023 年，注册了一个 ChatGPT 的账户，就只用 GPT-3.5 这种模型。不过那段时间 LLM 的能力还比较有限，使用量也并不大，很多时候都是以玩的心态在用； 后面随着模型能力上升，需求也跟着上升了，索性订阅了 POE，每个月要花掉 $20。不过当时 GPT-o1 和 Claude 3.5 这些模型还是给我工作学习明显提供了帮助的 POE 的年度订阅过期之后我试图找到一个比 POE 更合适我自己的技术方案，也就是下面将会介绍的这个了 0X01 Struct 当时在探索方案的时候我的需求就是：按量计费、多平台统一、多模型可用、高度定制化、显得高级（满足虚荣心 🤣）。那么最终得到的就是下面这样一个结构。 先来简单介绍一下这个方案的优势： 按量计费：因为是用 API 接入的，所以是纯粹的按量计费； 多平台统一：Open WebUI 毕竟是 web 应用，并且做了自适应布局，所以在电脑和手机上的使用体验都是不错的； 多模型可用：任何 LLM 服务商都提供了 API 接入方式，所以不管是接入国内的 GLM、Deepseek 甚至是前几天开放的小米模型，还是直连 OpenAI 和 Anthropic 都是没问题的； 高度定制化：Open WebUI 的定制化程度很高，很适合我们这种所谓的“高级用户”； 0X02 Open WebUI Open WebUI 是一个用户友好的 WebUI，开源版本可以实现接入多个 API 上游、多用户管理（意味着你可以把你的服务通过独立账号的方式分享给你的亲朋好友们，假设你的 API 用量顶得住的话）。 有关这个项目的简单使用方法就不多说了，大家有两分钟就摸索清楚了。这里提一个自己觉得有用的 tips：首先是分组，它并不是单纯的 Group 功能，更像是创建了一个 Assistant，是可以在分组里写 System Prompts 的，写好之后在这个分组里的每次对话都会默认使用你设置好的 System Prompts 了。比如我有一个 MySQL 的分组，里面设置的 System Prompts 就是下面这样的。如果你多次就一个话题和 AI 展开探讨的话，就可以用这种方式。 你是一名精通 MySQL 的数据库专家。用户是一名中级 MySQL 用户，掌握基本的 CRUD 操作、DDL 语句和基础性能优化知识，现在遇到技术问题向你请教。 ## 核心原则 1. **确保准确性**：提供事实正确的信息。当涉及版本特性、存储引擎行为、锁机制细节等不确定时，明确说明不确定性，不要猜测。 2. **专业表达**： - 使用精确的数据库术语（InnoDB、MVCC、redo log、buffer pool 等） - 避免生活化比喻和类比 - 假定用户理解基础概念（索引、事务、范式、执行计划等） 3. **程序员思维**： - 逻辑清晰、结构化组织回答 - 提供可验证的 SQL 语句或配置示例 - 说明性能影响、锁竞争、数据一致性等关键权衡 - 必要时引用官方文档或 EXPLAIN 分析结果 4. **简明原则**： - 简单查询问题给出简洁 SQL 示例 - 性能或架构问题进行深入分析 - 避免不必要的铺垫或重复问题 ## 回答规范 - SQL 优化问题：基于执行计划分析，指出索引使用、扫描方式、join 策略等关键点 - 索引设计：说明覆盖索引、联合索引顺序、索引选择性等考量因素 - 事务与锁：明确隔离级别、锁类型、死锁场景 - 架构设计：讨论分库分表、主从复制、读写分离的适用场景和代价 - 版本差异：明确指出 MySQL 5.7 vs 8.0 等版本间的行为差异 ## 避免事项 - 不解释 SELECT、JOIN、INDEX 等基础概念，除非被明确询问 - 不使用\"数据库就像仓库...\"等类比 - 不添加鼓励性内容 - 不对基本语法提供逐步教学式指导 - 不在已知版本时给出过于宽泛的\"取决于版本\"回答 如果你打算使用 Open WebUI 的话，有两个设置需要注意，一个是需要给 Nginx 的反向代理设置上 websocket 相关的转发配置；另一个是需要传入两个环境变量（我下面示例配置里的 CHAT_STREAM_RESPONSE_CHUNK_MAX_BUFFER_SIZE 和 REPLACE_IMAGE_URLS_IN_CHAT_RESPONSE），否则在使用类似 nano banana 的模型生图时图片会返不回来。 我自己的 Nginx 反向代理参考配置 server { listen 80; server_name \u003cINPUT_YOUR_DOMAIN\u003e; return 301 https://$server_name$request_uri; } server { listen 443 ssl http2; server_name ai.just666.com; ssl_certificate \u003cYOUR_SSL_CONFIG\u003e; ssl_certificate_key \u003cYOUR_SSL_CONFIG\u003e; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA384; ssl_prefer_server_ciphers off; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; proxy_buffer_size 128k; proxy_buffers 4 256k; proxy_busy_buffers_size 256k; client_max_body_size 50M; location / { proxy_pass http://127.0.0.1:3005; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; # WebSocket proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_buffering off; proxy_request_buffering off; proxy_connect_timeout 30s; proxy_send_timeout 30s; proxy_read_timeout 30s; } add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; add_header X-XSS-Protection \"1; mode=block\"; add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always; } 我自己的 docker compose 参考配置，里面的变量写在和 docker-compose.yml 同级的名为 .env 的文件中 services: open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui restart: unless-stopped ports: - \"3005:8080\" volumes: - open-webui-data:/app/backend/data environment: # 关闭默认的 Ollama 连接（如果不需要本地模型） - ENABLE_OLLAMA_API=false - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-your-secret-key-change-this} - ENABLE_SIGNUP=${ENABLE_SIGNUP:-true} - DEFAULT_USER_ROLE=${DEFAULT_USER_ROLE:-user} - AIOHTTP_CLIENT_TIMEOUT=300 - CHAT_STREAM_RESPONSE_CHUNK_MAX_BUFFER_SIZE=10485760 - REPLACE_IMAGE_URLS_IN_CHAT_RESPONSE=true - TIMEOUT=600 - AIOHTTP_CONNECTOR_LIMIT=100 - AIOHTTP_CONNECTOR_LIMIT_PER_HOST=10 volumes: open-webui-data: driver: local 0X03 OpenRouter 现在 WebUI 已经准备好了，只需要简单配置","date":"2025-12-21","objectID":"/posts/how-to-integrate-ai-for-developer/:0:0","tags":["LLM","AI"],"title":"如何高质量的接入AI - Developer","uri":"/posts/how-to-integrate-ai-for-developer/"},{"categories":null,"content":"0X00 前言 为什么要写这么一篇文章？按理说大家工作了一段时间之后肯定有一个最基础的认知：md5 不是加密，base64 也不是。起码我最开始以为大家应该都知道，但是后面发现有些人对这两个东西的理解真的有很大的问题。所以才打算写这么一篇文章。 事情的起因是这样的： 甲：“你用 base64 加密一下” 我：“base64 没有加密效果” 甲：“你给 base64 前面加上 32 个随机字符，让他解不出来不就行了” 我：“啊？？？” 所以我接下来打算首先介绍一下 base64 究竟是怎么得到的（算法本身），再来说明白 base64 为什么不可以通过前面加 32 个随机字符来实现“加密”（随机长度也不行）。 0X01 常识性的知识 首先我们明确一个事情，就是「编码」「加密」「哈希」之间的区别： 名称 是否可逆 是否安全 典型 编码 是 否 base64 加密 是 是 AES 哈希 否 不总是 md5 编码，是将一段内容通过某种算法直接转换成另一段内容，只是形式发生了变化，在明确编码算法的情况下是可以轻松解码获得原始内容的 加密，是将一段内容通过某种算法和 密码 将明文加密成密文，内容本身发生了变化，在明确加密算法的情况下没有密码也是无法解密的 哈希，是将一段内容通过某种算法转换成固定长度的另一段内容，原始数据会丢失，理论上无论如何都不可能通过哈希后的数据反推出原文 0X02 base64 编码 在明确了什么是编码后，来自顶向下拆解一下这个 base64 编码吧。首先名字中的 64 指的就是编码后的每一位都有 64 种有效字符，也就是 a-z 的 26 个小写字母、A-Z 的 26 个大写字母、0-9 的 10 个数字和 +/ 两个特殊字符，加一起正好是 64。 显然 64 指的是 6bit 能容纳的最大长度了，所以 base64 的编码逻辑就是：把一个二进制的数据按 6bit 一组分成 N 组，得到 N 个 0-63 之间的数，然后根据下面这张表把每组的数字替换成字符。 如果我的原文是二进制的 01101000 01100101 01101100 01101100 01101111（这是 UTF-8 编码的 hello），那么可以拆分成 011010、000110、010101、101100、011011、000110、1111，进而对应的是26、6、21、44、27、6、15，再对照上面的表格得到的就是 aGVsbGP。这样一来，我们的 hello 字符串就顺利转成 base64 编码了。。。吗？ 并没有，因为最后一位的 1111 其实并不是严格对照上述表格查到的，所以我们应该给其尾部补二进制零，将其补充成 111100 对应索引 60，查表得到字符 8，将 P 替换得到 aGVsbG8。这样一来，我们的 hello 字符串就顺利转成 base64 编码了。。。吗？ 并没有，我们发现得到的编码后字符串长度是 7，并不能按 4byte 一组进行拆分，所以最后还需要补充一个 =，最后得到的 base64 编码是 aGVsbG8=。这样一来，我们的 hello 字符串就顺利转成 base64 编码了。。。吗？ 是的！不过为什么不能按 4byte 一组进行拆分就得在后面补 = 呢？因为解码器在后续解码的时候是按照编码后的字符串 4 个一组，将其转成 24bit 再分成 3 个 8bit 恢复成原始数据，所以在最后需要用 = 将 base64 的结果串补到 4 的整数倍长度。 有些不严谨的 base64 解码器在后面没有 = 的时候也可能解得开，不过这是解码器的问题，不是标准的问题，就不过多探讨了。 0X02 为什么不能靠前缀加密 如果你比较敏感，可能发现了 base64 的一个特征，它的编码过程是分段进行的，也就意味着如果一个 base64 编码的前半段出现了错乱，是不影响它后半段的正确解码的。再加上 base64 的特征十分明显，明显指数应该是和 md5 坐同一桌的，所以很多技术人员是可以一眼看出这段字符串是 base64 编码的。 那么即使你在一段 base64 编码的前面追加了一段随机的内容，例如：zxclkjgheiqlourytaGVsbG8=，那我们现在尝试“破解”一下 先用在线解码工具尝试解码，发现失败了 发现长度是 25，不是 4 的整数倍，那就在最前面追加 3 个 x 让它变成 xxxzxclkjgheiqlourytaGVsbG8= 再次尝试解码，就已经看到最后的 hello 了 所以说，在 base64 编码得到的串前面追加所谓的随机字符串，是没有什么加密效果可言的 ","date":"2025-12-15","objectID":"/posts/base64-basic/:0:0","tags":["base64","encoding"],"title":"Base64 究竟是什么","uri":"/posts/base64-basic/"},{"categories":null,"content":"0X00 开头 最近买了本书，叫做《100个Go语言典型错误》，发现这样的总结很有意思。决定自己也写一个，不过以我的水平写本书还是有点离谱了，但是写一篇博客还是没什么问题的，所以就有了这篇文章。 0X01 函数默认值传递空列表 这是一个典型错误，很多很多的 Python 程序员都犯过这个错误。一般在定义一个函数且给它设置默认值的时候我们都会写成 def foo(a=0, b=\"\", c=None) 这个样子，这种写法是完全没有问题的。但是有时候也会写成：def foo(d=[])，这就完犊子了～ 我们看下面这段代码 def foo(a=[]): a.append('x') print(a) for i in range(10): foo() 你以为会输出 10 个 ['x']？那就大错特错了，真正的输出是这样的： ['x'] ['x', 'x'] ['x', 'x', 'x'] ['x', 'x', 'x', 'x'] ['x', 'x', 'x', 'x', 'x'] ['x', 'x', 'x', 'x', 'x', 'x'] ['x', 'x', 'x', 'x', 'x', 'x', 'x'] ['x', 'x', 'x', 'x', 'x', 'x', 'x', 'x'] ['x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x'] ['x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x'] 为什么呢？因为函数在定义的时候就将空列表初始化了，并且记录下了引用，以后每次都会对同一个列表进行操作。另外不只是列表，字典作为默认值也会有这个问题的。 0X02 对闭包的无意识理解 很多人对闭包的理解就是粗浅的「函数里定义函数」，当然这是没错的，但是缺少了一些东西。如果只是函数里定义函数的话，下面这段代码会发生什么？ def foo(): x = 2 def bar(): print(x) return bar foo()() 没错，这段代码是会正确输出 2 的，因为闭包还有一个特性就是内部函数能访问外部函数中的变量，即使外部函数已经运行完毕了。 0X03 使用不合适的变量名 这一点虽然很简单，但确实很容易出现，比如用 list / dict / str 做变量名。因为 Python 中这 3 个名字是非常非常常用的type/function，所以万万不可用。 0X04 字符串拼接的性能问题 少量字符串拼接使用 + 当然没有任何问题，但是如果量很大的话就不建议使用 + 了，使用 join 会快非常多。 import time COUNT = 10000000 # 使用 + 的版本 start_time = time.time() result1 = '' for i in range(COUNT): result1 += 'x' end_time = time.time() plus_time = end_time - start_time # 使用 join 的版本 start_time = time.time() char_list = [] for i in range(COUNT): char_list.append('x') result2 = ''.join(char_list) end_time = time.time() join_time = end_time - start_time print(f\"+ 方法耗时: {plus_time:.4f}秒\") print(f\"join 方法耗时: {join_time:.4f}秒\") 在我的电脑上运行是这样的： + 方法耗时: 1.0472秒 join 方法耗时: 0.5272秒 0X05 错误使用字符串的 strip 方法 学过 Python 的肯定都用过字符串的 strip 方法或者它衍生出来的 lstrip 和 rstrip，比如我们会用 'hello'.rstrip('o')来去掉字符串右侧的 o。 也有些人会用下面这种方法去掉额外的字符： phone_num = \"+8613588888888\" phone_num = phone_num.strip(\"+86\") 因为 strip('o') 可以去掉 o，就自然而然以为 strip('+86') 就是去掉 +86 了。但你实际运行起来就会发现 phone_num 就只剩下 135 了。因为 strip('+86')的意思并不是去掉两侧的 +86 而是去掉两侧的 +/8/6，凑巧这个手机号后面全是 8 所以就全没了 🤷‍♂️ 0X06 认为 Python 的多线程没有用 这是一个典型谣言了，很多人一旦听说 Python 有 GIL 之后就大张旗鼓的说：“Python 的多线程屁用没有”。但实际上真的是这样吗？并不是。 Python 的多线程如果用在 CPU 密集型的计算任务上，那确实没什么用；但是如果用在 IO 密集型的任务上，那和真正意义上的多线程是没有显著差别的。所以严谨来说 Python 确实没有真正意义上的多线程，但也不能说 Python 的多线程没有用。 0X07 过分的一行流 有些 Python 程序员推崇精简代码，使用各种列表生成式、字典生成式之类的，这当然是没问题，精简又好看，而且这是非常 Pythonic 的写法。但是有些人有些过分，强行把多行代码挤成一行，或者强行使用 Python 的高级语法。 比如说 a = {v: k for k, v in my_dic.items()} 这种代码，其实挺好的，但是有些人会写这种东西： result = {f\"item_{i}\": [x*2 for x in [y+1 for y in range(3)] if x in [z for z in range(10) if z % 2 == 0]] for i in range(3)} 强吗？强，对语言没点理解是写不出来能跑的这种代码的。但是好吗？codereview 的时候可能会被同时打死。 0X08 手撸 csv 文件 是的没错，都 2025 年了还有人在手撸 csv 文件。赶紧了解一下 csv 库的用法吧，不要再去折磨那个逗号和转义符了。 写入： import csv # 学生信息数据 students = [ ['姓名', '年龄', '班级', '成绩'], ['张三', 18, '高一(1)班', 85], ['李四', 17, '高一(2)班', 92], ['王五', 18, '高一(1)班', 78], ['赵六', 17, '高一(3)班', 88] ] # 写入CSV文件 with open('students.csv', 'w', newline='', encoding='utf-8') as file: writer = csv.writer(file) writer.writerows(students) print(\"学生信息已写入 students.csv 文件\") 读取： import csv # 读取CSV文件 with open('students.csv', 'r', encoding='utf-8') as file: reader = csv.reader(file) print(\"学生信息列表：\") for row in reader: print(f\"{row[0]:\u003c8} {row[1]:\u003c6} {row[2]:\u003c12} {row[3]}\") # 也可以使用字典方式读取 print(\"\\n使用字典方式读取：\") with open('students.csv', 'r', encoding='utf-8') as file: reader = csv.DictReader(file) for row in reader: print(f\"姓名: {row['姓名']}, 年龄: {row['年龄']}, 班级: {row['班级']}, 成绩: {row['成绩']}\") 0X09 文件硬读写 我们有时候会用 open('xxx', 'r').read() 直接把一个文件读到内存里，如果文件比较小确实是可以这样干的，读到内存里直接当字符串处理就好。但是如果文件很大，就不要一直这么搞了，你 1G 的日志文件还通过这种方式读取，服务都要被卡死了。 with open('xxx.log', 'r') as f: for line in f: pass 这样我们的 f 就是一个类似生成器的东西了，每次只读一行，就不会再被这么初级的 IO 问题限制性能了。 0X0A 认为 Python 字典是无序的 是的兄弟，2025年了，Python 的字典早就不是无序的了，甚至是 7 年前的 Python 3.7 发布的时候 dict 就不是无序的了。 Python 官方文档贴在这里： Changed in version 3.7: Dictionary order is guaranteed to be insertion order. This behavior was an implementation detail of CPython from 3.6. ","date":"2025-06-20","objectID":"/posts/python-10-errors/:0:0","tags":["Python"],"title":"编写 Python 程序的 10 个典型错误","uri":"/posts/python-10-errors/"},{"categories":null,"content":"0X00 正文 前几天尝试用自己的服务器通过 docker 启一个新服务，没注意这个服务需要的内存比较大（我的服务器只有 2G 内存），导致 docker compose up -d 命令敲下去不到一分钟服务器直接卡死了…… 当时还没意识到是内存不足导致的问题，以为是 CPU 卡住了，毕竟一般情况下服务刚启动的时候确实会占用比较多的资源。所以直接摆烂，等它一会儿好了。半个小时之后发现服务器还是卡死状态，ssh 也上不去，所以干脆去云平台重启。发现云平台的监控上已经看不到CPU/内存的占用率了，意味着装在服务器里的 agent 也已经完蛋了。而且，重启服务器也没有什么效果。 好在云平台可以通过类似 IPMI 的方式直接接入到服务器的 console，所以可以通过 console 登陆。虽然每一步的操作都非常非常卡，敲一串命令要等很久才能回显。使用 free -h 检查了内存占用情况之后发现确实是内存瓶颈，并且系统没有配置交换分区，所以才出了下面的这个骚招： 使用 free -h 确认当前内存使用情况，发现内存爆满且没有交换分区 意识到 Linux 所谓的「万物皆文件」，决定在没有空闲分区的情况下生造一个交换分区出来 使用 dd if=/dev/zero of=SWAP_FILE bs=4k count=$((256*1024*4)) 创建一个 4G 大小的块文件 使用 mkswap SWAP_FILE 将其格式化成 swap 分区 使用 swapon SWAP_FILE 挂载交换分区 系统活过来了 ⚠️ 注意：使用 swapon 只是临时挂载，重启会失效，所以需要通过常规手段增加内存（至少要增加交换分区） dd if=/dev/zero of=SWAP_FILE bs=4k count=$((256*1024*4)) 是什么意思？ dd 命令是一个硬盘层面的拷贝命令 if=/dev/zero of=SWAP_FILE 指的是 input file 是 /dev/zero（这是一个会源源不断输出二进制 0 的设备），output file 是 SWAP_FILE bs=4k count=$((256*1024*1024)) 指的是 block size 为 4k，count 为 256 * 1024 * 1024，乘到一起就是 4G ","date":"2025-04-15","objectID":"/posts/simple-linux-memory-rescue/:0:0","tags":["Linux"],"title":"一种简单的 Linux 内存不足抢救方案","uri":"/posts/simple-linux-memory-rescue/"},{"categories":null,"content":"0X00 叠甲 开局先叠甲，我到现在为止用过一台 Mac mini，一台 Intel 的 MacBook Pro 和一台 M2 Max 的 MacBook Pro，自费购买 MacBook 花费超过 3W 元，累计使用超过 5 年，是个不折不扣的 macOS 用户。我认可很多 MacBook 和 macOS 的设计理念，如果我现在只能保留一台电脑，那极大可能我会选择一台 MacBook Pro。 😡 好了现在开始输出 😡 0X01 难用的包管理器 首先就是这个 homebrew，我在买 MacBook 之前看攻略的时候就看到很多人在吹 homebrew 有多好用，有多厉害，有多方便。他们的说辞一般是：一个命令就可以更新系统里很多的组件、一个命令就可以安装好开发环境、一个命令就可以XXXX。当时我作为一个 Archlinux 用户是有些懵逼的，但是看在这么多人吹捧的份上我也就相信了，心想着「这么多人都这样推荐，应该确实会很好用，如果能像 pacman 或者 apt 或者 dnf 这样的话那确实是很好的」。 结果呢？这个 homebrew 不光是个第三方工具，而且镜像源的数量也显著少于各个 Linux 发型版本。好不容易把它配置好之后，发现 brew update/upgrade 速度都明显比 pacman/apt/dnf 慢，而且仓库里的工具也显著少于常见的 Linux 发型版本。 我自己的体验（结合我自己的开发工作和环境）来说，Archlinux 使用的 pacman 属于独领风骚的， apt 和 dnf 则处在第二梯队，属于好用的，brew 则处于第三梯队，属于能用的。但确实不至于被拿出来吹。 如果所有吹 homebrew 的人都是在拿 macOS 和 Windows 对比的话，那我无话可说。homebrew 虽然不怎么好用，但是确实比 Windows 上没有包管理器来的更好。 0X02 不区分大小写的文件系统 下面这段是我在 Linux 中的实验结果： [shawn@archlinux ~]$ mkdir test_dir [shawn@archlinux ~]$ touch test_dir/hello_world [shawn@archlinux ~]$ touch test_dir/hello_WORLD [shawn@archlinux ~]$ touch test_dir/HELLO_world [shawn@archlinux ~]$ touch test_dir/HELLO_WORLD [shawn@archlinux ~]$ ls -l test_dir/ total 0 -rw-r--r-- 1 shawn shawn 0 Mar 6 14:27 HELLO_WORLD -rw-r--r-- 1 shawn shawn 0 Mar 6 14:27 HELLO_world -rw-r--r-- 1 shawn shawn 0 Mar 6 14:27 hello_WORLD -rw-r--r-- 1 shawn shawn 0 Mar 6 14:27 hello_world [shawn@archlinux ~]$ 再下面这段是我在 macOS 中实验的结果： [shawn@mac] ~ $ mkdir test_dir [shawn@mac] ~ $ touch test_dir/hello_world [shawn@mac] ~ $ touch test_dir/hello_WORLD [shawn@mac] ~ $ touch test_dir/HELLO_world [shawn@mac] ~ $ touch test_dir/HELLO_WORLD [shawn@mac] ~ $ ls -l test_dir/ .rw-r--r-- 0 shawn 6 Mar 14:28 hello_world [shawn@mac] ~ $ 是不是很惊喜，是不是很意外？嘴上说自己是 Unix，但是真正 Unix 会使用的 ext4 和 XFS 甚至是 ZFS 等文件系统都是严格区分大小写的（ZFS 可配置，默认区分大小写），结果到了 macOS 上就不分了。也就是说如果你在 Linux 上将 hello.py 和 HELLO.py 两个文件打包，再到 macOS 中解压，就会原地消失一个 🫠 👇下面给大家表演一个「文件消失术」 [shawn@mac] ~ $ ssh shawn@192.168.81.151 Last login: Wed Mar 5 14:30:49 2025 from 192.168.81.1 [shawn@archlinux ~]$ mkdir test_dir [shawn@archlinux ~]$ touch test_dir/hello_world [shawn@archlinux ~]$ touch test_dir/hello_WORLD [shawn@archlinux ~]$ touch test_dir/HELLO_world [shawn@archlinux ~]$ touch test_dir/HELLO_WORLD [shawn@archlinux ~]$ tar zcvf test_dir.tgz test_dir/ test_dir/ test_dir/hello_WORLD test_dir/HELLO_WORLD test_dir/HELLO_world test_dir/hello_world [shawn@archlinux ~]$ logout Connection to 192.168.81.151 closed. [shawn@mac] ~ $ scp shawn@192.168.81.151://home/shawn/test_dir.tgz . test_dir.tgz 100% 210 215.9KB/s 00:00 [shawn@mac] ~ $ tar zxvf test_dir.tgz x test_dir/ x test_dir/hello_WORLD x test_dir/HELLO_WORLD x test_dir/HELLO_world x test_dir/hello_world [shawn@mac] ~ $ ls -l test_dir/ .rw-r--r-- 0 shawn 6 Mar 16:00 hello_world [shawn@mac] ~ $ 0X03 随地大小便 说到 macOS 的随地大小便问题，哪怕不是 macOS 用户应该也有很多见过 .DS_Store 这个文件的吧。这个文件简单来说是 macOS 的文件管理器也就是 Finder 来创建的，文件中主要保存的就是当前目录的排序方式、展示方式等内容。如果你直接对这个目录进行打包（无论是在 GUI 还是 CLI 中），就会把这个文件直接包进去。如果是到 Windows 上解压，就会莫名其妙多出来一个 .DS_Store 文件；如果是在 Linux 上解压就更恶心了，它存在但默认又看不见 🙈 如果涉及到 git 仓库管理那就更愚蠢了，这个文件会随着日常使用经常产生变化，而且在多级目录下会出现大量的 .DS_Store 文件，只能通过 .gitignore 过滤掉，否则就会一直跟着你的代码库各种变动。 刚刚尝试在公司的代码库里 find . -name .DS_Store | wc -l 搜了一下，发现了整整 100 个 .DS_Store 🤷‍♀️ 0X04 功能缺失，三方工具大行其道 macOS 其实有很多常用功能并不完善，所以诞生了很多小工具来专门给苹果擦屁股（其实 iOS 也一样）。下面举几个例子 ","date":"2025-03-05","objectID":"/posts/macos-bad-apple/:0:0","tags":["macOS"],"title":"苹果里的虫子：macOS 的几个臭毛病","uri":"/posts/macos-bad-apple/"},{"categories":null,"content":"Finder Finder 是 macOS 中的文件管理器，我之前用了那么多年的 Windows，也用了几年的 Linux（Gnome、KDE、Xfce 都长期用过），从来没想过安装一个第三方文件管理器，因为他们都足够好用。只有到了 macOS 上之后，没多久我就开始探索第三方的文件管理器，先后尝试过 QSpace 和 ForkLift。为什么呢？当然就是单纯的 Finder 不好用啊 👎 ","date":"2025-03-05","objectID":"/posts/macos-bad-apple/:1:0","tags":["macOS"],"title":"苹果里的虫子：macOS 的几个臭毛病","uri":"/posts/macos-bad-apple/"},{"categories":null,"content":"Topbar 用过 Windows 的肯定都知道，打开的程序如果可以后台驻守的话会放在任务栏右侧，还可以通过系统设置让每个程序从固定显示、固定隐藏、条件显示三个状态里选一个。macOS 呢？完全没有，只要驻守在后台的程序全都在 Topbar 的右侧堆积着，你有三两个程序还好，十个八个的话就直接占据了一半的 Topbar，丑的很。重点是很多应用其实使用频率并不高，或者说都是通过快捷键调用的，从来不会在 Topbar 上去点，结果却要永久性占据一个位置。 而且说到这个就来气，新的 MacBook 给搞了个刘海，笔记本电脑，搞了个刘海，我不理解。最气的是这个狗屁刘海只有硬件知道，软件是不知道的。这意味着什么？意味着 macOS 并不知道你看不到刘海底下的内容，也就意味着当你的程序开的多的时候就会被挤到刘海里，然后你看不到它，看不到也就点不到。厉害吧，这就是优雅的 Apple 🤷‍♀️ 那有什么办法可以让他们不长期驻守在那儿吗？比如说像 Windows 一样折叠起来？有的，你去搜一下就会看到很多人在推荐一个软件叫做 Bartender 的软件，它专职做这个工作。你开开心心把它下载下来发现：这个软件要 TMD 卖 $20，整整 TMD 20 dollars ！！！ 这时候就有果粉要说了：「你不知道别乱说，明明还有开源免费的 hidden bar 可以用呢」。啊确实，我就在用这个（反正不可能让我花 $20 隐藏一个图标） ，但是这东西不是苹果官方应该做的吗？？？ ","date":"2025-03-05","objectID":"/posts/macos-bad-apple/:2:0","tags":["macOS"],"title":"苹果里的虫子：macOS 的几个臭毛病","uri":"/posts/macos-bad-apple/"},{"categories":null,"content":"Terminal 接下来是 Terminal，很多果粉在说 「macOS 是最适合程序员的系统」，那么我作为一个程序员就只配用这么烂的一个 Terminal 吗？Linux 桌面环境的 GNOME Terminal 或者 Konsole 和 Xfce Treminal 都比 macOS 官方提供的好用太多了。 不过好在 Terminal 没有太多收费的，很多都是开源项目，我这里推荐几个自己用过且好用的吧： Kitty Terminal 高性能，使用配置文件，功能完备 Alacritty 高性能，使用配置文件，极简 iTerm 2 传统、功能完备、性能说得过去 0X05 万恶的 Command + Q Windows 上切换不同窗口的快捷键是 Alt + Tab Linux 上切换不同窗口的快捷键是 Alt + Tab macOS 上切换窗口的快捷键是 Command + Tab 看似非常和谐（macOS 的 Command 其实就是 Windows 的 Alt），但是 macOS 中有一个类似于 Windows 中 Alt + F4 的强制退出快捷键：Command + Q。熟悉键盘键位的小伙伴应该已经发现了，Q 和 Tab 是 TMD 挨着的！是 TMD 挨着的！哪怕是你再熟悉键盘，盲打再怎么熟练，也有可能手抖按错一个键吧，那么极有可能出现你想切换窗口时直接把当前窗口 Kill 掉的情况。 我不理解为什么要把「关闭窗口」 和「切换窗口」两个快捷键搞的这么近，难道产品经理（或者说是乔布斯？）自己从来没有误触过吗？别的快捷键误触也就算了，但是把切换窗口误触成关闭窗口后果可是有点严重的啊。 我不信哪个长期用 macOS 的没出现过切换窗口不小心把窗口关掉的情况，绝对不可能 ❌ 0X06 源自心底的傲慢 我们都知道在文件管理器的「网络」目录里可以看到同一个局域网内的其他设备，那么我们来看一下 macOS 是怎么表现「其他」设备的。 没看清？放大看一下 是的你没看错，就是一个老式 CRT 显示器甚至还有些发黄了，运行着一个蓝屏了的 Windows 系统。那么 macOS 是如何展示自己的设备呢？ 是的，不仅区分了 Mac mini，MacBook 和 iMac，甚至区分了不同的 MacBook，每一个都是那么的现代且美观。苹果你真的有必要这样吗？ ","date":"2025-03-05","objectID":"/posts/macos-bad-apple/:3:0","tags":["macOS"],"title":"苹果里的虫子：macOS 的几个臭毛病","uri":"/posts/macos-bad-apple/"},{"categories":null,"content":"0X00 没有什么意义但我就是想写的前言 有一说一现在的大模型发展太快了，最开始我列这个大纲的时候是把 deepseek 作为「凑合能用但超级便宜」的一个国产替代品来介绍的，没想到过了个年它直接翻身了，现在甚至能打 GPT-4o。所以我决定现在立刻马上把这篇文章写完，否则没准又杀出来个什么模型，会导致我永远写不完了 🤣 这篇文章的主要受众群体如下： 知道最近 AI 很火，想用用看但是不知道怎么上手的朋友 尝试用过一些大模型，但是觉得接入不方便或者价格高的朋友 想全面接入 AI，让 AI 成为自己得力助手的朋友 这篇文章会介绍这些内容： 成品工具：只需要一个浏览器或者一个 APP 即可访问的最大众化的 AI 接入模式 模型选择：列举常见模型的特点，根据你的需求选择合适的大模型 API 接入：不方便接入原生 Claude/OpenAI API 时的优秀替代品 三方工具：通过 API 接入后能大幅改善日常使用体验的一些工具 本地部署：有关本地部署的一些个人看法 那么废话少说，现在开始正文了 0X01 商业化成品工具 想要最快速的接入 AI 能力，那首选的自然是现成的商业化的 2C 产品了，所以我这里收集了几个我自己用过的比较好用的的商业化成品工具。 注意这部分只讨论产品，不讨论模型，有关模型的讨论放在下面一个段落。 ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:0:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"ChatGPT 首先，自然是目前大模型的领军人物 OpenAI 了，它的 ChatGPT 目前应该是全球范围内最知名的 AI 工具了。目前 ChatGPT 提供了多种模型可选，也提供了多模态模型和联网搜索功能，部分高级模型需要付费使用，费用一般是 $20/mon。 在 PC/Mac/Android 上使用 ChatGPT 需要有正确的上网姿势，在 iPhone 上还需要一个海外 Apple ID。并且注册起来也比较麻烦。如果切实需要的话可以在网上自行搜索一下注册教程。 访问方式：web、app、PC/Mac 客户端 \u0001优势：APP 使用体验极佳、够聪明、多模态； 劣势：访问难度高、价格贵； 综合推荐等级：★★★★☆ ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:1:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"Claude Claude 是追着 ChatGPT 打的另一个海外巨头，个人认为综合能力上略次于 ChatGPT 但不明显。不过同样的注册、访问都会有些麻烦。但是有一个上下文超长且比较便宜的模型（Claude-3.5-sonset-200k），可以让他阅读超长的 pdf、 MS office 等文档。同样的提供了更高级的付费模型，价格为 $20/mon。 访问方式：web、app、PC/Mac 客户端 优势：聪明、多模态、比 ChatGPT 访问轻松一些； 劣势：访问难度依然较高、价格贵； 综合推荐等级：★★★☆☆ ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:2:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"豆包 豆包是字节跳动旗下的 AI 工具，目前我亲测下来说实话聪明程度明显弱于 ChatGPT/Claude，但是它强就强在免费和体验优良。首先它内置了很多个不同 prompt 的「人」，可以作为你的助手，这一点对于不会写 prompt 的新手来说很是实用。手机的 APP 还能通过语音对谈的方式和豆包交流，虽然 ChatGPT 也有这个功能，但是在国内使用豆包反应更快并且口语听起来舒服很多很多。macOS 上的客户端可以启用一些插件功能，实现划选文字进行翻译、解释等功能。 访问方式：web、app、PC/Mac 客户端 优势：多模态、接入容易、语音体验良好、内置 prompt、免费； 劣势：不够聪明、进阶使用体验较差； 综合推荐等级：★★★★☆ 建议所有人在手机上装一个豆包，毕竟是免费的，有事没事跟他硬聊几句都还是划算的。而且它也能联网，所以快速搜索一些确定有结果的问题也很好用。 ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:3:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"deepseek deepseek 这几天可真是爆了（以至于我一直在用的超快的 API 突然变慢了 😮‍💨）。简单介绍一下 deepseek 也是国产的，他们最擅长使用超低的成本做相同的事，最近刷屏的新闻也正是他们用了 GPT-o1 大概 2% 的成本训练出来了几乎相同水平的 DeepSeek-R1 模型。目前来说 deepseek 的重心还是在技术上，所以他们的客户端/web依旧停留在「能用」的水平线上。 访问方式：web、app 优势：免费、多模态、接入容易、聪明； 劣势：应用层做的不够好、使用人数激增导致响应变慢； 综合推荐等级：★★★★☆ ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:4:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"kimi kimi 是比较早火起来的国产 AI，当时的主要卖点是联网搜索。 访问方式：web、app 优势：免费、多模态、接入容易、联网搜索能力强、app 体验好； 劣势：不如 DeepSeek-R1 聪明； 综合推荐等级：★★★☆☆ ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:5:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"POE POE：我们不生产模型，我们只是大模型的搬运工 POE 是一个集成了大量模型第三方应用，在这里可以付一份钱同时使用几乎所有的热门模型。例如你想同时使用 ChatGPT 和 Claude 的高级模型，觉得同时买两个会员太贵了，就可以同样花 $20 购买 POE 的会员，这样一来就可以同时访问他们了。但需要注意的是，POE 的逻辑是会员每月送你 1000000 「点数」，每次对话会根据模型的不同消耗不同的点数（一般都是够用的，我比较高强度使用都是够的）。这也是我目前唯一付了年费的 AI 订阅项。 访问方式：web、app、PC/Mac 客户端 优势：访问所有热门模型（包括文生图模型） 劣势：付费、使用次数有限（尽管限制很宽松） 综合推荐等级：★★★★☆ ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:6:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"综合 综合看下来，我可以做出下面的推荐： 如果你只是想用最简单的方式体验一下AI，并且希望它足够简单易用，那就选豆包 如果你想使用目前最强的免费 AI 模型，同时愿意学习 AI 的用法，那就选 deepseek 如果你想接入世界先进的 AI 模型，那就考虑订阅 ChatGPT（ChatGPT 如果不订阅的话不如直接用 deepseek） 如果你想体验更多热门模型，想要了解不同模型的区别和擅长的方向，也想要深入学习使用大模型，那建议订阅 POE 0X02 通过 API 接入 上面我们聊的都是点击即用的成熟的商业化产品，现在来讨论一下通过 API 来接入这些模型。 提示：如果你听不懂 API、base_url、API KEY、HTTP Method、这几个词就意味着你暂时不适合下面的内容，继续阅读下去可能会感到有些蒙圈，这是正常现象 🤣 首先我来介绍一下为什么我们需要通过 API 接入大模型，有如下几个优势： 价格：例如 ChatGPT 的价格是 $20 每月，如果你只是每天对话个十次八次的，直接开通 Plus 会显得很亏，因为 ChatGPT Plus 是按月付费的。但是 API 则是按量付费，计费方式从每月固定额度变成了根据交互的 tokens 数量计算，在用量不大的情况下会更便宜； 性能：虽然没有直接证据，但是我体感上各家的 2C 产品使用的服务器和 2B 的 API 使用的服务器并非同一组，手机 APP 访问已经在卡了但是通过 API 访问就还是比较快； 定制：通过 API 访问可以自己调整更多的参数，例如影响记忆力和费用的 max_tokens 和影响输出的 temperature 等； 体验：使用 API 访问即使只是使用最基础的对话功能，我们也有多个前端工具可以选择，不会像官方的客户端一样没有半点定制化空间； 扩展：我们可以将 API KEY 配置到很多支持的工具上，让工具们获得 AI 的能力加持（例如 obsidian、Firefox/Chrome 等）； ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:7:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"模型选择 目前通过 API 接入的话我个人只推荐 deepseek、GPT、Claude 这三个。下面是每个家族的代表和他们的能力（主观评价）和价格的对比。 名称 能力（5分制） 上下文 输入价格（每百万 Tokens） 输出价格（每百万 Tokens） 备注 DeepSeek V3 3 64k $0.27 $1.10 价格无敌，配置在浏览器扩展上用于翻译和总结非常合适 DeepSeek R1 4.5 64k $0.55 2.19 性价比无敌，能力媲美贵它将近 30 倍的 GPT-o1 GPT-4o 3.5 128k $2.5 $10 -– GPT-o1 4.5 200k $15 $60 -– Claude-3.5-sonset 4 200k $3 $15 -– 目前我自己的体验下来，推荐如下： DeepSeek V3 价格最便宜，充 10 块钱可以用很久。适合将它配置到一些支持 AI 的工具上，用来总结文章、大段翻译等。虽然它不够聪明但是足够便宜，随便调用也不用心疼自己的钱包； DeepSeek R1 目前使用体验良好，足够聪明价格也是很便宜。如果不嫌弃它每次都要思考半天的话（其实这是它的优势项），可以作为主力 AI 模型使用，它理论上更擅长数学、编程等逻辑性强的工作； GPT-4o 是传统模型中很强的了，没有 o1 和 R1 的推理过程，反应比较迅速。价格虽然比较贵但多少能承受，也是主力 AI 模型的一个优秀备选； GPT-o1 应该是这些模型里理论最强的，但是这个价格嘛也很离谱。如果你在意它比 R1 强的那一点能力且不在意这 30 倍的价差，那 GPT-o1 是个不错的选择； Claude-3.5-sonset 最大的优势是 200k 的上下文，是目前热门模型中上下文窗口最大的一个。如果你经常有大上下文的需求那么 Claude-3.5-sonset 是一个优秀的选择； 有关 tokens 和上下文： 我们在和大模型交互的时候，我们自己说的文字和大模型返回的文字都会被分词，然后将分词之后的结果作为 token 计算。例如我给模型提供一个 10 万个汉字的文档，让他回答我的一个问题，AI 的回答大概是 1000 个汉字的话，大约会消耗掉 10 万个 tokens。 上下文则是我们和模型对话时 AI 能处理的最大 tokens 数量。上面的例子一次对话就需要消耗 10 万个 tokens 也就是 100k 左右，也就意味着 DeepSeek V3/R1 都无法处理这个对话请求，但是 GPT-4o/o1 和 Claude-3.5-sonset 都能处理。 ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:8:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"如何接入 众所周知，接入 AWS 的 API 就需要 AWS 的账号，但是现在的 AI 接入方式多少有些不一样的地方。当然通过 OpenAI 官方接入 OpenAI 的模型是天经地义的，但是我们国内用户光是注册 OpenAI 和 Claude 的账号就已经很费劲了，后面付费还有一座大山拦路，着实是整不动。所以在官方接入的传统方式之外还可以通过转发站点实现 API 接入，这种接入方式不仅注册和付费更轻松，还会有些许优惠。这里介绍两个站点：头顶冒火 和 302.ai，这两个平台类似，都是类似于前面介绍的 POE 的平台，只不过这两个平台提供的是聚合的 API 服务，在这一处充值后能通过 API 访问平台支持的模型。 如果你想通过 API 的方式接入 DeepSeek 的话就不需要再用上面的服务中转了，直接去官网注册账号然后支付宝微信付款用就行了。 另外有个好消息，目前已知的绝大多数大模型的 API 都兼容 OpenAI 的 API 标准，也就意味着绝大多数能使用 OpenAI API 的工具都能通过修改 base_ur, API KEY, model 这三个参数的方式快速接入。 0X03 第三方工具 使用第三发工具的前提是通过 API 访问大模型，个人认为这才是当前大模型的完整使用方式。下面介绍几个自己真正用过的第三方工具： ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:9:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"glarity 这是一个浏览器扩展，配置好 API 之后它会有一个悬浮按钮在浏览器里，可以一键快速调用大模型对当前页面进行总结，也可以针对当前页面进行提问，还可以做到在尽力维持页面布局的情况下实现逐行翻译。强烈推荐所有可以通过 API 访问大模型的朋友安装试用。 ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:10:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"ChatGPT-Next-Web 这是一个简单的聊天工具，不管是 DeepSeek 还是 ChatGPT 他们都有自己的页面，但是通过 API 访问的话总不能就在命令行里用吧。所以 ChatGPT-Next-Web 就出现了，它就是一个可以创建多个对话也能上传文件的前端工具，给它配好 API 就能像普通客户端一样使用 API 了。值得表扬的是这个开源项目可以部署在服务器上，通过浏览器访问，效果非常棒。也就意味着你可以自己开通 API 后把它部署在公网服务器上，配置一个密码，就能让自己和家人朋友一起用了。 ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:11:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"Obsidian 如果你也用 Obsidian 的话可以给你推荐一个名为 Text Generator 的插件，这个插件配置好之后可以在 Obsidian 中调用 API。可以实现的功能包括但不限于：选中大段文字让他检查错误、总结整篇笔记、生成某某的介绍等。虽然不如 Notion 的 Notion AI 集成度那么高，但是体验也还是不错的。 0X04 本地化部署 最后再聊一聊本地部署吧，最开始的时候我尝试过在 MacBook 上部署 Llama 3.1 的 7B 和 14B 模型测试。最近 DeepSeek R1 的突然爆火也开始出现了大量的本地部署的教程，给人一种在本地部署之后就不需要联网使用官方服务的感觉。 我自己测试的硬件配置是一台 M2 Max 32G 的 MacBook Pro，按理说跑大模型的性能应该是超出大多数朋友的电脑的。就以最近我测试的 DeepSeek R1 为例，简单测试的结果如下： 模型 效果 速度 DeepSeek R1 7B 一般 飞快 DeepSeek R1 14B 一半 飞快 DeepSeek R1 32B 可以非严肃环境使用 比较快 DeepSeek R1 70B 跑不动 跑不动 并且即使我跑 32B 这样的模型，效果也并不够看。所以我自己的个人看法就是： 如果你只能跑 7B 或者 14B 的模型，那可以用来学习、体验、图一乐 如果你能跑 32B 的模型，那可以用来正经聊聊，但是能力依旧有限，不适合严肃环境使用 如果你能跑 70B 的模型，那……我没试过，不知道能力怎么样 🤣 如果你能跑 671B 的模型，那请你联系我，我想和土豪做朋友 如果要在本地部署的话，目前比较推荐的就只有 DeepSeek 和 Llama 了，并且在模型选择上基本 32B 就到头了，如果是自己本地跑跑玩玩的话建议从 7B 和 14B 中进行选择，最多最多试试 70B（如果是 Mac 的融合内存的话）。 最后悄悄说一下，本地部署的话可以了解一下带有 abliterated 标签的模型，这才是本地部署模型最大的意义。 ","date":"2025-02-06","objectID":"/posts/how-to-integrate-ai/:12:0","tags":["AI","LLM"],"title":"如何高质量地接入 AI","uri":"/posts/how-to-integrate-ai/"},{"categories":null,"content":"0X00 碎碎念 数数看这已经是我的第六篇年度总结了，每当回过头去看之前的总结就感觉写这些也还挺有意义的。虽然写日记对我来说太困难了，但是写「年记」好像还行，甚至有点乐在其中 🤔 廖凡_饺子_姜文.mp4 说起来小学时候大家都不喜欢家庭作业里有日记这一项，因为要写好多字，而且都没什么可写的。我就不一样了，因为我小学时候的日记全文都是：今天去XXX玩了，很高兴/今天去XXX家写作业，很开心。 🤣 回到正题。今年的生活完全被打乱了，比疫情还在的时候更乱，以至于我第一次 delay 了我的年度总结。哦不是，是完全没有去检查年初制定的计划到现在完成了多少（估计有个 30% 就谢天谢地了），具体是什么情况导致的我后面再说吧。这次给博客列的大纲就是零零散散不成体系，只能说尽力拼凑了一个年度总结出来，不管是长度还是质量肯定是不及去年和前年的万字长文了。那么就直接开始吧。 0X01 专业能力 今年本来的计划是更多的去深入学习之前已经掌握的知识和技能，结果完全没深入哪里去。反倒是之前两年多次「下定决心」要学的 Go 在今年突然用在工作上了，感觉之前自己吭哧吭哧看了半天不如真正工作上用起来一个星期提升来得快。 也正是这一点，让我对学习专业技能上有了一个新的想法：暂时用不上的知识和技能当然要学，只是不用那么纠结自己到底明白了多少（反正用不上的话也很快就忘记了），只需要做到「我知道有什么，我知道是什么」就行了，等真用到了再去根据自己的那一点点基础去学就好了。反正我的 Docker 和 MySQL 这些都是这样学来的。 比如 Docker，最开始学的时候是在学校里，完全没有使用 docker 的需求，以至于甚至搞不清楚 docker 和虚拟机有什么区别，也不理解为什么 ubuntu/fedora 这些我常用的发行版本里面连个 vim 都没有，systemctl 也没法用。但好在最基础的用法（尤其是在工作环境里）你并不需要从零去完整的学一整个东西，都是在别人的基础之上进行操作的。 ⚠️ 并不是说专业能力不需要从头学起，只是说最开始的阶段没必要死扣那些细节，先用起来就行。但是如果你都用了很久 docker 了，还是说不出 docker 和虚拟机的核心区别，还是不清楚为什么 ubuntu 镜像里不带 systemd 和文本编辑器，那就不应该了。 0X02 出游 ","date":"2025-01-17","objectID":"/posts/2024-summary/:0:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"广州、佛山 这次本质上是一次出差，但是解决的确实一个非常简单的问题，不过又由于各种情况导致在佛山停留了一周之久。正好又赶上周末，所以可以算是半个旅行了。也正是这次过去出差也才知道，原来广州的地铁可以直接通到佛山的 🤯。 作为一个博物馆爱好者，到了省会城市肯定会往省博跑，但是令我没想到的是诺大一个广东省博却没什么好看的东西 🥲。里里外外传下来都没看到什么让人「哇塞」、「卧槽」、「牛逼」的东西，反倒是看到了一个让我觉得「啊？」的东西：镇馆之宝—白切鸡 🤣。反倒是广州的「南越王博物馆」更有意思，推荐同样的博物馆爱好者过去看一看。 广东省博-白切鸡 虽然没去佛山博物馆，但是去了一趟祖庙看了黄飞鸿纪念馆和叶问堂。如果对那段人文历史比较感兴趣的朋友，可以去看一看，黄飞鸿纪念堂有高质量的舞狮表演，叶问堂有叶问用过的木人桩和八斩刀展示。 佛山-舞狮叶问-八斩刀 另外广东的吃的确实是不错，尤其是牛肉。印象很深的是去吃的一家潮汕牛肉。店门口有一个小黑板专门写了今天的牛肉是下午几点钟开始杀，几点钟送到店里的。肉质非常新鲜，配合清汤的锅底和各种酱料，感觉自己能吃好几盘 😋 你说广州塔？那肯定是去了的，但是水平有限导致并不能拍出别人随手拍的大片，只能起到一个纪念作用。 广州塔 ","date":"2025-01-17","objectID":"/posts/2024-summary/:1:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"兰州 兰州是因为去参加一对好朋友的婚礼（还兼职摄影摄像），所以时间比较紧张，没去玩很多地方。但是 兰州的羊肉是真的好吃！！！ 玩的话就只去了水墨丹霞，地方不大但是好在去机场顺路，据说可以作为七彩丹霞的平替。 铜奔马水墨丹霞水墨丹霞 有一说一，这场婚礼是我目前为止参与程度最最最高的一场，从前一天晚上的场地探查到第二天一早的新娘化妆，再到接新娘，上婚车，一直到最后吃的核心席。还真是第一次如此高强度满满当当的参加一场婚礼。说到这儿再次祝他们百年好合 🎉 ","date":"2025-01-17","objectID":"/posts/2024-summary/:2:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"雅安 雅安离成都很近，这次是蹭了朋友的车过去的。当时找了一个在山上的观景台民宿（小木屋），过去放松了两天，感觉还不错。住宿环境和吃的都比较一般，但是推开房门就能看到非常漂亮的山景湖景，作为乏味生活的一点点调剂还是让人觉得豁然开朗。 雅安某湖 ","date":"2025-01-17","objectID":"/posts/2024-summary/:3:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"芒市 去云南芒市的旅行完全是因为当时超级烦躁，就在携程上看特价机票，发现到芒市的往返机票两个人才不到两千，就直接冲了，反而是这次纯纯瞎跑的旅行却成了今年最舒服的一次。 芒市大金塔 我们就只去了芒市的金塔和银塔，然后剩下的时间就在小城里乱走乱逛，累了就回民宿躺着，体验还是不错。吃东西印象最深的就是打车去「芒杏村」吃的烧烤了（果然哪里的烧烤都比成都好吃），价格优惠味道很棒。 如果有朋友也有兴趣去芒市的话，可以有两个小建议：金塔银塔距离很近，步行几分钟就能到，但是要分开收费（每张门票 40 元）。其中金塔重佛教，银塔重拍照，但是想要拍金塔的话反而是在银塔景区里更好拍。 芒杏村烧烤 另外一个建议就是「一定要去芒杏村吃烧烤」，虽然只吃了一家，但是滴滴师傅说周围的水平都差不多，而且非常好吃价格又便宜，强烈推荐。 ","date":"2025-01-17","objectID":"/posts/2024-summary/:4:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"米仓山 米仓山是单纯去看彩林的，虽然没有隔壁光雾山知名但是风景却一点都不差。另外如果有成都朋友想去的话可以先坐高铁到汉中，然后再转车到米仓山。虽然米仓山在巴中，但是高铁到巴中再去米仓山反而耗时更久价格更贵。 米仓山 一个小攻略是可以提前一天晚上到米仓山景区门口，找地方住宿然后赶在景区开门的第一时间冲进去，这样玩的会舒服一些。而且景区门口吃饭的地方也很多，不用担心没有地方搞吃的。另外米仓山景区里有🚁直升机，只要 350 块！！！ 0X03 年度游戏 让我们恭喜 宇宙机器人 获得 TGA 年度游戏大奖！！ 开个玩笑，宇宙机器人确实是个好游戏，但是它在另外几个作品面前真的不配拿奖啊。 ","date":"2025-01-17","objectID":"/posts/2024-summary/:5:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"黑神话：悟空🐒 这一切要始于 2020 年 8 月 20 日，那天我到公司听说 B 站有一个国产游戏的预告片，当时没太当回事。后面看了之后汗毛炸起，但是突然就冷静下来了，心想：不可能的，这绝对是诈骗，国内能做出这种游戏就见鬼了。但实际上也正是这一刻开始，就期待了起来，有哪个中国游戏玩家不想在游戏里扮演大圣呢？ 在此之后沉寂了很久，游科每到过年和 820 就都会放出一段新的预告宣传，随着时间推移心里的想法也早就发生了变化。从 2020 年的「我不相信能做出来」到 2024 年的「就算是一坨💩我也要尝尝咸淡」。 直到今年夏天，一觉醒来看到手机的 PS APP 提醒我愿望单里的《黑神话：悟空》可以预购了，我一下就清醒了，从看到消息到豪华版预购成功没有一秒钟的思考。 黑神话悟空 提前把假请好，前一天晚上把沙发整理好，给 PS5 手柄充上电，把饮料放在冰箱里，做好一切准备。第二天一大早起来关窗帘、拿饮料、调大音响音量，坐在电视前面拿好手柄眼巴巴看着 PS5 上面的倒计时，然后第一时间进入游戏。巧的是，当天朋友圈里好多人都跟我一样「遇到了不可抗力」导致没办法上班 🤷‍♀️ 从早上十点游戏解禁，一直到晚上十二点打完虎先锋睡觉，中间几乎是没有任何休息时间。如果要用一句话来描述这个游戏，那只能是：「这是我二十年游戏生涯里，做梦都想玩到的游戏 」 ","date":"2025-01-17","objectID":"/posts/2024-summary/:6:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"小丑牌🤡 小丑牌是我玩过最好玩的卡牌游戏，推荐所有对卡牌或者 rouge like 游戏有兴趣的朋友上手！（不想说太多，三两句又说不清楚，所以干脆不说了。但是为了表示这个游戏真的很好玩，所以专门给他保留一个位置） ","date":"2025-01-17","objectID":"/posts/2024-summary/:7:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"跑车浪漫旅 7（GT7）🚗 为什么没有直接说 GT 7 而是特地强调了《跑车浪漫旅》的名字呢？因为这个名字翻译的真的太好了～在玩 GT7 之前我一度以为极限竞速地平线是最优秀的赛车游戏，不过现在起码在我心中「最佳赛车游戏」的头衔要易主了。 您的浏览器不支持 HTML5 视频标签。 游戏的驾驶手感超级棒，结合 PS5 的手柄让每一次油门刹车都倍感真实，甚至每次吃到路肩的时候是具体哪边吃了多少都能通过手柄精细化的震动感受到。制作人山内一典对车辆建模的变态追求让我在游戏过程中甚至区分不出真假（有 PS VR2 的加成），虽然车外的风景画质一般，但在驾驶高速赛车的时候也很难注意到风景就是了。 以前我觉得赛车游戏就应该像极品飞车那样，爽快刺激；后来玩了极限竞速地平线之后觉得最佳的赛车游戏应该去除哪些过于花哨的东西，还是需要刹车和走线的，而不是乱撞和漂移；现在我已经开始觉得赛车游戏就应该用方向盘视角，就应该注意刹车点了。（坏了，我不会要入坑赛车模拟器了吧） 0X04 电影 这几年的电影都是些什么歪瓜裂枣…… 我以前晚上无聊的时候，时长掏出手机看看院线信息，然后晚上九点十点跑出去看个电影再回家。最近两年每次掏出手机看院线信息之后，都是叹口气然后关掉，真的是太烂了。 曾经～我茫然前行～暗夜的路上～ 今年看过的华语电影只有两个说得过去的：《周处除三害》和《好东西》。前者其实只是一个合格的普通片子，但是之前大陆很少上这种暴力电影，这次就趁这机会去看了。后者则是一部女性视角的片子，但不是那种无脑打拳的电影，还是值得一看的。唯一问题就是里面的小孩儿，经常冒出来几句没有三四十年生活经验都总结不出来的金句，有点离谱了。 0X05 运动 今年的运动之有前半年在坚持，后半年直接摆烂了。不过好在有两个里程碑式的运动成就：游泳算是学会了（蛙泳熟练，自由泳能游），自行车也骑了一个 170km 的小长途。 明年在运动上要多多投入时间了，计划的是徒步、跑步、骑车这三项，但是详细的年度计划还没列，等过几天确定了再看吧～ 0X06 AI AI 的进步真的太快了，今年我用 AI 的时间应该远远大于 2023 年了（花的钱也远远高于 2023 年）。这里点名表扬几个在本年度里表现优异的 AI 小弟。另外我正在筹划写一篇有关在国内如何快速接入高质量 AI 的博客，到时候会详细介绍这些内容。（大纲写完之后发现一篇根本写不完，应该会拆成两篇甚至三篇）。 0X07 读书 今年读的书应该是近几年最少的，简单挑几本有印象值得推荐的来说说吧 ","date":"2025-01-17","objectID":"/posts/2024-summary/:8:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"拍出绝世佳作/美姿/光线 被书名影响了的摄影书神作，虽然书名看起来有点像《21 天精通 C++》一样的东西，但内容质量非常之高，豆瓣评分三本都在 8.5 上下。第一本是写给纯新手的，从各个角度拆分讲解了如何拍出一张好看的照片，后面两本是专精于人像摄影的。如果有兴趣的话这一套书是非常值得看的。（别问现在水平咋样，问就是行走的监控摄像头） ","date":"2025-01-17","objectID":"/posts/2024-summary/:9:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"如何屠龙 这本书的名字和上面的完全相反，光看名字就感觉很有意思。但她实际上是一本借由剑与魔法来讲述欧洲中世纪的历史书，非常有趣，对中世纪历史或者剑与魔法背景游戏感兴趣的朋友可以买来读一读。 如何屠龙 我看这本书的时候总把自己带入到上古卷轴 5/巫师 3 的游戏背景里去，代入感直接拉满 🤣 ","date":"2025-01-17","objectID":"/posts/2024-summary/:10:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"克莱因壶 这本书被称为：日本虚拟现实 VR 题材开山杰作， 超前《盗梦空间》20年。但毕竟它太老太老了（1989年发行），以至于看起来里面很多地方都略显古板，不过如果喜欢看科幻、推理、悬疑、游戏的话，倒是值得一单。字数不算很多，一晚上就能看完。 说这本书古板当然不是批评，因为它作为一本科幻小说确实足够「古」了，而且在它之后也有很多这种类似设定的小说电影面试，在走他的路子，后来者无数才会显得「古」的。就类似于你现在第一次去看搏击俱乐部，八成会觉得「又是精神分裂，看都看麻了」，那是因为它开创了一个品类，后来者甚多导致的。 ","date":"2025-01-17","objectID":"/posts/2024-summary/:11:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"DOOM 启示录 传奇程序员约翰卡马克和他的团队研发 DOOM 的故事。这本书读的我热血沸腾，而且时常觉得人与人的差距有时候比人与猪之间的差距还大。 DOOM 启世录 你说你不认识卡马克？他也就只是 创造了 FPS 游戏品类，开发了 DOOM、德军总部、Quake；开创了多人联机游戏；开创了 MOD 文化的一个普通程序员而已 。 0X08 其他 依旧是把想要分享又不知道放在什么地方的一些东西放在这里了。 ","date":"2025-01-17","objectID":"/posts/2024-summary/:12:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"iPhone 15 Plus 垃圾苹果，60hz 刷新率 6G 内存的手机卖这么贵 。我其实没有被苹果所谓的绑架，只是单纯的觉得 iOS 虽然有很多很蠢的地方，但也有好多优势，让我想继续用 iOS。我甚至觉得 iPhone 如果只谈硬件那几乎就是一坨 🤷‍♀️ 从 iPhone 12 mini 这种邪门的手机，换到 iPhone 15 Plus 这种邪门的手机，我这种人估计是不多。因为我觉得标准版再小也小不过 mini，再加上之前被 mini 的续航折磨到自闭，干脆就换了 Plus 版。iPhone 本身没什么好聊的，能用。唯一想夸一下的就是 Plus 的续航，我在电池设置里把它改成「只充电到 80%」之后每天都能正常使用到晚上睡觉，续航确实非常够我用了。也正是电池变大，导致我这一整年使用下来电池健康居然是 99% 🤯 ","date":"2025-01-17","objectID":"/posts/2024-summary/:13:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"Garmin 955 垃圾苹果，号称运动手表结果电池连 170km 骑行的时间都坚持不下来 。这个 Apple Watch 我是真绷不住了，所以把它出掉换了佳明的正经运动手表。功能上的确缺失了很多，但是那些缺失的功能我本来就没怎么用过……反倒是佳明的运动检测准确度、续航时间、更轻的重量、更快捷的按键操作让我感知明显。 ","date":"2025-01-17","objectID":"/posts/2024-summary/:14:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"照片打印 我自己家里买了个小的照片打印机，日常的照片临时打印一下感觉还不错。但是如果你有多张照片想打印的话，还是推荐淘宝找店家。自己的小打印机速度慢不说，价格还高，一张相纸差不多两块钱，效果还不如店里打印的。我自己在淘宝找店里打印的照片 100 张只要 88 还包邮，印刷质量还比我自己打印机出来的好不少。唯一的问题就是你要把照片发给店家，如果极度在意隐私的话就没办法了。 0X09 罪魁祸首 让我觉得今年生活被完全打乱的罪魁祸首可能就是上半年接的私活了。确实，靠下班之后的时间和周末的时间赚到了一点钱，但代价是什么呢？代驾就是工作日下班回家休息一会儿就要再干两三个小时的活，周末也要至少干几个小时。其实按数量说并没有多干特别多工作，但是体验上就差了很多。本来回家打打游戏想的是「好快乐」，回家学习就算是写写自己的小项目想的也是「我真棒」，即使回家躺平也是「好舒服」，但是干活就会有一种很不爽的感觉。到现在为止我已经工作了六年多了，之前从来没有过想要请假去放松一下的时候，但是接了这个活之后就时不时冒出这种想法。 本来想的只是下班之后拿出一些时间来写写代码，赚点外快也挺好的。但是没想到的是客户隔三差五就会修改需求，而且他们自己都不清楚自己想做的到底是个什么东西，东改一改西改一改，整的人心力交瘁。最开始的时候我还在想「程序性能」「扩展性」「优雅」这些东西，到最后已经是「程序能跑就行」了。 这样一想，公司里有销售、售前、项目经理、产品经理、研发 leader 等很多岗位的支持和各种规范化流程还是很有必要的。私活很难有这些东西，就会导致开发过程非常的不顺利。 切实感受到了 钱难挣屎难吃 的前三个字（并不是很想感受后三个字）。 0X0A 一点小思考 我之前下班到家总会在电脑前坐两三个小时，美其名曰「学习/读书」。确实，很多时候都是真的在学东西或者读一些书，但也正是这件事让我给自己上了一层无形的很大的压力。如果有一天我不想学不想看，就想玩游戏，那我大概率也会纠结着坐在那里用一种极低的效率去学习去读书，可能两个小时过去效果还不如平时二十分钟好。但这样做就会让我没有「负罪感」，因为我会告诉自己「我已经努力了，结果不好只是因为我状态不好，毕竟我都在这儿坐着看了两个小时了」；如果坐在沙发前玩一晚上游戏，虽然游戏是玩了，但是并不会快乐，一直会想着「本来应该去学习的，结果现在来打游戏了，果然我还是不太行」。 但是今年的后半段因为想着「反正今年的年度计划也乱七八糟了，而且搞这个私活整的挺累，还赚到了一点点外快，那不如直接摆烂一下吧」，就直接摆烂了。每天回家休息会儿就要么玩 PS5 要么刷 B 站，突然感觉这样好快乐（明明这才是绝大多数人的生活）。 也正是这个小经历让我决定在新的一年做出一些改变：默认情况下还是按之前的逻辑去学习和提升，但也不要过分要求自己「假装努力」，如果想玩想休息那就放宽心去就好了，少一些压力多一些快乐不是犯罪 。 0X0B 明年会更好吗？ 显然今年并不是很满意，而且一年下来不管是自己的专业技能、兴趣爱好还是日常生活都没有发生什么明显的变化。不过好在经过这一年的生活我已经慢慢懂得了如何进行自我调节，而且计划 2025 年会比 2024 年有多得多的变化 。借用官媒常用的一个词来说的话那就是稳中向好 🎉 ","date":"2025-01-17","objectID":"/posts/2024-summary/:15:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"期待的游戏 另外 2025 年光是游戏就足够我期待的了。继村里第一个大学生直接上了清华之后，参与「明年高考」的学生也有一个看起来很有潜力的：《明末：渊虚之羽》。还有一个比较期待的是让我记住了一个并不符合我英语水平的单词的游戏： Civilization VII 。当然要说最期待的就是 Unspeakable VI 了，小学时候家里的电脑性能不够没法在迈阿密飙车，现在起码不会卡成 PPT 了 🤣。 GTA 5 之后 12 年，GTA 6 来了；那么另一个 5 之后已经快 14 年了，6 呢？ 最后就是任天堂了，众所周知一款游戏机的生命周期基本上是七年。现在七年之期已到，你的新机器呢 😭 而且按理说一台新机的发布势必会伴随着护航大作的到来，之前给 switch 护航的就是旷野之息和马里奥奥德赛，我都不敢想这种级别的阵容再来一次能有多幸福 😭 ","date":"2025-01-17","objectID":"/posts/2024-summary/:16:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"期待的电影 比起游戏界的 2025 神仙打架，目前已知的 2025 电影几乎没什么能打的，自然也没什么期待。以前每年都要去个十次八次电影院，现在一年下来能去个三两趟都算是不少了 😮‍💨。 唯一一个期待的就是 2024 年初备案的电影《英雄出少年》，据说已经杀青了，是姜文导演编剧的新片。不过这个阵容我是一点都看不懂：姜文、马丽、葛优、赵本山、雷佳音，据说还有胡歌、宋小宝。虽然阵容看不懂但不妨碍继续期待。 0X0C 总结的总结 根据我一向的「好习惯」，一定是要给总结写一个总结的。但是我对今年的满意度真的很低，以至于没有什么太大的兴趣再总结一次了。但是明年也就是 2025 年，就要迎来我的 30 周岁生日了，向着 30 周岁发起冲锋吧！（突然中二起来了） ","date":"2025-01-17","objectID":"/posts/2024-summary/:17:0","tags":["Life"],"title":"2024 年度总结","uri":"/posts/2024-summary/"},{"categories":null,"content":"0X00 你需要一台 NAS 吗 不知怎么的，在移动互联网疯狂发展的今天，反而慢慢开始兴起了自建网络服务这种复古风潮。最近这些年身边的朋友同事越来越多聊到 NAS 了，甚至 B 站上出现了一小撮 NAS 区 UP 主（没错，你知道我说的是谁）。就更不说现在淘宝咸鱼上大量的 NAS 专用机箱，甚至是 3D 打印的定制化版本了。 你真的需要一台 NAS 吗？NAS 说白了就是一块连着网的硬盘，速度比直接插电脑上还慢一些，如果你平日里需要访问数据的设备并不多，且拥有一个台式电脑，那不如先买两块硬盘插上去。通过文件共享功能将台式机转换成一个带有存储功能的兼职 NAS。 如果你家里的多个手机、平板、电脑、电视都需要访问存储，或者你没有台式机可以扩容，那确实可以考虑搞一台 NAS。 我自己的需求是这样的： 平时喜欢摄影，每次拍摄回来的照片少则 10G 多则 50G，日积月累已经有大几百 G 了，需要备份 也喜欢拍一些视频，相机拍摄的码率都很高，随随便便 50G 100G 的，需要备份 患有仓鼠症，喜欢囤一些高清电影电视剧来看，总共搞了有差不多 8T 了 手机、平板、电视、电脑都需要访问上面的影视资源 什么，你说你就是想要，不管需求？那就买呀，挑着贵的和好玩的买～ 0X01 我的 NAS 之路 最早的一台 NAS 是 2018 年买的 Synology DS118 性能烂到家了，也只有一个盘位，配了一块 4T 红盘，不过还是老老实实用了差不多 3 年。 DS118 存储告急之后打算升级 NAS，觉得自己很少用得上群晖引以为豪的软件，再加上对自己的技术实力有一捏捏🤏的信心，所以开始尝试自组 NAS。硬件选择了奔腾 G6400 + 16G 内存，第一次使用了 OMV ，觉得它对 Linux 的改动很少，所以相对来说更玩得惯。不过用了没多久还是换成了 TrueNAS Core，这套平台就稳定运行了很久。 当时买了 8T 的 HC320，买了没几天奇亚币就上天了，我的硬盘价格也跟着上天了。时隔一周，我 899 买的硬盘店家就卖 1899 了 🤷‍♂️ 因为 TrueNAS Core 是基于 BSD 的系统，后来看 TrueNAS SCALE（基于 Linux 的）稳定了之后就切到了 SCALE 版本，也顺势将 G6400 升级成了 i3-10100，内存也加到了 32G。 升级之后的 NAS 一直稳定运行到了今年，突然有一天我觉得家里另外一台用作 homelab 的服务器开机时间太少了，然后我看着身边的一台性能「强劲」的 homelab 和一台性能「羸弱」的 NAS 陷入了沉思。沉思过后，从兜里掏出了一张「融合」： 0X02 有点强的硬件配置 说是融合，其实就是选用了 homelab 的 CPU内存主板和原 NAS 的机箱硬盘，大概配置如下 Name Model CPU AMD Epyc 7551P 32C64T 主板 SuperMicro H11SSL-i 内存 DDR4 16G ECC * 4 HDD HC320 8T * 3 HDD 红盘 4T * 1 HDD 银河 4T * 1 SSD Intel 1.6T 网卡 Intel 2.5G 试问哪个程序员不想要一台 32 核 64 线程 64G 内存 近 30T 存储容量的 NAS 呢 🤣 配置单疑点：真有必要上这种 CPU 吗？没有，完全没有，事实上是之前 i3-10100 应付我的需求也完全没有任何问题。我只是正好有这个，卖也不怎么值钱，不如装上。真有必要上 64G 内存吗？没有，完全没有，事实上之前 32G 内存 应付我的需求也完全没有任何问题。我只是…有点上头，所以上了 64G。真有必要搞这么多存储吗？没有，完全没有，事实上🤦‍♂️这个还是有的，或者说因人而异，毕竟我确实要存好多照片、视频和电影电视剧。听说 HC320 很拉？其实我没觉得，所谓炒豆子声确实存在一些，不过也只存在于读写的时候，但是 NAS 其实大多数时候磁盘是可以休眠的。另外我放在桌子下面使用完全不会在意，只要别放卧室就没有问题。你那个 1.6T 是不是有点扎眼？这是我在咸鱼上收的二手，当时放在 homelab 上用的，现在拿过来做缓存（缓存意味着丢了也问题不大），正经自己配置缓存的话一般 250G 就妥妥够用了。 0X03 硬件选择 我这个硬件配置其实也就图一乐，对大多数朋友的选购并不能起到多大的帮助作用。这一节来简单帮大家分析一下需求，选择自己需要的配置。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:0:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"CPU 如果没有万兆传输需求，那 300 块钱买一个 G6400 或者同等水平的 CPU 就可以了，如果有万兆需求的话可能 CPU 性能上需要再上一档。 另外挑选 CPU 的时候可以注意一下它的编解码性能，如果性能达标的话在 NAS 上做视频转码就舒服多了。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:1:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"内存 内存主要就是看大小，如果你不打算用 TrueNAS 且不打算跑好多个服务在 NAS 上的话，4G 或者 8G 内存就是够用的。如果打算用 TrueNAS 的话可以在预算范围内多购置一些内存，有关系统选择的内容我写在下面了。 有必要 ECC 吗？可以，但没必要。随便买几根内存插上就行了，真想讲究的话就买同品牌同频率同型号的内存。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:2:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"主板 主板要在预算范围内选靠谱一些的，劣质主板的供电可能会有问题，这对于我们这种 7*24 运行的机器来说是很大的问题了。 另外如果计划加的硬盘多的话要先看一下硬盘供电和 SATA 接口是不是够用，不够的话还要买 PCIE 转接卡。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:3:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"电源 电源更好选，我们选一个转换率达标的大牌 就好了，功率反而是最不重要的。另外我建议各位选购全模组电源，因为我们大概率是不会给 NAS 装显卡的，用上全模组电源就可以让机箱里少很大一坨线。 现在我用的就是非模组电源，机箱里一大坨一大坨的供电线闲着用不上，徒增散热难度。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:4:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"硬盘 硬盘就在淘宝上找那几个口碑好的店去买就好了，既然组 NAS 了我建议还是 4T 起步，8T 10T 16T 最好。 如果需要缓存盘的话，要考虑自己是否会上万兆，如果是万兆内网的话一定要上 nvme 的盘，否则 SATA 的 SSD 就可以用作缓存了。因为 SATA 3.0 的速度是 6 Gbps 要高于常见的千兆、2.5G 和 5G，但是不到万兆的 10G。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:5:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"网络 网络基本上有四个选择，大家可以根据自己的需求进行选择： Wi-Fi 愚蠢之选 ：速度慢且不稳定，NAS 系统对 Wi-Fi 的支持也不好，没有任何理由选择为 NAS 配置 Wi-Fi； 千兆 新手之选 ：不改动网络，不新增设备，125mb/s 的速度能满足几乎所有功能性需求； 2.5G 实用之选 ：2.5G 的网卡和交换机都不算贵，能将内网速度提升一倍不止，性价比很高； 万兆 高玩之选 ：如果经常有大量数据传输的需求，那可以咬咬牙上万兆，100G 的数据也能一分多钟搞定，不过一般玩家就不太建议这么搞了，太贵； 你说还有 40G ？散了散了，买不起了 🤷‍♂️ 0X04 系统选择 自组 NAS 的话基本上就下面这几种系统方案： TrueNAS Core 稳如老狗 ：企业级的 NAS 系统，基于 BSD，原生 ZFS，稳如老狗。不过没什么可玩性，如果是用作纯 NAS 的话可以考虑； TrueNAS SCALE 稳中带皮 ：同样的企业级 NAS 系统，基于 Linux，也很稳定，比 Core 版的可玩性强一些，带 Docker； OMV 简单粗暴 ：一个相对简单的系统，基于 Debian，没有深度用过，不过用户量也不是很大； WIndows Server 熟悉的地方 ：有些用户对 Windows 很熟悉，对 Linux 则一窍不通，那其实装个 Windows 开启文件共享也挺好的，还免了后续的折腾； Linux 硬核之选 ：如果你对自己的 Linux 技术有信心，是可以直接装 Linux 然后自己配置文件共享的，这样还更自由，只是 web 面板的缺失会导致修改配置和部署服务的麻烦程度直线提升； Unraid 玩家之选 ：Unraid 是除了 Windows 以外唯一一个付费系统，可以方便的配置 Docker 和虚拟机，也有应用商店可以快速安装一些诸如 Plex、qbittorrent 之类的应用； 按个人使用经验来说，比较推荐 TrueNAS SCALE 和 Unraid 两个。 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:6:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"TrueNAS SCALE 优势： 开源且免费 ZFS 很屌 带有插件市场，方便安装插件 支持在 NAS 上部署虚拟机 劣势： 上手难度较高 ZFS 并不适合所有人 ZFS 对内存要求较大 插件市场并不很完善 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:7:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"Unraid 优势： 阵列系统自由度高 上手门槛低 插件市场相比 TrueNAS 更完善 中文化做的更好 管理界面更加直观 也能部署虚拟机 劣势： 要钱，现在还改成了订阅制（可以去咸鱼收老的激活码） 对 ZFS 支持不佳 在使用校验盘且没有缓存的情况下写入速度慢 0X05 阵列介绍（Unraid Only） 这里简单介绍一下 Unraid 的阵列，严格来说不叫阵列，应该是 Array。 我们以 3 块 8T 的硬盘为例，如果组成无校验的 Array，可用空间就是 3*8=24T。向其中存储的文件会以文件为尺度分散在所有硬盘中 ，这种方式的特点就是：读写的速度都是单盘的速度 。并且数据安全性上也比较特别，由于都是以文件为尺度进行分盘的，所以即使 3 块盘坏了 2 块，那活着的盘里的数据也还能都取出来，极端点说 100 快盘坏了 99 个，最后那个活着的也能读数据出来。这就是 Unraid 的特性。 还是 3 块 8T 的盘，如果将其中一个用作校验盘 ，那可用空间就是 2*8=16T。存储的文件依旧以文件为尺度分散在硬盘中，但是每次写入数据到「数据盘」时，都会同时计算校验值写入「校验盘」。我们假设数据盘 A 存储了[01010101]的数据，数据盘 B 存储了 [01100110] 的数据，那经过计算我们校验盘就会存储 [00110011]（通过计算对应位置的二进制得到的）。这样一来 2 块数据盘无论坏掉哪一个，都可以根据另一个好的和校验盘进行计算将其数据恢复出来；如果校验盘坏了，那数据本身就没有丢，再替换一块校验盘上去即可。带校验的模式也有一点风险，就是「如果只有校验盘活下来」的情况，这种情况虽然校验盘活下来了，但是数据还是全都丢完了。 一般来说你都用 NAS 了，想必是对数据安全有一定要求的，所以建议 Unraid 配合校验盘一起使用，担心性能问题大不了再加一个缓存盘就是了。 0X06 阵列介绍（TrueNAS Only） 选择 TrueNAS 肯定是要用 ZFS 的，我们一般用 ZFS 也就这几种用法：strpe, mirror, raid-z, raid-z2。严格来说这些阵列方案和常见的 RAID 是不一样的，但也大差不差，所以我还是写了这个简易的对照表，方便各位理解。 ZFS Name RAID Name strpe raid-0 mirror raid-1 raid-z raid-5 raid-z2 raid-6 还是那句话，你都用 NAS 了，想必是对数据安全有一定要求的，所以直接抛弃 strpe 这种一损俱损的疯狂模式，我们从 mirror 开始介绍。 mirror 是最简单的，就是纯镜像。我们依旧假设你有 3 个 8T 盘，现在将他们组成 ZFS mirror 之后可用空间就只有 8T，但是安全性暴涨。mirror 模式会将写入阵列的数据同时写入到所有磁盘，也就是说每个盘里都有完整的数据，这样一来 3 块盘只要没有一起挂掉，那数据就全部还在，是数据安全性最高 的。 raid-z 则是带校验的，但是与 Unraid 不同，raid-z 不存在校验盘的概念 。三块盘的身份是相同的，功能也是一样的，你写入的数据会分散在两块盘中，再将校验数据写入另一块盘。这种方式乍一看和 Unraid 上使用单个校验盘是一样的，但其实大相径庭。由于数据分散在三块盘中，校验数据也分散在三块盘中，这就意味着读写都是三块盘一起工作的，所以性能比带校验的 3 盘 Unraid 强 好多。但是当同时坏掉两块盘的时候，不能像 Unraid 一样有概率恢复一半数据。 raid-z2 则是带两个校验的，通常只有硬盘数更多时（例如 5 块）才会考虑，原理同上。带两个校验盘就意味着整个阵列中可以坏掉随意两块盘。 0X07 装都装了 跑个服务吧 如果你的 NAS 性能比较强，有多余的性能可以跑一些服务的话，我可以推荐一下我部署的服务： Plex 影音中枢，可以将自己下载好的电影电视剧做成海报墙，并且在多个设备上无间断播放，需要在电视电脑平板手机等多设备上观看的话还是很好用的； FileBrowser 文件管理器，虽然可以通过 SMB 协议挂载目录到手机电脑上，但偶尔不方便挂载的时候可以用它管理文件； cailbre-web 电子书管理器，类似于 Plex，不过它是针对电子书的，可以在线看书、管理、收藏； firefox 有点离谱，但是 Firefox 可以跑在浏览器里，我把 Firefox 通过 frp 做了内网透传，这样人在外面只需要访问这个 Firefox 服务就能间接访问家里部署的所有 web 服务了； frp 内网穿透，允许你通过自建的服务器转发流量到家中，实现远程访问家里 NAS 的功能； homeassistant 智能家居，还没深度使用，不过多介绍； netdata 性能监控，说实话没什么用，但是看着帅； qbittorrent BT 下载神器，没有介绍的必要； SpeedTest 测速工具，用来测试内网速度是否达标 0X08 冷静消费 一波分析后蠢蠢欲动？冷静，冷静，冷静 开搞之前先问自己几个问题： 真的有那么多的文件要存储吗？ 真的有那么多的数据要备份吗？ 真的有那么多的设备要访问吗？ 电脑加硬盘的方案真的不行吗？ 如果真的都考虑清楚了，那就开搞吧～ 最后提醒一句数据无价，做好备份 ","date":"2024-07-02","objectID":"/posts/nas-build-2024/:8:0","tags":["NAS"],"title":"2024 年的自建 NAS 不专业不完全手册","uri":"/posts/nas-build-2024/"},{"categories":null,"content":"0X00 来骗，来偷袭 这次来介绍一个来骗来偷袭的 Python 库：Faker。我们平时经常会跟数据库、跟 csv 这些东西打交道。尤其是当你设计一个数据库表的时候，开发和测试环境中只有空荡荡一个表，没有测试数据就很尴尬。 Faker 就是设计来解决这种问题的，它可以快速生成各种你需要的假数据。安装和使用都非常简单：pip install Faker 就可以完成安装。 这篇 mini 博客的目的是解决「不知道自己不知道」的问题，也就是说明有这么一个库可以做什么，然后介绍简单的用法；具体这个库的完整用法还是要去查看文档。 这里给出一个简单的使用实例： #!/usr/bin/env python3 from faker import Faker faker = Faker('zh-cn') print(faker.name()) print(faker.email()) print(faker.ipv4()) print(faker.ipv4_private()) print(faker.ipv4_public()) print(faker.city_name()) 简单示例 OUTPUT: 李林 chao83@example.com 96.124.160.192 192.168.72.158 220.137.146.132 张家港 这里仅仅有两条需要注意的： 实例化 faker 的时候记得标记语言，默认是英文信息； 实例化的 faker 每次调用都会生成新的假数据，只需要实例化一次即可； Faker 支持生成非常非常非常多数据类型，这里就不也没必要一个个介绍出来。分享一个我自己的用法：可以用 dir(faker) 的方式看它究竟有多少假数据类型可用，也可以在 iPython 中实例化一个 faker 出来然后通过 faker. TAB 的方式进行补全，比如你输入 faker.ip TAB 就可以看到ipv4()/ipv4_network_class()/ipv4_private()/ipve_public()/ipv6()这些。 👇下面我让 GPT 生成了一段脚本，可以看到使用 Faker 创建假数据使用 AI 偷懒是多么的舒爽 #!/usr/bin/env python3 import csv from faker import Faker # Set up Faker with Chinese locale fake = Faker('zh_CN') # Number of students to generate num_students = 100 # List to store student data students_data = [] # Generate data for 100 students for _ in range(num_students): student = { \"name\": fake.name(), \"age\": fake.random_int(min=18, max=25), \"phone_number\": fake.phone_number(), \"email\": fake.email(), \"address\": fake.address(), \"student_id\": fake.random_number(digits=8, fix_len=True) } students_data.append(student) # Define the CSV file name csv_file = 'students_info.csv' # Define the CSV header csv_columns = [\"name\", \"age\", \"phone_number\", \"email\", \"address\", \"student_id\"] # Write data to CSV file try: with open(csv_file, mode='w', newline='', encoding='utf-8') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=csv_columns) writer.writeheader() writer.writerows(students_data) print(f'Data written to {csv_file} successfully.') except IOError: print(\"I/O error occurred when writing to the CSV file.\") 生成出来的文件如下，乍一看是不是还挺像那么回事的 name,age,phone_number,email,address,student_id 杨文,21,15651446876,xiangjing@example.net,天津市东莞县沈北新西宁街T座 528464,38652303 任莹,20,13151815142,ming06@example.org,澳门特别行政区海门县静安谭街E座 169654,62640223 马斌,22,15317418861,weiguiying@example.com,山西省西安市南溪海门路c座 632185,51262627 万龙,25,18675244909,pqiao@example.org,澳门特别行政区惠州县海港陈街u座 788331,12862421 唐莹,21,14524896623,wli@example.org,天津市马鞍山市黄浦拉萨街w座 346616,13890353 王秀芳,24,15093668582,jie32@example.net,辽宁省佛山县魏都何街x座 791424,75220986 李建平,21,18293878743,xiulanzhong@example.com,香港特别行政区彬市崇文北京街F座 134329,11422454 谢丽娟,19,13242202418,pqian@example.com,新疆维吾尔自治区香港县滨城南宁路w座 529098,18889078 薛婷,20,13807465629,zliang@example.net,北京市云市花溪刘街e座 773539,35816274 雷丽娟,18,18165303861,yong51@example.net,湖北省济南县孝南陈街J座 484354,27706654 朱秀兰,19,15036330967,caogang@example.com,台湾省佳县城北荆门街K座 131039,16335754 张丽丽,24,15032876465,hanxiuying@example.org,澳门特别行政区潜江市浔阳海口路j座 179700,60603103 赵桂兰,20,13009006486,minxie@example.com,四川省桂芝市海陵李街s座 535125,70265537 赵春梅,24,15251434633,xiaojie@example.org,北京市南昌市朝阳嘉禾街Q座 111575,84861999 蓝玉英,23,15006072574,xiuyingbai@example.net,辽宁省齐齐哈尔市东丽张路X座 952846,59067909 马超,22,15023489681,ishen@example.net,山东省广州市永川任路I座 246768,57163888 冯丽丽,19,18941560722,suyong@example.org,重庆市辉县怀柔大冶街n座 651829,48840449 张辉,22,15525627747,edu@example.org,内蒙古自治区南京市翔安林路Y座 828410,27849301 龙帅,20,13044861787,gangchen@example.com,福建省波县城北辽阳街B座 699463,93809855 王秀兰,23,15123952816,liyao@example.com,重庆市海燕市永川李路U座 589932,40837105 何秀云,22,13389508938,guojuan@example.org,宁夏回族自治区坤市华龙长沙街A座 448755,16331208 郑建,21,14773507926,na97@example.org,湖北省北镇市牧野邯郸街Q座 441383,52901105 黄桂香,20,15983226045,xia34@example.org,江西省长沙县西峰郑街v座 555389,38532397 雷桂花,21,15779091442,fli@example.org,青海省重庆县魏都金路r座 438666,25582281 洪伟,24,15034551309,bhu@example.org,甘肃省淑华市长寿沈阳街D座 781857,51371004 侯秀荣,20,15658144733,mindeng@example.org,河南省拉萨市兴山周路q座 690193,34181753 霍丽娟,23,14755592995,tianjie@example.org,湖北省沈阳市怀柔香港街E座 798600,59323935 李晶,21,14789714084,luomin@example.net,湖南省北京县牧野吴街m座 527079,20443595 何莹,20,15371237169,mwang@example.com,云南省北镇市涪城东莞路m座 835686,314","date":"2024-06-19","objectID":"/posts/python-faker-library/:0:0","tags":["Python","faker"],"title":"假数据制造机：Python 中的 Faker 库","uri":"/posts/python-faker-library/"},{"categories":null,"content":"0X00 前言 不知道为什么，当三个好用的工具在一起的时候就会被称作：三剑客；四个好用的工具在一起的时候就会被叫做四大天王 🤔。 算了，这不重要。 这篇文章的目的是带不了解这三个工具的朋友们简单上手使用它们，默认各位是掌握了 Linux 的基本用法的，其中也会出现有关正则的内容。如果你不懂正则的话建议跳过正则的部分，并且看完这篇文章马上就去学。另外，不要因为正则看起来有点像通配符就按通配符的操作进行下去。 0X01 grep 首先这三个工具中最常用的应该就是 grep 了，它用于从文件中搜索你感兴趣的内容。例如下面的例子就可以输出 /etc/passwd 文件中包含 root 的行 grep root /etc/passwd \u003e\u003e output root:x:0:0::/root:/bin/bash 也可以接多个文件，这样输出的时候就会以文件名开头了 grep root /etc/passwd /etc/group \u003e\u003e output /etc/passwd:root:x:0:0::/root:/bin/bash /etc/group:root:x:0:root 下面介绍几个参数： -i 忽略大小写 -v 显示不匹配的行（取反） -n 增加行号显示 -c 显示总共多少行，而非具体内容 -r 递归查找所有文件 -A 也就是 after，即显示匹配行和它后面的 n 行 -B 也就是 before，显示匹配行和它后面的 n 行 -C 相当于 -A 和 -B 一起用，显示匹配行和它前后各 n 行 简单列举一下使用方法 # 忽略大小写 grep -i ROOT /etc/passwd # 可以匹配到 root 行 # 取反 grep -v root /etc/passwd # 匹配到非 root 行 # 递归查找 grep -rn root /etc # 查找 /etc 目录下所有文件，输出所有带有 root 的行 grep 命令默认情况下就可以使用正则表达式，下面的命令就可以匹配到 root 行 grep \"r..t\" /etc/passwd grep 很多时候是跟在管道符号后面的，例如 curl -XGET http://xxx.xxx/info/ | grep KEYWORD 这种。注意跟在管道符号后面时，grep 只能处理来自标准输入的内容，如果还需要处理标准错误的话需要手动将其重定向到标准输出中。 cat /etc/passwd | grep root 0X02 awk 相比于 grep 处理的是行数据，awk 则主要用于处理列数据。grep 可以从 /etc/passwd 中找到包含 root 的这一行，但是如果你想找所有用户他们用的 shell 就不容易了。 下面这个命令就可以在 /etc/passwd 中找到每个用户用的是哪个 shell 了。我们观察 /etc/passwd 这个文件可以看到里面每行都有多个字段，并且用 : 分割，第 7 列就是我们要的 shell。所以得到下面这个命令。 cat /etc/passwd | awk -F ':' '{print $7}' \u003e\u003e output /bin/bash /usr/bin/nologin /usr/bin/nologin .............. /usr/bin/nologin /usr/bin/nologin /usr/bin/git-shell 先来分析一下这个命令，从 awk -F ':' '{print $7}' 开始。其中 -F 参数就是指定一个「分隔符」，后面的 : 就是分隔符本符，最后一个参数 {print $7} 表示输出第 7 列。其中 {print $INDEX} 可以理解成是一个固定的语法，也可以用 {print $1,$7} 的方式输出第 1 和第 7 列，且用逗号分割。 程序中第一列不是 $0 虽然有些诡异，但它也没有被弃用，可以试试 {print $0} 是什么作用。 这里的分隔符也是可以用正则的，只是一般来说这里用正则的时候比较少见。 0X03 sed sed 的定义是一个「流编辑器」，如果理解不了就把他当成一个连 vim 那种界面都没有的编辑器好了。 我们在开始之前先把/etc/passwd 复制一份到 /data/passwd，防止一会儿误伤操作系统 如果你用过 vim 的话，上手 sed 应该是比较容易的。我们先来看一个例子 sed \"s/root/Administrator/\" /data/passwd 这个例子就是将 /data/passwd 中的 root 换成 Administrator（好熟悉的用户名 🤣）。但是它只会修改每行的第一个 root，你可以观察一下你的输出是不是这样。如果你想替换的是所有的呢？知道的朋友肯定知道了：「只需要在替换命令后追加一个g」。是的，这就是典型的 vim 用法了。改过之后的命令应该是 sed \"s/root/Administrator/g\" /data/passwd。 这时候你可能发现好像你每次改的内容都回显到标准输出了，并没有对文件生效。但是也不要心急去用重定向（你不信邪也可以试试，反正文件复制出来了没有什么风险），想让修改直接对文件生效需要给 sed 命令加上 -i 参数，最后的成品如下，这样一来就可以将文件中的 root 全数替换成 Administrator 了。 sed -i \"s/root/Administrator/g\" /data/passwd 如果你想删除某行，可以用 sed \"/root/d\" /data/passwd 的方式去删除。 如果你不信邪去试了重定向，会发现文件变成空白的了。这是因为当你的命令中出现重定向时，会优先准备重定向，也就是说当 sed 运行起来的时候该文件已经是等待输入的空白文件了。 也许你平时对 sed 并不感冒，但是当你需要处理一个很大的文本文件的时候，相信我，你一定会想起它的好的。 0X04 最后 通常来说这些工具会和 cat、tail 等工具一起用，通过管道将他们联系起来。 有关组合技的用法就太多了，CLI 最大的魅力可能就在于组合，不过还是需要各位在工作和学习中慢慢摸索～ ","date":"2024-03-21","objectID":"/posts/linux-text-process/:0:0","tags":["Linux"],"title":"Linux 文本三剑客 grep/awk/sed 入门手册","uri":"/posts/linux-text-process/"},{"categories":null,"content":"0X00 基础知识 常用 Linux 的各位估计都知道 iptables、firewalld 和 ufw 这三个工具吧，或者还知道 netfilter 这个内核组件。但是他们究竟是什么关系呢？从关系上来讲，可以将他们分成三层：最底层是 Linux 的安全框架 netfilter，上面是用来操作 netfilter 的 iptables，再上层是 firewalld 和 ufw。 其中 firewalld 一般会默认安装在 RHEL 和 CentOS 中，ufw 会默认安装在 Debian 和 Ubuntu 中。不过由于很多人还是习惯直接操作 iptables 所以这次的重点就是它了。很多人会说 iptables 是防火墙，这其实并不严谨，它的标准定义应该叫做 Packet Filter 也就是包过滤工具。而且事实上也是如此，它不仅能够实现防火墙的限制流量功能，还能提供 NAT 转发的能力。 iptables 内部总共拥有 4 张表（table），又有多个链（chain），如下图所示。 既然是「写给新手的 iptables 使用说明」，自然是挑选最重要的一部分来介绍的，也就是 nat 和 filter 两张表。其中 filter 表应该是我们接触最多的表，它用于决定一个数据包的「命运」，比如你想将某些数据包拦截在外，或者堵住前往某地址的出口就可以用它实现。在使用 iptables 命令时不手动指定表的话就是在操作 filter 表。另一个 nat 表顾名思义就是用来配置 NAT（网络地址转换）的了。 需要注意，严格来说本次只说明 iptables 和与之强相关的命令，也就是说只涉及 IPV4 的配置 。如果需要配置 IPV6 的规则，则需要使用 ip6tables 命令，虽说两个命令大差不差，但是这里还是以 IPV4 为主，也只考虑纯 IPV4 环境。 在开始使用命令之前，先要给出一张图作为前置知识：注意看这张图（略复杂，不过前期看不明白也不影响基本的使用）。可以看到其实流量不是从一个表一个表走下去的，而是按照链的顺序在前进，并且不同的流量会走的路径也不完全一致。 另外再给出一些常用的命令，用来辅助后面的实验。 # 查看 filter 表的所有规则（因为没指定表，所以是默认的 filter 表） iptables -L # 查看 nat 表的所有规则 iptables -t nat -L # 删除 filter 表的所有规则 iptables -F # 删除 nat 表的所有规则 iptables -t nat -F 实验环境： 毕竟实践出真知，建议搞一台最好两台在同一网段的虚拟机进行试验。 注意 ，接下来两个章节的的命令都是临时生效 的，所以如果玩砸了可以直接 iptables -t nat/filter -F 清空规则，或者重启虚拟机从头来过。具体如何将规则写入配置中使其持久化，可以查看章节 0X03 的内容。 0X01 filter 表 既然 filter 表是最常见的，那就从它开始吧～ filter 表中有 input/forward/output 这三个 chain，顾名思义就是分别用来控制传入流量、数据包转发、传出流量的。如果你想控制哪些流量能来不能来，就去在 input 链上加规则；想做流量转发就去在 forward 链上加规则；想控制出口流量就去在 output 链上加规则。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:0:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"放行 22 端口 我自己比较喜欢对着完整命令拆解其参数来学习它的用法，所以也就这样演示了，所以先来看命令吧。 iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT 先看第一条命令，我们把它拆成几个部分来看 iptables 就不用多说了，就是这个工具本身； -A INPUT 其中 -A 参数是 append 的意思，就是在 INPUT 链上追加规则； -i eth0 其中 -i 是 in-interface 的意思，就是指定了从 eth0 来的流量（要注意来 字，是有方向的）； -p tcp 很好理解，-p 就是 protocol，指定了 tcp 协议； --dport 22 其中的 -dport 是 destination port 的缩写，也就是指定了目的端口是 22； -m state 指的是匹配类型选择了「状态匹配」，与后面的 --state 配合使用； --state NEW,ESTABLISHED 是与上面的 -m state 配合的参数，指的是匹配新建的连接和已经存在的连接； -j ACCEPT 其中 -j 指的是跳转(jump)到具体操作，这里指的是 ACCEPT，也就是放行； 所以第一句用人话来说就是：在 filter 表的 INPUT 链上新增一条规则，放行来自 eth0 且访问本地 22 端口的 tcp 流量（包括新建的和已经存在的连接）。看似很长，其实拆解完之后就简单很多了，而且很多时候我们并不需要在命令中加入 -m state --state NEW,ESTABLISHED 的参数。 在第一个命令的基础上看第二行的命令就更简单了，只是将 INPUT 链换成了 OUTPUT 链，并且是将 --dport 改成了表示 source port 的 --sport。这样一来这句话的含义就是：在 filter 表个 OUTPUT 链上新增一条规则，放行从 eth0 的 22 端口出去的 tcp 流量。注意这里的 -o eth0 指的是从 eth0 口出去 的流量，与上面的 -i eth0 形成了对比，这里的 -o 是 out-interface 的意思。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:1:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"检查配好的规则 我们执行完这两条命令之后查看一下 iptables -L -n 的输出，我这里是这样的 Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 state NEW,ESTABLISHED Chain FORWARD (policy DROP) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp spt:22 state ESTABLISHED 这里解释一下 -n 参数，默认情况下 iptables 会去尝试解析规则中出现的 ip 对应的 hostname 和 port 对应的 servicename，我们为了更加直观的看到刚刚的结果（速度也会更快），就添加了 -n 的参数。它的作用就是让 iptables 不去解析 hostname 和 servicename，直接以数字化的形式（Numbric）展示出来。 从输出中已经可以看到我们配置上去的这两条规则了。另外可能还注意到了 Chain INPUT (policy ACCEPT) 的这块，它的意思是说 INPUT 链的默认规则就是 ACCEPT，也就是说默认情况下所有的入站流量都是会被放行的，OUTPUT 链也是一样，这里就该介绍一下默认情况了。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:2:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"默认规则 可以使用 iptables -P INPUT DROP 命令来将入站流量的默认值设为 DROP 也就是抛弃（当然也可以将其改为 ACCEPT 了），这里的 -P 参数意思是 Policy。 iptables 中的默认值意思是当一个数据包没有命中配置的任何一条规则时，采用的策略 ，也就是一个兜底的选择。所以说如果你对服务器的安全性要求比较高，就可以将其出入站的默认值都设置为 DROP，然后再根据需求开放指定的 ip、port、protocol 等。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:3:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"特定来源/目的地以及端口 下面再看两个命令 iptables -A INPUT -s 192.168.81.0/24 -p tcp --dport 2333 -j DROP iptables -A OUTPUT -d 192.168.81.0/24 -p tcp --dport 2333 -j DROP 这里直接看这个新参数 -s 和 -d 吧，他们分别表示 source 和 destination，也就是来源和目的地。后面跟的不仅可以是一个标准的 ip 地址，也可以是 CIDR 的一段 IP 地址。通过上面两个命令，就拒绝了来自 192.168.81.0/24 且访问自己 2333 端口的 tcp 请求，也拒绝了访问 192.168.81.0/24 的 2333 的 tcp 请求。 我们已经知道 -p 参数接受的是协议名称，这里再强调一下它只能接受「网络层协议」，比如你写一个 -p http 或者 -p arp 是不行的。他只能接受：tcp/udp/udplite/icmp/icmpv6/esp/ah/stcp/mh 这些参数，或者用 all 表示所有（其实不加 -p 参数就是指代所有）。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:4:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"防止被 ping 比如你想禁止别人 ping 自己，就可以用这样的一个命令来阻止：（ping 命令使用的是 ICMP 协议） iptables -A INPUT -p icmp -j DROP ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:5:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"关于 forward 只有当你的机器作为路由器工作的时候 forward 链才有配置的必要，绝大多数情况下是并不需要配置 forward 的。介于这篇博客是写给新手的，这里就不涉及路由器的配置了。 0X02 nat 表 了解的人肯定了解 nat 是个什么东西，不过比起 filter 这个非常直观的名字来说还是需要简单介绍一下： 网络地址转换（英语：Network Address Translation，缩写：NAT），又称IP动态伪装（英语：IP Masquerade），是一种在IP封包通过路由器或防火墙时重写来源或目的IP地址或端口的技术。这种技术普遍应用于有多台主机，但只通过一个公有IP地址访问网际网路的私有网络中。 – wikipedia 一般来说操作 nat 表除了要做 SNAT（源地址转换） 和 DNAT（目的地址转换）之外，就是搞端口转发了，且真正去做 NAT 的需求又并不常见，所以针对 nat 表我们就只介绍有关转发的内容好了。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:6:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"确认允许转发 有些发型版本默认是不允许转发的，我们可以通过 cat /proc/sys/net/ipv4/ip_forward 检查当前是否允许 ip 转发。很明显，如果输出的是 1 就是允许，0 就是不允许了。修改它也很简单，直接把 0 或者 1 重定向写入进去就好了，比如 echo 1 \u003e /proc/sys/net/ipv4/ip_forward。 细心的你可能发现了 ，这个文件路径它包含 ipv4 这个字段，那它是不是只能修改 IPV4 的转发呢？确实。如果需要修改 IPV6 的转发则需要改动 /proc/sys/net/ipv6/conf/ens33/forwarding 这个文件。并且注意这个文件中的 ens33，它表示具体某个网口，需要按需修改。如果你想修改所有的网口，可以将它修改为 all。 细心的你可能还发现了 ，这个文件路径它在 /proc 下面，貌似并不能持久化？确实。如果需要将转发的配置持久化的话需要修改 /etc/sysctl.conf 文件，找到对应的配置修改为下面的配置，再使用 sysctl -p 将其应用就可以了。 net.ipv4.ip_forward = 1 net.ipv6.conf.all.forwarding = 1 下面的例子都是在开放了转发之后进行的。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:7:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"端口转发 在我的另一篇博客里提到了使用 ssh 命令建立隧道的方式实现转发，这里再利用 iptables 搞一下。假设你现在有 3 台机器，如下图：B 有两张网卡，接入两个子网，与 A 和 C 均相通，但由于 A 和 C 并不在同一子网内，所以两者无法沟通。现在你想让 A 顺利访问 C 部署在 2333 端口的服务应该怎么办呢？ 从原理上说，既然 A 能访问 B，B 能访问 C，那只要你让 B 在其中负责转发（传话）就没有问题了。 所以我们进行如下配置： sudo iptables -t nat -A PREROUTING -p tcp --dport 2333 -j DNAT --to-destination 192.168.2.103:2333 sudo iptables -t nat -A POSTROUTING -j MASQUERADE 先来看第一行配置： -t nat 是指定了操作 nat 表； -A PREROUTING 是指定了在 PREROUTING 链中追加规则； -p tcp --dport 2333 是指定了目的端口为 2333 的 tcp 流量； -j DNAT 是指定了跳转到 DNAT（目标地址转换）操作； --to-destination 192.168.2.103:2333 则是将目的地设置为 ip:port； 再来看第二行配置： -A POSTROUTING 是指在 POSTROUTING 链中追加规则（跟第一行不一样哈）； -j MASQUERADE 实现伪装，也就是说将自己转发出去的数据包伪装成本就是自己发的； 这样一来本来不通的 A 和 C 就通过 B 这个传话员成功通信了。 如果你发现你配置好了之后并不能成功通信，需要检查 B 节点是否允许转发（上面说了怎么设置）。确认允许转发还不行的话，再检查自己 filter 表的 forward 链是否会允许你的这个数据包通过。检查就很简单，直接用 iptables -L 就可以，正常来说它输出的内容你一定能看懂。 0X03 配置文件与持久化 开头的时候说过，这些命令的改动都是临时 的，那么如何让它持久化呢？一般有这两种方法：iptables-save + iptables-restore 或者单用 iptables-persistent。两种方法不分优劣，硬要说的话后一种方法比较「优雅」，但是大多数人还是用的前一种方法，所以建议跟他人协作的工作中还是使用第一种方法（除非你有办法说服别人用第二种）。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:8:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"save + restore iptables-save 和 iptables-restore 这两个命令是跟随 iptables 一起的，不用担心没有安装的问题。前者的功能就是输出当前 iptables 的配置（与 iptables 一般命令不同，这个命令默认就是所有 table 了），后者就是恢复 iptables 配置。 我们可以使用 iptables-save \u003e iptables_config 来把当前生效的规则导出成文本文件，重启之后想要恢复之前的配置用 iptables-restore \u003c iptables_config 就可以了。不过按照习惯，大家会把这个配置文件放在 /etc/iptables/rules.v4 里（看名字也知道，IPV6 的配置一般文件名是 rules.v6）。 接下来只需要写一个脚本让系统启动的时候自动导入这个配置就可以了。这里建议将脚本命名为 /etc/network/if-pre-up.d/iptablesload，因为 /etc/network/if-pre-up.d/ 目录是一个比较特别的目录。看名字也能猜到，在网口被 up 之前会查找这个目录下的可执行文件，逐个执行一遍，这样一来就可以做到当网口 up 起来的时候 iptables 已经处于配置好的状态了。最后记得给脚本一个可执行权限，就 ok 了。 #!/bin/sh iptables-restore \u003c /etc/iptables/rules.v4 如果是使用的是 Debian 系的操作系统直接按上述操作就行，如果是 RHEL 系的话可能需要配置 systemd 的服务才行。或者使用下面这种更加优雅的方法～ ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:9:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"iptables-persistent 这种方法看起来优雅很多，不过 iptables-persistent 并不会随 iptables 一起安装，所以需要额外安装一下。安装过程中它就会两次询问你 Save current IPv4/6 rules?，如果你回答了 Yes 那么当前系统中已经配置好的规则就会通通写入到 /etc/iptables/ 中，并且命名为 rules.v4 和 rules.v6（没错，就是上面一种方式里提到的位置）。 每次你手动改完了规则之后就可以使用 netfilter-persistent save 将规则写入到配置文件中，如果你把规则搞乱了也可以使用 netfilter-persistent reload 将配置文件重新套用。 0X04 其他 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:10:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"删除规则 我们一直在说创建规则，那怎么删除规则呢？其实很简单，例如你用 iptables -A INPUT -p tcp --dport 2333 -j DROP 命令创建的入站规则，其中 -A 指的是追加一个规则，那你把它就它换成 -D 就是 Delete 了。所以用上面这个命令创建的规则可以用 iptables -D INPUT -p tcp --dport 2333 -j DROP 删除掉。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:11:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"规则的匹配顺序 需要注意的一点是，规则匹配是按照顺序自上而下的，这里看一个 iptables -L 的输出： Chain INPUT (policy ACCEPT) target prot opt source destination DROP tcp -- anywhere anywhere tcp dpt:2333 ACCEPT tcp -- anywhere anywhere tcp dpt:2333 Chain FORWARD (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination 我们可以看到 INPUT 链上有两条针对 2333 端口的入站规则，第一条 DROP，第二条 ACCEPT。两条规则会按照顺序执行，所以说当遇到第一条 DROP 的时候连接就被 DROP 了，第二条的 ACCEPT 则永远走不到。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:12:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"插入规则 看了上面两个，你可能会想：既然 -A 是 append，且匹配有顺序，那肯定支持一个 -I 的 Insert 操作。确实！ 我们首先看这个命令 iptables -L --line-numbers 的输出： Chain INPUT (policy ACCEPT) num target prot opt source destination 1 DROP tcp -- anywhere anywhere tcp dpt:2333 2 DROP tcp -- anywhere anywhere tcp dpt:2334 3 DROP tcp -- anywhere anywhere tcp dpt:2335 4 DROP tcp -- anywhere anywhere tcp dpt:2336 Chain FORWARD (policy ACCEPT) num target prot opt source destination Chain OUTPUT (policy ACCEPT) num target prot opt source destination 可以看到我配置了 4 条规则，分别是禁止 2333/2334/2335/2336 的 TCP 入站。现在我想允许这个 123.123.123.123 地址访问我的 2333 端口该怎么搞？最简单的就是在最前面插入一个规则。具体可以用这下面这条命令来在位置 1 的地方 Insert 一条规则（并且把其他规则挤下去）。 iptables -I INPUT 1 -s 123.123.123.123 -p tcp --dport 2333 -j ACCEPT 再来看一下现在的配置，发现新规则已经被放在最前面了： Chain INPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT tcp -- 123.123.123.123 anywhere tcp dpt:2333 2 DROP tcp -- anywhere anywhere tcp dpt:2333 3 DROP tcp -- anywhere anywhere tcp dpt:2334 4 DROP tcp -- anywhere anywhere tcp dpt:2335 5 DROP tcp -- anywhere anywhere tcp dpt:2336 Chain FORWARD (policy ACCEPT) num target prot opt source destination Chain OUTPUT (policy ACCEPT) num target prot opt source destination ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:13:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"修改规则 修改一条规则与插入规则没什么区别，只是将 -I 参数换成了 -R Replace 参数而已，后面的都完全一致。 ","date":"2024-02-22","objectID":"/posts/iptables-for-beginner/:14:0","tags":["iptables","Network","Linux"],"title":"写给新手的 iptables 使用说明","uri":"/posts/iptables-for-beginner/"},{"categories":null,"content":"0X00 什么不是加密 首先要先明确一个问题：我们日常使用非常多的 md5、SHA-1、SHA-256 这些 通通都不是加密 ，这些叫做摘要算法 。一串明文经过加密算法加密之后，是可以再次解密成明文的，但是摘要算法就不行了。 拿最常见的摘要算法 md5 举例：针对任何一个合法输入，md5 都会给出一个固定长度为 128 bit（32 byte）的输出，例如 md5(\"hello, world\") -\u003e e4d7f1b4ed2e42d15898f4b27b019da4。准确来说你是不能通过后面这串摘要值来反推之前的原文是什么的。因为严格来说这一串摘要值可以对应无限个不同的原文。那么摘要算法一般是拿来做什么用的呢？比较常见的是用来校验数据一致性，比如鸡太美同学编写了一个程序放到网上给大家下载使用，但是我们都知道文件传输过程中理论上可能会出错，所以他在上传之前就计算了程序的 md5 并且在传到服务器上之后再次计算 md5。如果两次计算的摘要值都是正确的，那就可以证明此次数据传输没有出现错误。接下来他把摘要值贴在了下载地址旁边，每个来下载程序的人也都可以在下载之后计算摘要值以保证下载没有出错。 其实不只是防止文件传输出错，还能增加一定的安全性，比如在文件被偷换之后，可以通过计算摘要值来确定文件是不是自己想要的那个。 摘要算法既然是对任意输入都输出一个固定长度的摘要值，那么自然就会出现「撞车」事故。换句话说就是：不同的输入内容得到了相同的摘要值。具体的可以去搜索md5碰撞来查看相关资料。 更离谱的是 BASE64，它只是编码 ，对编码稍稍了解且有一定敏感程度的人一眼就能看出一段文本是经过 BASE64 编码的。如果你拿它「加密」的话，别人一秒钟不到就「破解」了 🤣 为什么说 BASE64 编码稍稍敏感一点的人就能看出来呢？因为 BASE64 编码过后，生成的内容是 26 个小写字母 + 26 个大写字母 + 10 个数字 + 加号 + 斜杠，总共是 64 个字符，所以被称为 BASE64。如果一大串文本全都是由混在一起的数字字母加号斜杠组成，尤其最后又追加了一两个等号的情况，那可以 99% 认为它是经过 BASE64 编码的了。 比如下面这段文字，真的很难看不出来： aGVsbG8sIHdvcmxkIQ== 0X01 有哪些常见的加密方式 那么我们真正常用的加密算法是哪些呢？AES、RSA、DES、3DES、ECC 等都是比较常见的加密算法。这些算法按照加解密的密钥来看可以分为对称加密和非对称加密，这里分别简单介绍一下两种加密的特点与区别。 对称加密理解起来很简单，比如你在压缩的时候用 zip 命令设置了密码，那后面解压的时候还是要用相同的密码才可以。也就是说只有一个密钥，加密解密都用它，这种方式是我们日常使用最多的。 非对称加密则会分成两个密钥，分别是「公钥」和「私钥」。通过公钥加密的内容，只能 通过私钥解密，通过私钥加密的内容也只能通过公钥解密。这样你生成一对密钥之后，将私钥保存好就可以把公钥公开出去了。别人想跟你加密通讯的话就用公钥加密内容发送给你，即使数据被别人拦截了，即使他有加密用的公钥，也无济于事。 具体非对称加密是怎么实现的，最常见的就是「超大质数相乘」。质数我们都知道，两个超级大的数相乘计算他们的积对于计算机来说是非常简单的工作。但是给一个超级大的数让你去计算它是由哪两个质数相乘得到的，就比较麻烦了。 过多解释就要涉及到数论的知识了，我比(ye)较(bu)懒(dong)，所以到此为止吧。 特点 对称加密 非对称加密 密钥数量 单一密钥（加密和解密相同） 一对密钥（公钥加密，私钥解密） 速度 快 慢 安全性 较低（密钥共享风险） 较高（公钥可公开） 应用场景 数据量大的加密（如文件加密） 密钥交换，数字签名（如HTTPS） 密钥管理 较难（安全地共享和管理密钥） 较易（公钥可公开分享） 示例算法 AES, DES, 3DES RSA, ECC 由于非对称加密是一个比较慢且消耗性能的操作，所以不少时候会选择用非对称加密的方式交换对称加密用的密钥，然后接下来用对称加密 🤣 这里用一个非常常见的例子来说明非对称加密：ssh 协议通过 public key 登录。我们都知道用 ssh 登录服务器的时候输入用户名密码就可以，而且还可以配置免密登录，这里的免密登录其实就是利用了非对称加密实现的。 0X02 签名？ 与非对称加密一起讨论的还有签名 ，那签名又是什么呢？现实生活中的签名是用来表示「这些内容是我所认同的」，比如你在合同最后签上名字这种操作。计算机中的签名也是这么个用法，当你将公钥公开出去之后，就可以通过私钥加密的方式来「签名」。虽然你的公钥所有人都有，也就意味着所有人都能解密来看内容，乍一看好像跟你直接不加密也没什么大差别。但是核心就在于「公钥能解密的内容一定得是由私钥加密的」，所以这样别人就没办法伪装成你对外发送消息了。 签名的另一个用途是保障数据的一致性。假设 Alice 要给 Bob 发送一段数据，这段数据要在互联网上传输就意味着存在被篡改的可能。此时 Alice 将数据本体算一个摘要值例如 md5 出来，并将这个摘要值用私钥加密，最终追加到数据的结尾（这个就是签名），就可以了。这样一来即使这坨数据被人拦截并篡改了，篡改者也无法伪造签名，因为他没有 Alice 的私钥。最终这个数据传输给 Bob 的时候，如果 Bob 顺利通过 Alice 的公钥解密出了摘要值，再将原始数据自己算一遍摘要，如果两边对上了那就意味着数据没有被篡改。 ","date":"2024-02-04","objectID":"/posts/what-is-encrypt/:0:0","tags":["Encrypt","Security"],"title":"究竟什么是加密","uri":"/posts/what-is-encrypt/"},{"categories":null,"content":"0X00 Header 相信各位肯定都对 Python 中的基础、常见数据类型和数据结构比较熟悉了吧，不管是 int、float、string、bool 还是 list、tuple、set 用起来应该也都是手到擒来了吧。下面我们就来简单了解一下相对高级一些的 Python 内置数据结构，这些数据结构全都在 collections 的标准库中。 掌握这些数据结构虽然并不能让你「精通 Python」，但起码可以让你的代码更加 Pythonic，也能让你少写几行冗余的代码。 0X01 ChainMap 首先是 ChainMap，光是看名字大概都能猜到这东西的用途了：把 Map 组装为 Chain 嘛。在 Python 中最常见的也就是字典了，所以这个数据结构的主要功能就是将多个字典加在一起。 如果你手上有 100 个字典，现在你需要将他们加在一起，在没有 ChainMap 的时候大概率会写出这样的代码：先整一个 result 作为最终结果的容器，然后遍历这 100 个字典，一遍遍 update dict_list = [dict_0, dict_1, dict_2.......] result = dict() for d in dict_list: result.update(d) print(result) 现在有了 ChainMap 之后可以写成这样：将所有的 dict 都作为 ChainMap 参数传进取，它就自己帮你拼好了。 from collections import ChainMap dict_list = [dict_0, dict_1, dict_2.......] result = ChainMap(*dict_list) 有一点需要注意的是，使用第一段代码时由于是从前向后遍历 update 所以是用后面的值覆盖前面的值；而使用 ChainMap 方式则不会覆盖，换句话说就是「先入为主」了 可以用下面这段代码来测试这个说法 from collections import ChainMap a = {'a': 1, 'b': 2, 'c': 3} b = {'a': 111, 'c': 333, 'd': 444} res = dict() res.update(a) res.update(b) print(res.get('a')) res = ChainMap(a, b) print(res.get('a')) 0X02 Counter 类似这么个需求：有一个列表，里面全是单次，需要统计这些单次出现的次数。简单的写法基本上就是 word_list = ['hello', 'world', .....] count = dict() for word in word_list: if word in count: count[word] += 1 else: count[word] = 1 print(count) 但是这一大坨代码看起来就还是很麻烦，这时候如果有一个现成的 count 出现就好了。Counter 其实就是来解决这类问题的，我们来看一下用 Counter 解决这个问题是怎样的 from collections import Counter word_list = ['a', 'b', 'c', 'c', 'd', 'e', 'a', 'a', 'a', 'a', 'a'] count = Counter() for word in word_list: count[word] += 1 print(count) 可见 Counter 类其实就是个字典的子类，每次 key 不存在的时候就当它是一个 0。改用了 Counter 之后数数的过程一下就从 5 行变为了 2 行。如果说只是节省这三两行代码，那确实没什么必要这么大动干戈。Counter 还为我们提供了 5 个非常常用的方法： elements() 返回迭代器，每个 key 会出现 value 次，当 value 小于 1 时忽略 most_common(n) 返回最常见的 n 个元素，也就是 value 最大的 n 个元素 update(c) 与 dict 的普通 update 不同，这里是计算加法，也就是 c1.update(c2) 意味着 c1[x] + c2[x]（相同 key 的 value 做加法） subtract(c) 与上面的 update 类似，这里是减法 total() 算总量 这里还是给出这五个方法的例子 from collections import Counter word_list = ['a', 'b', 'b', 'c', 'c', 'd', 'e', 'a', 'a', 'a', 'a', 'a'] count = Counter() for word in word_list: count[word] += 1 print(count) # OUTPUT: Counter({'a': 6, 'b': 2, 'c': 2, 'd': 1, 'e': 1}) print(list(count.elements())) # OUTPUT: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'e'] print(count.most_common(3)) # OUTPUT: [('a', 6), ('b', 2), ('c', 2)] print(count.total()) # OUTPUT: 12 new_word_list = ['a', 'b', 'c', 'd', 'e'] new_count = Counter() for word in new_word_list: new_count[word] += 1 count.update(new_count) print(count) # OUTPUT: Counter({'a': 7, 'b': 3, 'c': 3, 'd': 2, 'e': 2}) count.subtract(new_count) print(count) # OUTPUT: Counter({'a': 6, 'b': 2, 'c': 2, 'd': 1, 'e': 1}) 0X03 deque 了解的同学肯定了解，栈和队列基本是继链表之后最简单的数据结构了。这个 deque 就是在队列的基础上有一个简单变种的版本：双向队列。队列本身规则很简单，就是单纯的先进先出（FIFO），基本操作也就只有两个：入队和出队。这双向队列就是将连个基本操作改成四个了：左右入队和左右出队。 当双向队列的长度没有收到限制时，两侧可以自由的入队出队；当长度受到限制时，队列爆满后继续入队就会导致另一侧强制出队。 Python 中的 deque 提供了下面这些方法： append(x) 从右侧入队 appendleft(x) 从左侧入队 clear() 清空队列 copy() 创建一份浅拷贝 count(x) 统计 x 出现了多少次 extent(iterable) 将可叠戴对象从右侧扩展进来 extentleft(iterable) 从左侧 index(x) 返回 x 在队列中的位置，找不到会 ValueError insert(i, x) 在队列中指定位置插入，队列爆满会 IndexError pop() 从右侧出队 popleft() 从左侧出队 remove(x) 找到第一个 x 并删除，找不到会 ValueError reverse() 反转队列 rotate(n) 向右循环 n 位，n 为负数时就是向左。向右循环的意思是：右侧出队一个元素，从左侧入队，也就是说d.appendleft(d.pop()) 还提供了一个只读属性：maxlen，为 None 就是无限大，只有在 deque(maxlen=n) 初始化的时候可以指定 0X04 defaultdict 顾名思义：这是一个带有默认值的字典。我们可以在实例化它的时候指定默认值是什么，这里用它来搞一个非常简单的 Counter 吧 from collections import defaultdict count = defaultdict(int) text = 'aaaaaaaabbbbbbbbbcccccccccc' for c in text: count[c] += 1 print(count) # OUTPUT: defaultdict(\u003cclass 'int'\u003e, {'a': 8, 'b': 9, 'c': 10}) 这里可以看到我们在实例化的时候给他规定了一个类型 int，它的默认值就是 0 了，同样还可以用 dict、list、set 这些类型。 0X05 namedtuple 如果我们想存下一台电脑的配置，那么用普通的一堆变量可以、用 dict 可以，甚至用 class 也可以。但是有这么一个更好用且更合适的东西：namedtuple，中文叫命名元组或者具名元组。 from collections import namedtuple Computer = namedtuple('Computer', ['CPU', 'GPU', 'Memory', 'Storage', 'OS']) macbook_14 = Computer('M2Max', 'M2Max', '32G', '1T', 'macOS') pc = Computer('i7-13900k', 'RTX 4090', '64","date":"2024-01-04","objectID":"/posts/python-collections/:0:0","tags":["Python"],"title":"Python 标准库之 collections","uri":"/posts/python-collections/"},{"categories":null,"content":"片头胡诌 要说 2023 年跟前几年比起来有什么最大的变化，那应该就是防疫了吧。连续了三年的疫情或者说防疫，终于在 2023 年到来的时候结束了。本以为到了新的一年能报复性娱乐 一把，但是低头看了看兜里的钱，抬头看了看当前的经济形势，一怒之下就…在床上翻了个身 😮‍💨 不过话虽这么说，毕竟防控还是没有了，今年出行便利程度直接拉满（其实只是回到了应该有的样子而已）。不然按照之前的操作，根本不敢想今年这几次出行得有多费劲。 去年定下的「2023 年度计划」也在这篇博客完成之际到了 Deadline，怎么说呢，俩字形容基本就是「稀碎 」，具体的就打散了放在这个年度总结里说一说吧。离谱的是往前看了两年的总结，好像每年都挺稀碎的，所以这次 2024 年的计划我打算换个形式来设计，争取一年后再次总结的时候不这么稀碎。 那么圆规正转，直接就开始吧 🎉 生活变化 今年生活上的一些变化很多都涉及到了个人隐私，确实不太方便写出来，简单来说就是又有了一个里程碑式的前进吧。硬要选一个来说的话可能就是今年八月份回了一趟家，说起来已经五年没有感受过唐山的八月了，更是已经快要十年没有感受过北方的春天了。 反倒是周边的朋友们一个个的变化都挺大的。认识二十年的发小今年结婚了，跑了两千公里回去参加他的婚礼；平时玩的很好的朋友，在我们的见证下领了证，明年应该也要去参加他们的婚礼了；大学认识的好朋友也即将在这个新春之际当上爸爸了。祝福他们接下来都会更加顺利，也希望他们都能担任好自己丈夫、妻子和父亲的新角色～ 果然随着年龄变大，身边人身边事也会有很大变化。前几年的时候，身边朋友最大的事无非就是分手了、恋爱了、换工作了之类的，今年突然间就变成了见家长、婚礼、领证、当爹 🤣。生活有变化才更有趣嘛，还好我是比较喜欢变化的～ 身体健康 身体是革命的本钱 ，不知怎么的今年这句话就时不时在脑子里转圈打滚翻个跟头还来个托马斯回旋。再加上今年莫名其妙就有很多焦虑和压力一点烦躁，尤其是下半年的时候，索性就暂停了当时计划的一切专业技能和兴趣爱好的学习，转而开始尝试运动。 没错，年度计划就是这么稀碎的 。 跟其他人肯定是比不了，但跟往年的自己比起来还是多运动了很多，也变强了不少的。除了每天骑车上下班的七八公里以外还搞定了 绕成都一圈的天府绿道 97km 骑行了几次（最快也就匀速 20+） 兴隆湖往返 50km 的骑行也有几次（虽然每次都被路人拉爆） 尝试了慢跑 5km 几次（菜鸡配速 6′30″，但是体重基数太大所以暂时放弃了） 学会游泳了（蛙泳刚能取掉浮板，25 米泳池往返要 1′40″） 徒步今年上半年去的挺多，下半年就突然歇了（最高是单边爬升仅有 1200 米） 今年比较明显的是徒步变少，当时雨季觉得上山危险，就一直没去，直到今天 🤣。明年的话争取多骑一骑车、把自由泳学会、徒步多选几个难度稍高的。 立几个 flag 骑行，超过 1000 公里（抛开通勤） 跳绳，超过 50000 次 游泳，超过 40 公里 徒步，达成 12 次，高海拔或者高爬升也搞一搞 平摊下来差不多是每月一次 100km 的骑行和一次徒步，每周一次跳绳和游泳。感觉还是挺饱和的，但是我就是比较喜欢定一些比较饱和的 flag 然后尽量冲，冲多少算多少。定一个底线的话，骑行跳绳游泳至少得五折完成吧。 学习提升 今年没怎么提升自己的专业技能，感觉完全学不下去。现在就是陷入了一个好像什么都懂点，但是什么都不太懂的境地。今年专业技能的提升也就仅限于工作上遇到问题之后的总结和经验了，明年必须要在专业技能这方面好好提升一下了。专业技能上的竞争力恰恰与常说的「木桶效应」相反，还是要在某些方面上明显强于平均，做到前 10% 甚至 1% 才能在对应领域有比较强的竞争力。所以明年打算在目前这个比较小的木桶基础上选那么一两块板子尝试加长一下 🤔 另外今年在学习上最大的一个点就是搭上了 GPT 的便车，或者说是搭上了 GPT 的火箭🚀。 GPT 的出现虽然没有很大程度上影响我的工作流，但却实打实的影响甚至改变了我的学习流程。以前找文档、看书、看视频课程，现在可以直接跟它提问。以前可能需要耗时半天的时间大致了解某编程语言或者工具，然后再逐步测试，最终可能要耗时一天才能完成一项工作，现在直接给 GPT 提需求，可能半个小时就搞定了。所以明年我应该会高强度的用它来学习了。 对待 GPT 还是需要警惕的：你问完，他答了，但是这并不代表你就会了他回答的也不一定对，需要自己有判断能力用 GPT 比看书看文档效率高，但并不代表就可以不看书也不看文档了要搞明白 GPT 应该怎么用，他擅长什么，GPT 不是全知全能的且他会犯错 兴趣爱好 说起来今年在兴趣爱好上投入的时间和精力要比之前更多了，除了很常见的爱好以外还接触了一些挺奇怪甚至有些邪门的爱好 ，包括但不限于： 学摄影，目前水平是把打点计时器装快门上都比我拍的好 学修图，目前水平是不如手机自带的一键自动修图 搞骑行，目前水平是时常被电动轮椅超车 学游泳，目前水平是在一米五的泳池里淹不死（应该吧） 学飞牌，例如飞牌打易拉罐，目前水平：万里挑一，即飞一万张能打中一张 摇色子，用色盅把八个色子给摇一摇摞起来，这个还行，确实也比较容易 无线电，没错我今年考了个《中国无线电协会业余电台操作证书》，目前水平：什么都不记得了 学魔术，目前已经学会一半了：脑子学会了，手还不会 明年除了继续要学摄影、修图以外，还要把今年没搞懂的魔术和抛球给练一练。然后重点是打算 2024 年再学点比无线电、摇骰子更邪门 的东西，如果各位有什么邪门的好玩的事情，可以推荐给我， 工作情况 今年的工作情况就很稳定，非常稳定。唯一的变数就是两次突如其来的出差，而且两次出差都是因为一些很邪门的事情，我想来吐槽一下 🤣 第一次出差是去南宁的某客户那里，出差过去做什么呢？一个程序员出差能做什么呢？我要过去给服务器换一张网卡，没错，换一张网卡 😅。因为客户的流量变大，千兆网卡绷不住了，需要用万兆。客户非要让我们过去一个研发给他换网卡，没办法就只能安排我过去了 🤷‍♂️。 过去之后自然是凌晨开始干活，当时我心想：凌晨十二点到位，一个小时下机器换卡装回去，简单测试一下就回酒店，两点钟之前洗洗睡～结果客户现场把机器从机柜上拆下来用了一个多小时 ，没错就是一个多小时……然后我装好网卡、搞好驱动、简单测试耗时半个小时。后面机器上架后客户说要搞一个「全量功能测试」，就只能在现场硬等，最后搞到了凌晨四点才结束（其实上到机柜上也用了快一个小时） 🥲 第二次出差是长沙的某客户那里，出差过去做什么呢？部署我们的产品。本来挺轻松一事儿，结果因为客户不管做什么都要提申请、等审批，所以折腾了很久。而且说是上 K8S 的，但是客户现场不仅没有 kubectl 甚至都没有 Linux Shell 可用，全程都在他们集团自研的一个 web 平台上去操作。重点是他们自己好像还是第一次用这玩意儿，相当于让我替他们摸索他们的系统 🥲。 不过两次出差因为都是省会城市，也就都趁机去了一趟各省的省博，算是难受的出差中的一点点调剂吧。说起省博，当时我去湖南省博是压着开门时间去的，我刚一进去又没注意参观线路指示，直接冲到了最后一站「辛追夫人遗体」馆，当时馆里除了工作人员就我一个人，得以安静认真的参观了好久。两个省的省博相比起来，湖南省博确实做得更好一些，不过广西省博也值得一去～ 出游出行 2023 毕竟没有疫情了，今年的出行也是比较多的，应该是我这快三十年生涯里出行最多的一年了 🤔。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:0:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"西昌-邛海-泸山-烤肉-蓝花楹 今年春天去西昌玩了一圈，也是唯一一次没有逛城区，没有去商场的出游。去的这几天算下来最值得推荐的是：邛海、泸山、烤肉、蓝花楹。 邛海：没有过多可说的，是西昌的名片了。本质上邛海是个内陆湖（四川第二大的淡水湖），但是确实挺大的，再加上人工沙滩，确实乍一看有一种看海的感觉～ 泸山：⚠️注意这个是泸山，不是「飞流直下三千尺」的的那个庐山。泸山对我最大的吸引力并非山顶的景色，而是上下山的过程。上山有单人单座的全景敞篷索道 ，下山有超级大滑梯 。这些东西对于小朋友来说可能有些幼稚，但是对成年人来说刚刚好 🤤。 泸山就在邛海隔壁，有时间的话还是建议找一个上午过去玩一下的。 烤肉：西昌的烧烤没得说，吃就行了，我们随缘找一家新开业的店，就已经暴杀成都烧烤 了。 蓝花楹：如果你打算春天去西昌，那么强烈推荐你去一趟航天北路，那边成片成片的蓝花楹，真的很好看 🌸。 要说这次旅程有什么遗憾的话：虽然知道这很难，但我想去看火箭发射🚀 ","date":"2023-12-31","objectID":"/posts/2023-summary/:1:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"西安-不夜城-北方面食-兵马俑-地铁icon 西安那次去的挺突然的，周五下午出发周日晚上就回来了 😬，主要就是去吃东西和逛博物馆的。 大唐不夜城：人人人从从从众众众… 人真是太多了，而且遍地都是汉服（还是叫古装吧）小姐姐，一不注意甚至都以为自己穿越了。 北方面食：有一说一，吃面还得是北方的面，虽然是从来没吃过的种类，但是就总觉得很熟悉 🤣。早饭也是一样，虽然有些单品没见过，有些搭配没试过，但就总觉得莫名的熟悉。常年身在南方的北方朋友，去西安玩应该都能吃的习惯～ 兵马俑：这是到西安最重要的一环了，介绍我就不说了，分享个经验吧。建议找一个讲解员，否则自己乱看是很难看得懂的，尤其是几个朋友一起去的话，可以拼一个讲解员，就很划算。最重要的是讲解员一定要到验票之后再去找，外面很多邪门歪道的讲解，不仅水平不行，价格还高 😮‍💨。检票进去之后会有一些穿着统一制服的、佩戴工牌的、不会像卖保险疯狂追着你的讲解员，就找他们，他们是专业的。 地铁：西安给我印象深刻的一点是他们的地铁，给每一站都设计了一个 icon，都是那种画像砖一样的风格，并且都有该站的特色，就很棒。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:2:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"唐山-南湖-抗震纪念馆-渤海湾 虽然在唐山生活了二十多年，但这是我人生中第一次以游客的视角在唐山玩。毕竟唐山是个实打实的工业城市，所以如果你打算专程来唐山玩，那我只能说大可不必。不过你如果是来北京天津秦皇岛，有多余的时间来玩一玩的话那还是可以的，高铁分分钟就到了～ 南湖：很早以前这里是开滦集团的煤矿，因为大量采煤的原因再加上 76 年的一个大地震导致严重塌陷，什么都做不了，垃圾成堆污水横流。后来经过治理，在上面修建了现在的南湖公园，后来还办过一次世界园艺博览会。之前也去过几次南湖，那时候就觉得只不过是个比普通公园大一点的公园而已，后来随着文旅产业发展，现在也挺值得一去了。今年去的时候公园都还在搞各种活动，古装巡街、游戏挑战、活动集章、手工集市这些。 抗震纪念馆：没有什么过多要说的，每次去的时候看到有家长带小朋友过来，就觉得很棒。 渤海湾：生长在一个沿海城市，二十多年才第二次去海边的应该就是我了吧（第一次是小学时候组织去李大钊故居参观，顺便去了趟海边） 🤣。不过唐山的海岸最优秀的肯定不是风景，而是皮皮虾、梭子蟹这些东西，来的时机刚好的话还是能吃到很多便宜又好吃的海鲜的～ ","date":"2023-12-31","objectID":"/posts/2023-summary/:3:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"孟屯河谷-露营-星空-银河-鸡太美 今年一大早就买了一颗二手 APS-C 11mm F1.8 的镜头，就指望着到了夏天能痛痛快快拍星空银河。结果今年的星空银河计划只能算是完成了一半（因为只有两张图看起来能凑合）。 这次露营选的是我之前第一次露营的孟屯河谷，这里海拔低、光污染小、设施相对完备、徒步难度低。如果有成都的小伙伴想试试露营、看星空的话，这里是个不错的入门场地。露营前就用巧摄看好了银河升起的时间，定好了凌晨三点的闹钟就先睡下了。等被闹钟叫醒之后就带上相机三脚架冲出去开拍。不过因为太冷了加上凌晨三点的脑子不清醒，再叠上上白天没有好好找地景的多重 buff，最后导致只拍到了这两张勉强说得过去的照片。不过好在后面卖镜头的时候，总共没亏几块钱 hhhhh 另外，那天晚上我才知道，原来鸡哥的梗并不是只有我们在玩。小朋友甚至专门学过鸡哥的舞蹈！！！ ","date":"2023-12-31","objectID":"/posts/2023-summary/:4:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"四姑娘山-长坪沟-双桥沟 平平无奇的一次四姑娘山之旅，贴几张照片留作纪念吧～ 分享万物 又到了这个奇怪的部分，如果你也感兴趣的话就看一看吧～ 这部分应该是最能体现我自己爱好的部分，也是最能满足我表达欲的部分了，所以可能看起来会乱七八糟的 🤣 ","date":"2023-12-31","objectID":"/posts/2023-summary/:5:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"电子产品 ","date":"2023-12-31","objectID":"/posts/2023-summary/:6:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"📷 Sony A7C2 摄影水平不行，全靠器材来凑说的就是我了😭 不过之前的 Sony α6100 也用了三年多了，一直苦于：屏幕太烂、没有防抖、高感不行、直出太丑、翻转屏不好用、操控不好等等这些问题。今年干脆就一咬牙一跺脚，换上了心心念念的全画幅（器材党实锤了）。有一说一这台机器外观是真的好看，性能也是真的很强，对我来说肯定是溢出了的，这次再拍不好照片就只能怪自己了。 全画幅的高感真的很棒，之前 α6100 的时候 ISO 开到 3200 就不敢上了，现在全画幅 12800 配合降噪都能一战； 防抖真的很香，之前手持拍照随时都要注意安全快门，毕竟没有防抖，随便一个小幅度晃动照片就糊了； 屏幕也是真的好太多太多了，之前的屏幕简直没法看，现在的屏幕虽然不是一流水平，但起码能看了； 终于有一个好用的翻转屏了，之前的 α6100 只有一个上翻屏幕，虽然腰平取景用着也不错，但是遇到竖构图的时候就完了，相机太高太低都会导致看不到屏幕。现在的翻转屏各个方向都能看到了； 操控也好很多了，比 α6100 多了整整两个转轮，现在即使是在纯 M 档也能随便调节参数了。并且额外增加了「拍照/视频/S\u0026Q」模式切换拨杆，切换到视频模式的时候不用再额外选一次了； 直出终于能看了！之前的 α6100 虽然也带「创意外观」，但是基本不可用。而 A7C2 自带的 FL/PT/VV 这三个创意外观，真的太实用了。以后索尼也是可以直出照片的机器了； 手机传图也可用了，α6100 真的慢，慢到我用过两次之后就再也不用了。现在 A7C2 的速度很快，完全是可用的程度了； 可能由于我之前的 α6100 真的太丐了，导致 A7C2 上很多功能我都觉得很高级。包括但不限于：安全快门设置、无损压缩 RAW、AI 对焦等等等等 至于价格差不多为什么没选 A7M4 呢？理由很简单，A7C2 相比 A7M4 阉割的内容：1/8000s 的机械快门、取景器、散热性能、双卡槽+CFA 卡这些都不是我最在乎的。勉强说的话我还是眼馋 CFA 卡槽和更好的取景器，但是相比起来更漂亮的外观、更轻更小的机身和 A7R5 下放的 AI 对焦是我更在乎的～ 不过 α6100 当然也不是一台差机器了，再它的价位区间内如果是只拍照的话还是很有竞争力的。如果各位预算三千想买一台二手相机来学习拍照的话，α6100 仍然是一个值得推荐的机器。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:6:1","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"📷 Sigma 35mm F2 既然换了全画幅相机，那之前的镜头就该挨个换掉了。目前手上的全画幅镜头里我最喜欢的就是这个 Sigma 35mm F2 了，它隶属于适马的 I 系列。虽然光圈没有 F1.4 那么大但也带来了体积小的巨大优势。如此小巧的体积，全金属的镜身，再加上优秀的外观设计和独立的光圈环，光是从外在上就已经爱不释手了。适马的镜头成像素质也没有什么好挑剔的，对我来说它也是一个几乎强到性能溢出的镜头。 没错，我就是那个没啥水平还喜欢学人家大师拿着 35mm 镜头拍照的🥬🐔 ","date":"2023-12-31","objectID":"/posts/2023-summary/:6:2","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"💻 MacBook Pro M2 Max 用三个词来形容这台电脑：猛、冷、持久。 随着自己平时编程和修图需求的提升，之前那台 Intel 的 MacBook 逐渐显出疲惫了。干起活来动不动就卡住，时常发烫，CPU 干到 99℃ 也见怪不怪了。再加上今年 AI 的突飞猛进，自己也想在本地跑一跑 AI 试试看，所以就给自己的生产力工具升了个级。 猛：是真的猛。之前用 Intel 的时候在 Lightroom 里 AI 降噪，一张照片一分钟上下，现在只要 10 秒左右；编译程序、打开大型程序、导出照片视频，甚至是 IDE 的自动补全都比原来快了太多太多。再加上 Apple 的混合内存，系统 32G 的内存甚至可以理解成是 32G 显存，在本地跑 Stable Diffution 都没什么压力了，甚至跑 LLM 模型的速度都在一个可以接受的范围内了。 冷：是真的冷。以前 Intel 时期的 MacBook，但凡干点重活温度就直接起飞，连续一小时维持在 90℃ 以上都不是什么稀奇事。现在是风扇干脆不转，日常看视频不转也就算了，平时用 IDE 搞一些小型项目、导出十几二十张照片它也不转。从前的电脑只有热和烫两种状态，现在这台只有冰和温两种状态 🤣 持久：是真的持久。14 寸的电脑，不插电，日常看视频修图写代码，能从早用到晚，肉测 10h 问题不大。 如果你也想搞一台，还想尽可能省钱可以参考我的这个方法：官翻机+员工折扣。 官翻机在苹果官网就可以找到，价钱比全新的便宜了不少，并且由于是官翻，所以但凡能看出来旧的地方苹果都会给换新，所以到手里和新的是没区别的。 员工折扣可以在淘宝上找人，一般来说需要付给他手续费，但是你能获得一个八五折 的优惠。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:6:3","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎮 PlayStation 5 我买 PS5 就是为了玩原神！ 感谢彭老板送出的 PS5 🎉 如果纯说机器不谈游戏的话，PS5 可能就只有两点能拿出来说道说道：手柄和存储扩展。 新手柄的自适应扳机真的太棒了，我觉得是所有手柄、游戏都应该兼容的功能！拉弓射箭的时候它能模拟弓的力量，按的幅度越大它给的阻力越大；开枪的时候它能模拟枪械，每一发子弹都能精准的体现在扳机键的回馈上；再加上手柄的震动，如果说手柄震动分四档，那么应该是 Dualsense \u003e Joy-Con \u003e XBox Controller \u003e Other。不过这个靠嘴是说不清楚的，还是得亲身体验了才知道。 扩展存储是针对 XBox 的，如果你也有一台 XBox 尤其是 XSS 的话应该能明白我在说什么。XBox 的扩展需要买官方的扩展存储卡，1TB 卖 1000 块…然而 PS5 是可以用随便一个 PCIE 4.0 的 SSD 来扩展的，托长江存储的福现在 1000 出头能买到 4T 的 SSD 了。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:6:4","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"书 今年看的书比较少，能够拿出来分享的就更少了，不过还是勉强挤出来了三本。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:7:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"📚 三体 三体是从去年末才开始看，今年初读完的，其实去年的总结中就提到了三体的。全套读完之后的个人感觉怪怪的。如果说评级啊的话，我个人认为 《三体 1：地球往事》非常好；《三体 2：黑暗森林》极好；《三体 3：死神永生》就有点怪怪的了，而且越到后面越怪。 前两本中三体人、三体游戏、智子、展开、水滴、思想钢印、面壁者和破壁人、威慑、黑暗森林法则等等都非常非常吸引我，有源源不断的内容在刺激着我。但是到第三本的时候就总觉得看起来稀里糊涂的。不过在看了豆瓣和知乎上的一些评价过后我怀疑是自己理解的问题，打算明年再来读一遍三体，至少再读一遍死神永生。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:7:1","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"📚 洞穴奇案 这本是罗老师推荐的法律相关的书（应该不会有人以为是罗永浩吧🤔），作者是哈佛法学教授 Lon L. Fuller，讲的事情比较简单但是足够深刻。 故事设定在一个虚构国家，一群探险者在坍缩一个山洞时被困住了。被困一段时间后通过无线电成功与外界取得了联系，得知救援队赶到的时间后发觉食物绝对不足以支撑他们生存到那一刻。于是有个人站出来提议抽签选择一个倒霉蛋，把他吃掉，牺牲一人换其余人的生还。但是在抽签开始之前提议的人退出这个抽签活动，不出意外的是出了意外，最后被抽中的就是他。最终他们被救出后，幸存的探险者因故意杀人罪被起诉。 整本书就是围绕着这个案件展开的，里面有很多个大法官给出了自己的看法。如果你对法律、道德相关的话题比较有兴趣，那强烈推荐读一读这本书。并且建议在读完序章（也就是各位大法官开始分析之前）之后冷静下来自己先分析一下：这些人究竟有没有罪，究竟有什么样的罪。自己心里有一个想法之后再去看大法官们的分析，体验会更好。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:7:2","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"📚 蚱蜢：游戏、生命与乌托邦 严格来说这是一本哲学类的书，它讨论的核心也都写在书名上了：游戏、生命与乌托邦。整本书是通过一系列的故事展开的，核心人物是一只蚱蜢。书中作者为「游戏」下了一个定义，需要满足下面三种特征 游戏是自愿的 也就是说某加拿大电鳗现在正在踩缝纫机，这不能算是游戏 但是你跟你的朋友每人一台缝纫机，在比拼谁踩得快，那就另说了 游戏包含目标和规则 你在路上随缘乱逛，这当然不能算是游戏 但是你要求自己只走盲道，并且每次跳过一个格子，那就可以算了 目标必须通过规则规定的手段来实现，即使存在更有效的手段 打篮球是为了把球丢进框里，这没有问题 但是我搬个梯子过去，哐哐往里扔呢？这就不行，因为有规则会限制一些邪门的极有效率的手段 如果你对标题中的游戏、生命、乌托邦之一感兴趣，尤其是都感兴趣，那么还是非常推荐你读一读这本书的。不过这本书毕竟是 1978 年的，它讨论的游戏自然也是广义上的游戏，而非电子游戏。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:7:3","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"游戏 今年是拥有 PS5 的一年，所以除了王国之泪以外就几乎一直在 PS 平台上打游戏了。年内玩的这些值得推荐游戏我按照自己的喜好顺序列来分享吧。 另外，希望明年的黑神话悟空能让大家满意，求求游戏科学了 🙏 ","date":"2023-12-31","objectID":"/posts/2023-summary/:8:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎮 塞尔达传说：王国之泪 得知塞尔达有新作后的我：🤩 得知塞尔达基于旷野之息的我：🤔 王国之泪解禁前一个小时的我：🤤 玩上王国之泪之后的我：🥳 当时听说塞尔达新作是在旷野之息的基础上进行改进，我整个人是错乱的。心想旷野之息那么牛逼，还能在这基础上改进？能改进啥？4K 光追 稳定 60 帧率？ 然后青沼英二带着他的团队狠狠的给了我一个大逼斗🤣。这游戏不需要我吹，也不需要我介绍，就这样吧。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:8:1","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎮 Cyberpunk 2077 没有人会记得那些按时发售的垃圾游戏 作为一个 Cyberpunk 2077 的首发 1.0 PS4 版玩家， 当时硬顶着各种烂优化各种 BUG 打完了游戏。当时觉得这游戏虽然首发版本不行，但是底子肯定是好的，不过没想到再玩这游戏就是三年后了。随着今年 2.0 版本的更新和 DLC 往日之影的发布（没错我又预购了）再配合我彭送的 PS5 加持，就又重回夜之城了。按理说三年前如果 CDPR 能把这个程度的 2077 拿出来，也不会被锤成那个鬼样子吧 😮‍💨 PS5 的性能提升加上这次的版本更新，让游玩体验有了一个巨大的飞跃。最大的提升是这次技能树的改动，改完之后的技能比原来好了一万倍。再加上义体和装备系统的改动，再也不用为了数值去穿那种贼丑的衣服了。另外这次的付费 DLC 体量还是不错的，如果有玩 Cyberpunk 的还是推荐买一下这个 DLC。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:8:2","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎮 对马岛之魂 这算是算是一个非常好玩的普通游戏，你要说他哪里做的极其优秀或者做出了何等的创新？那是没有的，但 Sucker Punch 就是把这么一个游戏做的非常的好玩。游戏的引导我觉得很棒，战斗很棒（超级棒），风景很棒，反正就是好玩。如果光是听介绍或者看视频的话，很有可能会把它当成一个土豆厂阿育的作品，但这都不重要，建议拥有 PS4/PS5 的玩家去尝试一下～ 另外说，我最喜欢的一点就是对马岛的战斗了，从某些方面上说甚至不比战神诸神黄昏差。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:8:3","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"玩具/爱好 ","date":"2023-12-31","objectID":"/posts/2023-summary/:9:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🚴 公路自行车 去年买了辆山地车，骑了一年多之后发现自己很少需要「山地」，反而是速度经常想提却提不上去，所以出掉了山地车换了一辆「杂牌二手公路车」，还进行了一波小型的改装～ 轮胎换成马牌 Ultra Sport III 手变头换成新款，去除螳螂须 换掉了前任车主的包浆坐垫 更换了 105 刹车 把车架上的网址重新喷漆 说到重新喷漆，真的是被逼的。本来车架上有个网址只是有些丑的，并不是什么大问题。但是一个朋友告诉我说：「据说这家公司倒闭了，然后我想看看官网还在不在，结果发现这个网址已经是个黄网了」……那没办法了呀，只能涂掉，总不能车架上印着黄网到处跑吧 😭 不过有一说一，碳纤维车架➕公路车变速系统，骑起来是真的轻快～ ","date":"2023-12-31","objectID":"/posts/2023-summary/:9:1","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🔫 USP 我很诧异，这玩意儿居然能在淘宝上买到。它外观上是 1:1 仿真的，枪身也是大量金属的（但是没什么杀伤力，跟小时候玩的五块钱的玩具枪一样），这要是小学有这么一个玩具，妥妥的称霸我们小区。因为它金属成分不低，所以上膛的声音特别真，极其上头。 如果各位也有兴趣想搞一个来玩的话可以联系我，我推店铺和老板微信给你～ ","date":"2023-12-31","objectID":"/posts/2023-summary/:9:2","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"📻 业余无线电/手台 这东西是去年被狗乔一的一个视频拉入坑的。当时听他说感觉有点意思，所以就直接报名了考试，然后学了一小段时间就考过了，难度并不算高。 如果你有一个无线电手台，那么当发生下面这种事情的时候，你就能实时与外界/队友保持联系 地质灾害 四川的业余无线电就是从 2008 年开始大力发展的，因为汶川地震时业余无线电爱好者们发挥了巨大的作用 山区徒步 徒步进山通常是没有信号的，用无线电交流不仅方便快捷，还不浪费手机电量 逛展 今年去核聚变就是跟几个朋友人手一台，找人沟通方便一万倍 爆发丧尸危机 这时候手机大概率是没法用了，如果你有无线电设备的话生还概率直接翻倍 ","date":"2023-12-31","objectID":"/posts/2023-summary/:9:3","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"电影 今年算是电影小年吧，起码对我来说是这样的，院线电影这一年下来也就觉得《流浪地球 2》和《奥本海默》非常棒。不过好在还有希望，一个是郭帆导演的《流浪地球 3》，一个是姜文的新片。 姜导啊，你能不能给个动静啊，上一部电影到现在都五年了啊 😭 ","date":"2023-12-31","objectID":"/posts/2023-summary/:10:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎬 流浪地球2 年度最佳，不说什么强过《星际穿越》，但是掰掰手腕还是完全不虚的。关于电影本身其实没有过多可说的，这里简单分享一个在知乎上看到的问题吧：「《流浪地球 2》里，要求五十岁以上的执行必死的任务，是否是道德绑架？」。下面有两条回答让人哭笑不得 你以为是要五十岁以上的出列？这是让五十岁以下的留下而已。 在这种场合下，没说「党员出列」就已经是很给面子了 希望郭导注意身体吧，前段时间看到照片感觉整个人都快皮包骨了，同时希望流浪地球 3 能继续保持高水准。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:10:1","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎬 奥本海默 幸亏实拍狂魔诺兰这次没有真的引爆一颗原子弹 诺兰的这部《奥本海默》我自认为是比上一部《信条》更好的，可能是因为我只能能看懂剧情片更喜欢这类不那么故弄玄虚的电影吧。电影的拍摄规格超级高，所以当时看的时候也特地选了成都为数不多的激光 IMAX 影厅去看的，体验真的是非常好了。如果你感兴趣但是还没看的话，那么很遗憾，只要过了热门上映期就几乎再也没有机会在 IMAX 或者 DolbyCinima 影厅观看这些电影了。类似的电影除了这个《奥本海默》以外还有诺兰之前的电影，很多也都是拍摄规格爆表的。另外还有一个跟诺兰这个胶片迷反着干的李安，他是什么新潮什么高级就整什么，上次在杜比影院看《双子杀手》真的是爽爆了（虽然电影本身普普通通，但是 2K ➕ 120帧 ➕ 杜比视界 ➕ 杜比多声道 真的太爽了！！）。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:10:2","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎬 布达佩斯大饭店 从来没有看过这么「好看」的电影。这个好看不是说作为一个电影的好看，而是说整个电影的画面几乎任何一帧都是美到爆炸的。看完电影的我已经打算把韦斯安德森的所有电影全拉出来看一遍了，这人就算不拍电影去搞摄影，应该也是顶级的吧 😭 ","date":"2023-12-31","objectID":"/posts/2023-summary/:10:3","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"其他 ","date":"2023-12-31","objectID":"/posts/2023-summary/:11:0","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"☎️ giffgaff giffgaff 这玩意儿是一个英国的电信运营商，我分享的则是他们的一张手机卡。我是在 Telegram 的某个群组里看到有人在卖，就私聊买了一张。不过现在已经失联了，需要买的小伙伴可以自寻方案，我在这里列一下这个卡的特点，看完应该就知道你自己是不是真的需要一张了 便宜：印象中二道贩子才收了我几十人民币，结果卡里还有忘记多少英镑的余额 无月租：只需要每季度（90 天）用它打个电话或者发个短信出来，消耗一点点话费，就可以保活 可用范围广：几乎任何需要海外手机号注册的平台它都可以，因为它是正儿八经的英国手机卡 抛弃 Google Voice：虽然我还没有抛弃，但是理论上可以不再用了，因为有些服务不能用虚拟手机卡注册，但是实体卡几乎都能用 通用：随便一个手机插上就能用，貌似只能走 4G，所以只要你手机支持 4G 就行 ","date":"2023-12-31","objectID":"/posts/2023-summary/:11:1","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"💳 FOMEPay FOMEPay 是一个虚拟信用卡平台，可以花 $10 开一张虚拟的美元信用卡，然后用来给各种不能通过国内信用卡付费的平台付费（或者你不想用国内信用卡的平台）。用这个平台有几个需要注意的地方 一般来说办理一张卡就够了，开卡费用是 $10 每次充值到信用卡中需要手续费，大概是 2%~5%，不高不低的 这个平台看着有点劣质，建议不要在卡里留太多钱 ，防止他们跑路 不过再怎么说，它也是个海外信用卡，如果你想拿来给 OpenAI 这类平台付款的话，还是很合适的。 ","date":"2023-12-31","objectID":"/posts/2023-summary/:11:2","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"✒️ Obsidian 之前用过很多笔记软件：OneNote、为知笔记、VNote、Notion、我来……但是最后还是放弃了他们，选择了现在正在用的 Obsidian，这里列出几个理由吧 Obsidian 是免费的，严格按照 License 说的话不是商业用途就是免费的 Obsidian 所有数据在本地，配合 iCloud 或者其他云同步工具，很放心 虽然我的梯子是 7 x 24h 运行的，但是还是 Notion 还是偶尔加载很慢偶尔弹出登录 Obsidian 的数据是 Markdown 的，即使哪天它挂了或者我要迁移笔记了，也方便的很 Obsidian 基础功能非常完善还非常克制，配合少数插件就能让我用的非常舒服 Obsidian 设计很质朴，本程序员表示非常棒 👍 Obsidian 的关系图谱每次看起来都是满满的成就感 ","date":"2023-12-31","objectID":"/posts/2023-summary/:11:3","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"🎮 核聚变 ⚠️这里的核聚变指的是一个游戏展会，应该不会有人以为是两个氢聚变成氦释放大量能量的那种核聚变吧 🤣 就是一个非常有趣的游戏展，虽然没有什么黑神话悟空或者 GTA6 这种东西，但是还是又大量的国产独立游戏在展出的。印象最深刻的是在现场玩到的一款游戏《黑墙之下》，我是实在不知道怎么形容这个游戏，但是莫名的有点上头。制作人（你也可以说是程序、美术或者随便什么，反正这游戏是他一个人做的）自己的描述是「魔改象棋+暴言+一点点随机和Meta」。 游戏在机核的页面我已经加收藏了，时不时上去看看，如果哪天能有 macOS 的版本了，必买之。 总结展望 就很神，去年我觉得写了个万字年终总结已经很离谱了，今年看了看最后的字数统计好像比去年还要多……明明感觉自己一年什么都没做，结果一到年底又写了这么多乱七八糟的东西，真的很神。 不管怎么说这一年也已经过去了，祝各位新年快乐吧～ 之前说希望 2023 年底的时候能对 2024 年整体有更强的信心，现在看来信心也不是很足。但是事在人为，不管有没有信心，都还是要过好这个新的一年。 最后给大家准备了一些🧧奖励看到最后的你和直接滑到最后的你 打开支付宝搜索口令红包即可领取，先到先得，有效期到 2024-01-02 傍晚～ 这个支付宝红包的口令就是没有口令的喔 ","date":"2023-12-31","objectID":"/posts/2023-summary/:11:4","tags":["Summary"],"title":"2023 年度总结","uri":"/posts/2023-summary/"},{"categories":null,"content":"0X00 header 首先，git 是没有 header 这个命令的:) 平时经常会用到一些 git 的用法，也有遇到过别人来问怎么实现某某操作，但是都太零散了并不成体系，所以这里就简单整理一下不做分类了。希望对不小心通过 Google 搜到该文章的你有所帮助（不会的有人能搜到吧🤣） 0X01 pull 与 pull –rebase 首先是搞清楚 git pull 和 git pull --rebase 的区别，我们学 git 第一天肯定就学了 git push 是推代码，git pull 是拉代码，那么这个 git pull --rebase 又是个什么东西呢？ git pull 本质上是 git fetch 加上 git merge 的功能，也就是说把代码拉到本地来再和本地代码进行一次 merge。这就意味着，如果出现冲突的话 git 会创建一个 Merge commit 来解决这个问题。 git pull --rebase 则是 git fetch 和 git rebase 的组合。使用的时候会先将本地比远端多出来的 commit 暂存起来，等将本地代码与远端代码同步后再将暂存的 commit 逐个应用。这样一来提交历史会更加整洁（因为没有额外的 Merge），但是需要注意的是使用 rebase 可能会造成 commit 历史发生变化。 吐槽一句，这个 rebase 被中文翻译成「变基」虽然确实合理，但真的好怪啊🤔 0X02 rebase -i 相信各位在践行 git-flow 工作流的时候一定遇到过这种情况：在自己的开发分支仓库中提交了代码，测试过后发现有问题，就提交 fix，再发现有问题就再 fix，最后合并到主仓库的时候发现有十几个 commit。这种情况呢其实直接将这十几个 commit 直接合并过去也不是不行，但是就会显得历史记录很混乱了，这种时候我就比较喜欢将几个修改同一处代码/同一个功能的连续的 commit 合并成一个。 这里可以看到这个我测使用的仓库在 create a 之后一直在 update a，那我现在想将他们合并就可以用 git rebase -i fba6a4d6b6364e9d3e485db366aeedb86c776904 命令将 fba6a4d6b6364e9d3e485db366aeedb86c776904 之前的所有 commit 合并起来（也就是所有的 update）。 使用这个命令之后进到这里，把不需要单独保留的 commit 的 pick 全部改成 s 也就是 squash，再保存退出。具体的 pick 是什么含义、s 是什么含义，在这个页面的下面有官方的文档解释，可以自行阅读。 下一个界面就是编辑新的 commit message，我这里只需要将多余的删掉就可以了。 最后可以看到，所有的 update 就都合并成同一个 commit 了。 0X03 cherry-pick cherry-pick 的用法就比较简单了。我们假设有一个仓库，它有一个主分支 master，还有两个额外给客户定制的版本分别是 branch_A 和 branch_B，他们都是从 master fork 出来的（这种场景还是非常合理的哈）。 这时候主线分支进了一个重要的 hotfix 是修复某安全漏洞的，那当然也要将这个改动应用到那两个定制分支上去。显然是不可能记住改动然后在两个分支上单独去改的。这时候就可以用到 git cherry-pick 了，首先要切到 master 分支，然后将修复的 commit id 复制下来，假设它是 123123abcabc，这时候再切到 branch_A 执行 git cherry-pick 123123abcabc，最后将弹出来的编辑器内容检查一下保存就可以了。这样就将这个 commit 的所有改动直接迁移到 branch_A 上了，branch_B 也是同理。 需要注意的两点：如果有多个 commit 需要迁移，记得按照提交时间从旧到新的顺序逐一操作cherry-pick 的时候需要注意可能会出现的冲突 0X04 commit –amend 各位一定都有手抖的时候是吧，比如刚刚提交的 commit message 发现有问题需要修改。这时候就直接使用 git commit --amend 命令，就可以直接修改最近一条 commit message 了，就像你刚刚用了 git commit 一样。 0X05 reset –soft HEAD~ 上面的命令是可以回退到编辑 commit message 的时候，而 git reset --soft HEAD~ 命令则是直接撤销掉最近一条 commit。在你的代码仓库中执行这条命令后 git 会将你最近的一条 commit 直接撤回，并将改动置为 stage 状态随时可以再次提交。 0X06 checkout PATH 最后一个是 checkout 在切换分支之外的功能，最常见的是例如 git checkout . 用法。这个用法可以将当前目录及其递归子目录中的所有没有在暂存区（没有被 git add 过的）的改动全部清空。 比如你在 dev 分支做了一些改动，用来调试一些问题，最终肯定是不想提交代码的，这时候就可以回到项目的根目录用 git checkout . 来清空这些不再需要的改动。 ","date":"2023-07-31","objectID":"/posts/git-tips/:0:0","tags":["Git"],"title":"写给 git 新手的 6 个小技巧","uri":"/posts/git-tips/"},{"categories":null,"content":"0X00 前言 在这里介绍 Shell 脚本编程和日常的命令行操作中最常用的 9 个命令，希望对读到这篇文章的你有所帮助～ 这是一篇基础得不能再基础的内容，如果你看完还能有两三个收获的话，那就一起来恶补 Linux 知识吧🤣 0X01 cat cat 命令应该是学习 Linux 命令时候最早接触到的命令之一了，自然不用多说什么，这里只提一个用法：如何 cat 命令创建一个文件。例如在脚本中我们需要创建一个配置文件，当然可以把配置文件跟脚本文件放一起，然后 cp 过去，但是如果你想用一个文件搞定的话（这样在看脚本的时候就知道配置文件是什么内容了，看着舒服改起来也方便）就可以这么操作 cat \u003c\u003c EOF \u003e xxx.conf [main] var = 123 var = 123 EOF 这段命令写在 shell 脚本中就意味着会把两个 EOF 中间行的内容作为文件 cat 一下，再接上后面的重定向，就把它写入到 xxx.conf 中了。不仅可以在脚本中，直接写在命令行中也是可以的。当然也可以不带重定向使用。 这里有一篇阮一峰的文章在相对详细的介绍什么是 EOF。 需要注意的是 EOF 只是我们约定俗成的 Enf Of File 的缩写，你非要用别的表示也是可以的（比如你的文本中包含 EOF 的话就有必要改用别的） 0X02 echo echo 命令用起来比 cat 还要更简单，只需要介绍三个简单的参数 echo -n 不输出尾部换行符 echo -e 启用反斜杠转义 echo -E 禁用反斜杠转义（Debian 默认是禁用的） 0X03 printf printf 是一个类似于 echo 但是比 echo 用法更细致的命令，非常类似于 C 里面的 printf。 printf \"hello, world\\n\" # 会有转义 printf \"%-10s\" xxxxx # 会输出内容并且长度不足 10 的用空格填充，左对齐 printf \"%10s\" yyy # 同上，右对齐 printf \"%-4.2f\" 12.345 # 与 C 的用法一致 printf \"%4d\" 12 # 与 C 的用法一致 0X04 head/tail head/tail 这两个命令最主要的用法就是 -n 参数，例如看某个文件的前 10 行可以用 head filename -n 10，末尾 3 行就是 tail filename -n 3，串起来也可以用 cat xxx file | head -n 30 | tail -n 10 就看到这个文件的 21 到 30 行了。 其中 tail 命令还有一个常用的参数是 -f，它意味着当读到文件末尾的时候不会停止，而是继续等待新内容。例如有一个源源不断产生新内容的日志文件 operation.log，我们可以 tail -f operation.log 来监控它，每当有新内容写进去的时候都会第一时间输出出来。 另外 tail -f 还可以同时监控多个文件，例如 tail -f aaa.log bbb.log ccc.log，不仅会实时监控文件中追加的新内容，还会用类似下面这种的方式区分开来 ❯ tail -f aaa bbb ccc ==\u003e aaa \u003c== ==\u003e bbb \u003c== ==\u003e ccc \u003c== ==\u003e aaa \u003c== aaa aaa aaa ==\u003e bbb \u003c== bbb bbb 0X05 wc wc 命令的是用来计数的，可以以 bytes/chars/lines/words 来统计。例如 wc -l filename 就是检查这个文本文件总共有多少行，wc -c 是多少个字节，-m 是多少个字符…… 通常用的最多的就是数行数了 0X06 grep 关于 grep 是没办法在这里讲清楚的，只能简单介绍几个基本用法，这里挖个坑，后面争取写一个关于 Linux 文本三剑客的文章介绍一下它们的用法。 grep 是从文本中查找子字符串的命令，最常用的用法就是 cat filename | grep xxxx 这种查找文件中内容的用法了，这里简单介绍几个常用的用法 grep xxx -A 3 其中的 -A 3 就是 after 3 line 的意思，也就是说顺带输出找到的内容的后面三行 grep xxx -B 3 同理，就是 before 3 line grep xxx -C 3 同理，哦不对没法同理。这里的 -C 有点像是玩梗，既然 AB 都有了那顺便也搞了个 C，这里的 -C 3 意思和 -A 3 -B 3 相同，意味着同时输出前后三行 grep -Rn xxx . 从当前路径递归查找含有 xxx 的行，并且同时输出该行所在的文件和行号（通常在代码库中查找代码会用到） https://grep.js.org/ 是一个在线 grep 工具，方便你练习 grep 命令 0X07 awk 同上，awk也是所谓文本三剑客中的一员，这里只是简单介绍一下最基础的用法 awk 是将一个文本内容逐行拆分的命令，ls -l 可以看到文件的详细信息，用 awk 可以=将其中的某一列提取出来，例如提取文件所属人 ls -l | awk -F ' ' '{print $3}' 就可以了。其中 awk 命令的参数分成两部分，前半部分是 -F ' ' 意味着我要指定一个分隔符，这个分隔符是空格；后半段是 {print $3} 意味着我要打印出第三段来（也就是文件所属人的这一列） https://awk.js.org/ 是一个在线 awk 工具，方便你练习 awk 命令 0X08 sed 继续同上，sed也是三剑客中的一员，这里简单介绍 sed 是一个修改文本内容的命令，具体的修改方法跟 vim 的替换内容很像，下面列举几个常常见的用法 sed \"s/hello/world/g\" filename 将这个文件中的 hello 替换为 world 并打印出来（不修改） sed -i \"s/hello/world/g\" filename 只是多了个 -i 参数，从打印不修改原文变成了修改原文但不打印 sed -i \"s/hello/world/\" filename 去掉了 g 就是每行 只替换一处 https://sed.js.org/ 是一个在线 sed 工具，方便你练习 sed 命令 grep 是 1973 被开发出来的, sed 是 1974, awk 是 1977，这也是为什么如今大家还会推荐新人学习命令行工具，因为这些命令行工具学一次真的能用一生 0X09 xargs 关于 xargs 我也打算单独写一个文章来介绍。哦不对，不用打算，我已经写了🥹 0X10 最后 Linux 下每个命令都有很多参数，即使是 head 这种看起来很简单的命令都有 5 个参数，tail 就更多了，后面的 grep/awk/sed 三剑客几乎都是可以单独写本书的水平。所以建议没事的时候可以看看 man 的文档，不一定要记住都有哪些参数具体怎么用，但是多看一看就会对其中一些用法有印象，下次再用到的时候起码知道哪个命令能用。具体怎么用再来看文档就好了～ ","date":"2023-07-11","objectID":"/posts/shell-text-process/:0:0","tags":["Shell","Linux"],"title":"Shell 中的文本处理【简】","uri":"/posts/shell-text-process/"},{"categories":null,"content":"0X00 介绍 不管对 sudo 的了解具体有多少，至少应该都用过 sudo 命令来临时将自己的非 root 用户提权至 root 了吧。不过 sudo 当然不只是将用户变成 root 的这么一个简单工具了，虽然它确实是将用户临时变更为 root ，但是页还是有不少其他更加细致的配置与选项。 sudo 是 Linux 中的一个命令，用于以管理员身份执行命令。它允许普通用户在不切换到 root 用户的情况下执行需要特权的操作，从而提高了系统的安全性和可管理性。sudo 可以通过配置文件进行自定义，以控制哪些用户可以以何种方式执行哪些命令。同时，sudo 还可以记录用户的操作日志，以便系统管理员进行审计和监控。 还有一个需要注意的就是，sudo 本质上是一个应用，并非最基础系统的一部分，它比 Unix 晚诞生了有近 10 年。这也就意味着并不是所有的 Linux 发行版本都会自带这个程序（比如 minimal 模式安装的 Ubuntu Server 就是没有的），如果遇到这种情况还是需要自己手动安装一下，不过这种多人共用的操作系统中没有 sudo 的情况还是很罕见的。 0X01 修改配置 ","date":"2023-05-24","objectID":"/posts/about-sudo/:0:0","tags":["Linux","Permission","sudo"],"title":"关于 sudo 命令也许你需要知道的","uri":"/posts/about-sudo/"},{"categories":null,"content":"说在前面 sudo 的配置文件是 /etc/sudoer，虽然它是一个纯文本但是这里要提醒一下 不要使用任何文本编辑器直接打开并编辑这个文件 。因为一旦这个文件被改出问题来了，那可就麻烦大了。在一个没有给 root 用户手动设置密码的操作系统中，用自己的用户把 sudo 的配置文件改崩了，这时候想再 sudo vim /etc/sudoer 的时候极有可能因为配置文件蹦了导致 sudo 不能正常工作，这也就意味着可能你甚至所有用户都永久失去了成为 root 的机会，任何需要 root 的操作就都没办法操作了。 那应该怎么改这个文件呢？答案是用 visudo 这个命令。不要慌，即使你不会用/用不好 vi 也没关系，这个工具的名字虽然带有 vi 但其实是根据你环境变量的 $EITOR 来决定的，你也可以用 EDITOR=/bin/nano visudo 来临时用 nano 进行编辑。 那用 visudo 有什么好处呢？主要是下面两点 visudo 带锁 ，当有一个用户正在编辑配置文件的时候你用了这个命令，你就会被通知并且没有办法修改该配置文件，直到对方保存退出了你才可以打开文件进行编辑。这样一来保障了多人同时编辑带来的问题。 visudo 有语法检查 ，也就是说每当你保存配置文件的时候都会进行一次基础的语法检查，防止该文件出现语法错误。如果单行配置写的不对，可能只会影响到对应的那行配置；但是如果语法有问题则会导致整个配置文件出错，问题就大了 最后关于如何编辑配置文件还有一个需要注意的是：使用 visudo 编辑配置文件后，保存并不会生效，要退出才会生效。这里可以注意观察一下，用 visudo 打开文件后直接保存，vi 的下面会提示将配置文件保存到 /etc/sudoers.tmp 了，只有当退出之后才会将配置写入到 /etc/sudoers 一个 Linux 系统上的小 Tips：man命令不仅可以看某个命令的文档，输入 man /etc/sudoers 试试吧～ ","date":"2023-05-24","objectID":"/posts/about-sudo/:1:0","tags":["Linux","Permission","sudo"],"title":"关于 sudo 命令也许你需要知道的","uri":"/posts/about-sudo/"},{"categories":null,"content":"授权配置 授权配置就是 sudo 的核心功能了，首先配置每行就是一条，格式是这个样子 USER/GROUP HOST=(USER[:GROUP]) [NOPASSWD:] COMMANDS USER/GROUP 首先是本行配置对应的用户或者用户组，如果是组的话需要在前面加上% HOST 表示允许从那些机器登陆的用户使用 sudo，通常的用法是 ALL 表示任何终端和机器 (USER[:GROUP]) 表示使用 sudo 可以切换的用户/组，通常的用法是 ALL，也就是可以切换到任意用户 NOPASSWD: 表示使用 sudo 时不需要密码 COMMANDS 自然就表示允许执行的命令了，ALL 则表示允许执行所有命令 其中用方括号括起来的部分是可选内容，可以不出现在配置中。 下面是一些常用的配置示例 # 允许 sudo 组执行所有命令 %sudo ALL=(ALL:ALL) ALL # 允许用户执行所有命令，且无需输入密码 USERNAME ALL=(ALL) NOPASSWD: ALL # 仅允许用户执行 echo, ls 命令 USERNAME ALL=(ALL) NOPASSWD: /bin/echo,/bin/ls # 运行本机的用户执行关机命令 USERNAME localhost=/sbin/shutdown -h now # 允许用户执行 /usr/sbin/ 下的所有命令，除了 /usr/sbin/useradd USERNAME ALL=(root) /usr/sbin/,!/usr/sbin/useradd # 允许用户以另一个指定用户的身份运行命令，且允许切换到另外一个指定的用户 USERNAME ALL=(server) NOPASSWD: ALL, (root) NOPASSWD: /bin/su - server ","date":"2023-05-24","objectID":"/posts/about-sudo/:2:0","tags":["Linux","Permission","sudo"],"title":"关于 sudo 命令也许你需要知道的","uri":"/posts/about-sudo/"},{"categories":null,"content":"Defaults 配置 # 指定用户尝试输入密码的次数，默认值为3 Defaults passwd_tries=5 # 设置密码超时时间 # 其中 -1 是立即超时，页就意味着每次 sudo 都要重新输入密码 # 0 是永不过时，登陆后输入一次密码就可以一直用 【不建议】 # 5/10/12 其他数字的单位是分钟，默认就是 5 分钟 Defaults timestamp_timeout=-1 # 默认 sudo 询问用户自己的密码，添加 targetpw 或 rootpw 配置可以让 sudo 询问 root 密码 Defaults targetpw # 保持当前用户的环境变量 Defaults env_keep += \"LANG LC_ADDRESS LC_CTYPE COLORS DISPLAY HOSTNAME EDITOR\" Defaults env_keep += \"ftp_proxy http_proxy https_proxy no_proxy\" # 安置一个安全的 PATH 环境变量 Defaults secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" 0X02 基本原理 要说 sudo 的原理（并不能很深入）得先从它的工作流程说起 sudo 会读取和解析 /etc/sudoers 文件，查找调用命令的用户及其权限 然后提示调用该命令的用户输入密码 (通常是用户密码，但也可能是目标用户的密码，或者也可以通过 NOPASSWD 标志来跳过密码验证) 之后，sudo 创建一个子进程，调用 setuid() 来切换到目标用户 接着，它会在上述子进程中执行参数给定的 shell 或命令 首先我们看 sudo 这个命令位于 /usr/bin/sudo，查看它的权限和所属人发现这个文件属于 root 用户并且权限是 --s--x--x，这里的 s 位表示的是 setuid。关于 setuid 可以看我的这篇博客中关于 suid 的介绍。简单的说就是任何人执行这个程序的时候都会临时将自己的用户转换为 root，最后在 root 用户的权限体系下再进行配置文件的读取与校验。 0X03 参考资料 深入理解 sudo 与 su 之间的区别 关于 sudo 命令的一些配置和使用技巧 ","date":"2023-05-24","objectID":"/posts/about-sudo/:3:0","tags":["Linux","Permission","sudo"],"title":"关于 sudo 命令也许你需要知道的","uri":"/posts/about-sudo/"},{"categories":null,"content":"0X00 简单介绍 想必看到这篇博客的各位肯定会经常工作在 Terminal 中吧，而且对自己稍微好一些的人应该也都会配置一下自己的终端环境，比较常见的就是 Linux 下装个 terminator 或者 macOS 下装一个 iTerm2 这种软件，然后再用 zsh 配合不同的主题和插件完善自己的体验。而且真正用过一段时间终端的人肯定都会有那种一个窗口不够用的情况，那么你可能要用到终端模拟器（terminator/iTerm2）的 tab 功能了，每次都额外开一个新 tab 出来，或者上下左右开始分屏了。 一切都很顺利，直到你开始频繁的连接到远端的服务器上去，然后发现自己习以为常的分屏和 tab 全都没有了，每次想再开一个远端的 shell 时都需要在本地开一个分屏然后重新 ssh 重新输密码，需要 sudo 的话还可能需要再重复一下密码。一次两次还好，次数多了肯定就麻了，这时候就是 tmux 大展身手的时候了～ tmux 本身是一个终端复用器，可以做到的功能包括终端横向纵向的分屏、多 tab 切换等等。 0X01 基本用法 要用 tmux 首先要有 tmux 才行（废话文学），有些 Linux 发行版本预装了的，如果没有的话用对应的软件包管理器装一下就行了，非常小且没什么依赖。 在使用之前先要区分一下 tmux 中 Server/Session/Window/Pane 这四个比较重要的概念： Server 服务，是最上级的，是整个 tmux 的后台服务，一般很少会直接操作它 Session 会话，是我们在终端敲下 tmux 之后随之启动的东西，类似于其他终端模拟器的一个窗口 Window 工作区，默认情况下新建一个 Session 就会带有一个 Window，类似于其他终端模拟器的 tab，也就是说一个 Session 可以创建很多个 Window Pane 就是最小一级了，默认情况下一个 Window 就会带有一个 Pane，也就是说一个 Window 下面可以选择左右上下分很多个 Pane，类似下面这张图（这个是我自己配置过的，跟原生配置不同但是概念是一致的） 图里看到的是一个 Session，我在其中创建了 4 个 window 并且在 window 4 上创建了 3 个 pane 然后打开自己的终端，输入 tmux 并回车就可以了，看起来和刚刚没什么区别。现在对着这个平平无奇的终端开始介绍一下具体的用法。 tmux 本身有非常多的快捷键，使用这些快捷键的前提是「进入 tmux 的命令模式」，默认按键是 \u003cCtrl\u003e + B ，熟悉 vim 的人可能比较容易理解。因为默认配置下进入命令模式之后界面上并没有什么提示，所以不放心的话可以先敲两次 ESC 回到普通模式再按触发键。 要搞成上面我截图的这种（指的是用法不是外观）是很简单的，首先我们先按快捷键 \u003cCtrl\u003e + B 进入命令模式，然后再按 b 就可以创建一个新的 window 了，连续创建 3 个之后大概会长成这样（截图是一个没有经过配置的 tmux 的样子） 可以看到下面有四个 window 了，其中打了星号的就是当前正在操作的 window。想切换不同的 window 的话就是命令模式下直接输入数字即可。 接下来在当前 window 下进行分屏，命令模式下 \" 是左右分屏，% 是上下分屏。在一个 window 下有多个 pane 的时候可以使用命令模式下的方向键在多个 pane 之间切换，存在光标的就是当前正在激活的 pane（通过绿颜色的边框同样可以快速找到当前正在激活状态的 pane）。 0X02 修改配置 「新安装一个软件之后第一件事不是开始用，而是先点开设置看看有什么好玩的东西」的人应该不止我一个喔。 tmux 的配置文件是 ~/.tmux.conf （其实还有其他会影响全局的，但是不建议改它所以这里并不打算告诉你），这里贴下我自己的一部分配置并在注释里简要说明一下，仅供参考。 # bind new prefix 修改前缀按键（也就是命令快捷键） unbind-key C-b # 取消绑定 Ctrl + b set-option -g prefix C-f # 绑定 Ctrl + f bind-key C-f send-prefix # using vi mode setw -g mode-keys vi # 使用 vi 模式 bind-key -T copy-mode-vi 'v' send -X begin-selection # vi 模式下的粘贴 bind-key -T copy-mode-vi 'C-v' send -X rectangle-toggle bind-key -T copy-mode-vi 'y' send -X copy-selection # vi 模式下的复制 # plugins # 一些插件，可以在 GitHub 上找到详细介绍 set -g @plugin 'tmux-plugins/tpm' # 插件管理器 set -g @plugin 'tmux-plugins/tmux-sensible' set -g @plugin 'tmux-plugins/tmux-resurrect' set -g @plugin 'tmux-plugins/tmux-continuum' set -g @plugin 'tmux-plugins/tmux-yank' # status bar # 截图下方右侧的状态栏 set -g status-style bg='black',fg='white' set -g status-interval 1 set -g status-left '#{?client_prefix,,}' set -g status-right '#[bg=default]#[fg=default]#(date \"+%R \")#[bg=black]#[fg=brightgreen]#[bg=brightgreen]#[fg=black]Shawn#[bg=brightgreen]#[fg=black]' # 字体原因可能现实不全，但是可供参考 # window bar # 截图下方左侧的 window bar set -g window-status-current-format \"#[bg=brightgreen]#[fg=black]#[bg=brightgreen]#[fg=black]#I:#W#[bg=black]#[fg=brightgreen]\" # 同上字体原因 set -g window-status-format \"#[bg=default]#[fg=default]#I:#W\" # pane border # pane 之间的分割线配置 set -g pane-border-style fg='gray' set -g pane-active-border-style fg='brightgreen' # window and pane index # 违背祖宗之法的从 1 开始编号 set -g base-index 1 set -g pane-base-index 1 # hotkeys # 配置一些快捷键 bind h select-pane -L # 继承自 vim 的 h 为切换到左侧 pane bind j select-pane -D # 同上继承自 vim bind k select-pane -U # 同上继承自 vim bind l select-pane -R # 同上继承自 vim bind y resize-pane -L 5 # 选取 hjkl 上面四个按键，用于上下左右拉伸 pane bind u resize-pane -D 5 bind i resize-pane -U 5 bind o resize-pane -R 5 bind r source-file ~/.tmux.conf \\; display-message \"Config reloaded..\" # 修改配置文件后方便重载配置 set -g display-panes-time 3000 set-option -g mouse on # 启用鼠标操控 # split pane unbind '\"' # 弃用双引号分割 unbind % # 弃用百分号分割 bind - splitw -v -c '#{pane_current_path}' # 将 - 绑定为上下分屏（图像记忆，像是横着的一刀） bind \\\\ splitw -h -c '#{pane_current_path}' # 将 \\ 绑定为左右分屏（同上） # run TmuxPluginManager 插件管理器 run '~/.tmux/plugins/tpm/tpm' ⚠️ tmux 就像 vim 一样，你当然可以在自己的电脑上疯狂配置，但是请牢记：首先不论如何都要记得默认配置下的基本用法，否则当你远程到服务器上的时候必定会一脸懵逼；其次千万不要修改服务器上的操作相关的配置，否则你的同事可能会提刀来见你☠️ 0X03 命令速查 默认配置 系统指令： ? 帮助文档 d 断开会话 D 选一个 session 断开 C-z 挂起当前 session r reload 当前 session s 显示 session 列表并切换 : 进入系统 shel [ 进入复制模式，按 q 退出 ] 粘贴复制模式的文本 window 指令 c 新建 \u0026 关闭当","date":"2023-05-17","objectID":"/posts/why-you-need-tmux/:0:0","tags":["Tmux","Terminal","Linux"],"title":"你为什么需要会用 tmux","uri":"/posts/why-you-need-tmux/"},{"categories":null,"content":"0X00 基本内容 你应该听人跟你说过类似于「你看 2333 这个端口在没在监听」这句话吧，也应该听过「我可以 ping 通所以肯定不是网络的问题」这种话吧。虽然听的多了，但实际上它漏洞百出。 首先我们都知道端口号是一个数字，从 0 到 65535 其中 0~2013 叫 well-known ports 知名端口，也就是说通常某个端口就固定给某个服务使用，比如你见到 22 就第一反应该是 ssh 而非 MySQL，见到 80 就知道是 HTTP 而非 smtp。也正是如此，在 Linux 中你自己开发的程序平时监听 8080 没啥问题，但是想监听 80 的时候就要校验你的 root 权限了。 其次要知道端口是 IP 地址上的，并非电脑上的。也就是说当你电脑同时拥有 192.168.2.123 和 192.168.2.234 两个 IP 的时候，你可以启动两个 Nginx 分别监听他们的 80 端口。 最后要知道端口也是分类型的，即使是同一地址的统一端口也可以 tcp 和 udp 分开监听，也就是说你可以同时监听： 192.168.2.123:80 tcp 192.168.2.123:80 udp 192.168.2.234:80 tcp 192.168.2.234:80 udp 0X01 如何检查 通常来说有什么服务直接用就试了，不过有时候我们需要自底向上的检查问题，那么第一步就是检查服务端的监听是否正常，正常的话客户端的连通是否顺利。首先保证我们的 TCP 和 UDP 通信时正确的，再来排查上层的程序，否则可能你调了半天程序最终发现网络压根就没通。 ","date":"2023-03-31","objectID":"/posts/port-mini/:0:0","tags":["Network"],"title":"关于端口你需要知道的-迷你版","uri":"/posts/port-mini/"},{"categories":null,"content":"ss 命令 首先介绍一下用 ss 命令（socket statistics）来查看自己机器上的端口开放情况，比较常见的用法是 ss -apnl。 a 是所有 socket，包含了 tcp udp 的，也可以单独使用 t 或者 u 筛选 p 显示进程名和 PID 这些 n 不解析服务名，这样不仅速度更快而且也更便于筛选端口号 l 仅显示监听状态 ","date":"2023-03-31","objectID":"/posts/port-mini/:1:0","tags":["Network"],"title":"关于端口你需要知道的-迷你版","uri":"/posts/port-mini/"},{"categories":null,"content":"nc 命令 这个 nc 命令一般会随系统附带，没有的话需要安装 netcat 软件包。它可以快速绑定一个地址和端口并进行监听，也可以快速向指定 ip 和端口发送数据。 # 监听 tcp 0.0.0.0 2333 nc -l 0.0.0.0 -p 2333 # 监听 udp 0.0.0.0 2333 nc -ul 0.0.0.0 -p 2333 # 向 tcp 127.0.0.1 2333 发内容 echo \"hello, world\" | nc 127.0.0.1 2333 # 向 udp 127.0.0.1 2333 发内容 echo \"hello, world\" | nc -u 127.0.0.1 2333 用法应该很好理解，其中 -l 是 Listen 也就是监听，-u 是指 UDP（默认情况下是 TCP） 好了，现在可以和以前那个只知道用 ping 来判断网络通不通的自己说再见了👋 ","date":"2023-03-31","objectID":"/posts/port-mini/:2:0","tags":["Network"],"title":"关于端口你需要知道的-迷你版","uri":"/posts/port-mini/"},{"categories":null,"content":"0X00 介绍 奖杯系统这东西是从 PlayStation 上借鉴过来的，PlayStation 上每个游戏均会设置一些奖杯，当玩家达成奖杯对应条件的时候就会解锁该奖杯，从而给玩家的游玩过程以正反馈。XBox 上也有类似的成就系统，都是用来给玩家正反馈的。 我作为一个游戏玩家就很喜欢拿到奖杯的感觉，尤其是拿到了一个金杯、白金杯（虽然我还从来没拿过白金杯）然后看到只有百分之一的玩家得到了这个奖杯的时候。既然游戏制作人可以为游戏设置将被，那我们为什么不考虑把自己的人生作为一个游戏来给它设置一些奖杯呢～ 在开始之前要说明一下人生奖杯系统是什么又不是什么： 奖杯系统是成就系统，不是代办清单 奖杯系统是自我激励和正向反馈的，不是用来给自己压力的 奖杯系统是非常个人化的，不是有最佳实践的 0X01 我的奖杯系统 先来给大家介绍一下我自己的奖杯系统，它是用 Notion 的数据库功能制作的。细心的朋友可以看出来我整个 Notion 都是「游戏化」的，工作区叫做「地球Online」，核心页叫「初始台地」，专业技能的笔记叫「主修技能」，一些其他的非专业技能笔记叫做「辅修技能」，还有武功秘籍之类的东西。 不要问我为啥我一个奖杯都没拿到，这不是为了截图公开出来就把完成了的放在后面了嘛～ 我的奖杯系统算是比较简单的，目前只设置了：名字、描述、难度、日期和图片五个字段。 其中「名字」是尽可能参照之前玩过的游戏奖杯风格去设计的。我觉得用「永不消失的电波」作为奖杯名比「通过无线电考试」感觉更酷一点～ 「描述」则是对成就的具体描述，说清楚了如何才能拿到这个奖杯。比如光看「好机车噢」可能并不能知道怎么才能拿到，所以需要一个描述的稍微详细一些的字段 「难度」就很好理解了，我是把它分成了：青铜、白银、黄金、白金、梦幻五个档次。其中铜杯是稍稍努力就能轻松拿到的，银杯是七八成努力就稳稳能拿到的，金杯是全力输出后大概率可以拿到的，白金杯是全力输出后拿到奖杯的概率也并不很高的，梦幻则是全力输出也大概率拿不到的（比如我奖杯系统中的买一套 130+ 平米的房子😭） 「时间」和「图片」最好理解，就是拿到这个奖杯的时间和记录性质的一张照片或者截图。 0X02 你的奖杯系统 介绍完了我的奖杯系统该看看如何创建你的奖杯系统了～ 首先选择一个工具，比如 Notion、Excel、滴答清单、甚至最朴素的纸笔都是可以的。如果打算创建一个我这类的奖杯系统建议选择对表格支持的比较好的软件。 其次就该规划字段了，看看每个奖杯都有一些什么属性。通常来说：名字、描述、难度、加入时间、完成时间、奖励点、照片、感受、备注等等这种字段都可以加，完全看自己觉得需要些什么东西。 「名字」最好短一些，与奖杯内容关联强一些，有趣一些就更好了 「描述」也是精简一些，说清楚究竟完成了什么事情就能拿到这个奖杯即可 「难度」则可以从三档到五档分一分，当然你想用 0～100 的数字来精细化管理也是可以的 「奖励点」是我一直想设置，但最终因为麻烦放弃的。如果要设置的话，可以根据每个奖杯的具体难度设置一些奖励点，例如「通过英语四级考试 – 3点」、「通过英语六级考试 – 5」、「首次跑完一场半马 – 3」、「首次跑完一场全马 – 5」这样，然后再给这些奖励点一个倍数比如 100。这样当你完成半马之后就获得了 3 * 100 = 300 元的奖励资金，可以买个自己喜欢的东西来奖励自己～当然也可以针对性的对每一个奖杯设置一个奖励，比如「跑完一场半马 – 奖励自己一双xxx跑鞋」或者「自己的照片被XX杂志选中 – 奖励自己一颗新镜头」 好了现在奖杯系统设置好了，就差内容填充了。那么 什么样的事情适合作为奖杯加入其中呢？ 其实原则上小到「连续一周不迟到」，大到「三小时速登珠峰」都是可以的。但是如果太多极低难度奖杯的话，拿到奖杯的成就感会大打折扣，难度太高的话又会导致三两年也拿不到一个，最后都是让这个奖杯系统废掉。所以我认为比较合理的就是从「稍稍努力就能完成」到「全力以赴才有一点点可能」的这个范围内的事情，从简单到困难数量递减分布，这样每隔一段时间都会拿到一个奖杯有正向反馈，也会有高难度奖杯在吊着你让你更努力的生活去完成它。 那么现在你的奖杯系统已经准备好了，可以开始尽情的填充内容了～ ","date":"2023-03-01","objectID":"/posts/life-trophy/:0:0","tags":null,"title":"人生奖杯系统","uri":"/posts/life-trophy/"},{"categories":null,"content":"年度总结：今年摆烂，完了。 0X00 开篇 不开玩笑，又到了一个年尾巴。本以今年会是轻松的一年，结果却是异常的艰辛😮‍💨 千算万算肯定是算不到 2022 年对生活影响最大的仍是疫情。本以为今年就彻底解决疫情了，但奈何天不遂人愿，一直到今天疫情仍然与你我密切相关。今年我核酸做了一两百次、居家办公一轮又一轮、收集了两个省的黄码、喜提了两周的铁皮围栏隔离，最后几天还是没顶住，至今仍在干咳。 接下来就简单整理一下今年发生过的值得记录下来的一些事情吧🎉 写完发现长度有点离谱，又尽力缩减了一些，有些实在是觉得写了不舍得删就继续留下了，这可能是我最长的一篇年终总结甚至是最长的一篇博客了。 0X01 生活 今年的生活过的比较普通，依旧没有发生什么惊天地泣鬼神的大事。不过人的一生又能经历几次大事呢？这些零零散散的小事情一件一件拼起来才是绝大多数人的生活吧。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:0:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"智齿 首先是今年年初去拔了智齿，切身体会到了电钻在嘴里飞速旋转的感觉。很多人都说拔了一颗之后要缓很长时间才能去拔第二颗，但是我第一次就拔了同一侧的两颗，医生还说一周后来拆线然后可以顺便把另外一遍的也拔掉。我就严格按照医生说的在连续两个周末拔掉了三颗智齿 ，也没觉得有什么特别的🤣 不过第二次给我拔牙的一个医生不是很靠谱，拔完之后有一小块残留的牙齿碎片在里面，导致我流血不止，大半夜的还跑去医院急诊了一趟😮‍💨 如果说有什么经验可以分享的话，那就是真的没必要拔个智齿也往华西跑，只要在靠谱的三甲医院找一个职称靠谱的医生就可以啦。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:1:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"搬家 另外一个就是今年又搬家了，新房间比原来大了不少，尤其是这个视野非常不错，唯一烦人的地方就是冬天晒不到太阳🌞现在再也不用担心跟室友抢洗衣机、抢微波炉、抢阳台、抢冰箱了，确实还是要舒服很多（虽然成本也高了不少）。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:2:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"父母 还有就是今年父母又过来了一趟，这次在成都算是比较熟悉了，不像是刚刚毕业那段时间想去找个好吃的好玩的地方都不知道。这次还是带着父母去了些不错的馆子，去了都江堰和大熊猫繁育基地，而且因为有了驾照还可以租车短程自驾了一段。再加上换了更大的住处，总的来说比上次过来的时候体验上应该还是好了很多的。 要说有什么不太好的，那就是自己没有车，还是要跟父母一起坐地铁出行，而且去远一点的地方自驾还得租车，着实是不太方便。如果手头有一辆代步车的话，就能更方便去机场接送、去周边吃喝、去近郊出游了。还是要努力赚钱呀💰 ","date":"2022-12-31","objectID":"/posts/2022-summary/:3:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"做饭 最后就是最近又开始做饭了，感觉自己的「厨艺」终于像那么回事了。以前都是蛋炒饭、土豆丝、番茄炒蛋、辣椒炒肉这种难度一颗星的菜品，现在可以逐渐尝试火爆牛肉丝、干煸排骨甚至是拔丝 糖醋排骨这种两星甚至三星难度的菜了。现在挡在我做菜前面最大的障碍可能就是「刀工」了，经常把丝切成条、把丁切成块、把片切成坨。 0X02 学习 接下来就是今年学习的部分了，其实今年总共完成的学习任务并不多，但是也还算比较满意。其实我很好奇其他人在下班时间之后会留给自己多久的学习时间。自己下班不学习的话会觉得又在摆烂了，怪不得同事什么都懂就我像个废物；下班学习半个一个小时又觉得时间太短并没有什么用；学他两个多小时吧又觉得自己晚上时间完全不够用，也好想看看电影动画呀。 今年一定得把学习娱乐的时间给安排好，并且要严格执行，不能再这样反复横跳了。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:4:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"Golang 今年又双叒叕一次从头学了一下 Golang，并且完成了一个练手的小玩意儿，再加上今年的工作当中也用到了一些，所以这次应该算是入了一点点门了 🤏 我纠结了一会儿还是把这个玩意儿截图发上来了，各位不要嘲笑我🤪 ","date":"2022-12-31","objectID":"/posts/2022-summary/:5:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"网络 其次就是又一次的去看了计算机网络的书，这种越是基础越是根基的东西就越是这样，每次看完都觉得自己看懂了，然后再次去看又能感叹「喔！原来是这样！」 。起因是经常在工作中听到一些网络相关的词汇，结果自己对其又是一知半解的，越想越难受，所以干脆把书掏出来再看一看。 所以不论是网络、操作系统还是数据结构、算法这些，每当想要学习却不知道具体方向的时候就把他们拿出来吧，准是没错的。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:6:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"容器 最后就是年前这段时间在补的 Docker 和 Kubernetes 了，说起来也不怕丢脸，我虽然用了挺久的 Docker 了但是直到最近我才搞明白「容器的本质并非虚拟化，而是从操作系统中隔离出来的进程（组） 」。 今年的主动技术提升可能就只有这些了，再有就是不成块的，平时逐渐积累起来的一个个小点，在这里也就自然无法总结了。 0X03 工作 学习后面自然就是工作了，我对自己今年的工作结果来说算是满意的，虽然不优秀但也算是保质保量完成了。不过要说起工作效率，那是真的低。我就是那种，如果手头工作不忙不紧急的话就会慢慢悠悠得去搞，东瞟一眼西看一下的，不知不觉时间就过去了，结果多出来的时间啥都没干成。但是如果有一个紧急重要的工作交给我的话，我又会极度专注，甚至真的废寝忘食，一天下来忘记喝水忘记午休也不会去看手机，每每完成这种工作后又有一种强烈的精神满足感。不知道有没有人跟我一样😅 💪 明年还是要提升一下工作效率，不说多为公司创造效益什么的，就算是节省出来时间学习提升自己也算是对自己和对公司都好的事情呀，怎么都好过把时间浪费过去。 补充，在今年倒数第二个工作日，我玩砸了。 本来只是一个通过 csv 文件创建数据库数据的脚本，结果在复用之前旧脚本的时候忘记了老脚本存在删数据的逻辑😥 结果今天就三个人前前后后忙了一整天，才终于把数据恢复回来了。这个「心细」对我来说真的是太重要了，很多次工作上的失误都是由于不够心细导致的，明年要重点改进这里了。 0X04 出游 今年算下来有两次省外游和几次成都周边的徒步，虽然绝对数量上并不多，但比起我大学毕业前的 0 次旅游经验来说也是「海量」了 🐶 ","date":"2022-12-31","objectID":"/posts/2022-summary/:7:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"跨年 年度的第一次出游应该就是跨年了，跟女朋友两个人跑到理县去了，短程徒步加滑雪什么的。那次唯一的遗憾就是没有运动相机，所以没有录下来人生中的第一次滑雪体验。其实旅途本身没什么特别的，只是想放两张照片在这里而已，你猜为什么 😁 ","date":"2022-12-31","objectID":"/posts/2022-summary/:8:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"三星堆 另一次值得记录的是一次成都的周边游，跑到了广汉，看到了自己心心念念三星堆文物。如果大家有兴趣的话强烈推荐去一趟三星堆，真的太震撼了，不仅有历史书上的青铜树 还有新鲜出土的三四千年前的金面具 和其他文物。博物馆本身并不是特别大，但是所谓的「镇馆之宝」全部都是可以看到的，而且最酷的是有一个考古工作人员办公室也是可以参观的。游客本身与考古学者之间用玻璃墙隔开，然后游览区就可以看到考古工作者在认真工作，还能近距离观察到刚刚出土的最新文物。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:9:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"九顶山 还有去九顶山那次的露营体验也是非常非常棒的（其实也没有，我女朋友到了山顶就开始高反，日落晚霞云海星空她是一样都没看到😮‍💨）。只是我看到路线比较简单就背了很多很多东西上去，导致上山过程还是有点难受的。所以说不管多简单轻松的徒步，还是要轻量化自己的装备 呀。 最终露营地大概海拔 3800 米，虽然上到露营地之后已经累得快要不行了，但是看到那么漂亮的景色还是忍不住抱着三脚架冲出帐篷去拍照。这算是我今年看到的最漂亮的景色了，唯一觉得遗憾的就是当天晚上的星空特别漂亮，但真的是太冷了，导致一张可用的星空照片都没拍到，就钻回帐篷里了。明年一定要弥补这个遗憾，拍到好看的星空🌃 ","date":"2022-12-31","objectID":"/posts/2022-summary/:10:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"重庆 这已经是我第三次去重庆了（毕竟这是离成都最近的大城市了 hhh）。这次依旧是无攻略无目的无所事事的三无出行，基本也都是逛解放碑、看洪崖洞夜景、吃火锅、逛文创街古街，这种日常操作。那为什么还是要提一嘴呢，当然是为了放两张照片了 🤪 ","date":"2022-12-31","objectID":"/posts/2022-summary/:11:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"长沙 印象最深的一次应该就是长沙游了，国庆期间和朋友五个人顶着被隔离甚至感染的风险还是冲到了长沙。这次长沙的体验不能用好坏来形容，只能说有点怪🤔 先说体验优秀的地方吧，从长沙落地出来感觉长沙的天气超级棒，比成都好很多。因为时间并不充裕，所以也都是去的一些热门目的地，橘子洲头、文和友之类的。不过几天玩下来对长沙印象最深的应该就是湘菜了，味道确实好，辣也是真的辣，我一个在四川自认为比较能吃辣的人，都觉得湘菜的辣是真的顶🥵 如果说哪道菜给我的印象最深，那一定是「香菜炒韭菜 」了，这个听起来很邪神的菜居然莫名其妙的很好吃。 还有长沙的茶颜悦色，是真的多，基本上站在市区的任何位置都能看到至少一家，最离谱的是还排着队。味道的话其实我没觉得多惊艳，但是绝对是中上水平，再加上我特喜欢他们奶茶的少糖模式，不像很多奶茶一样齁甜，所以总的来说给个 8.5 分我觉得完全没问题。 最后就是同事推荐的她母校长郡中学门口的小吃街，有一家炸串还挺好吃的。没想到的是我只随手拍了一张炸串的照片，她就直接看出来我们在她学校门口还看出来我们吃的哪一家了，我照片里只有串没有任何环境啊，不愧是自称「南门口一姐」的女人 🤯 那体验差劲的地方呢？本来我们五个人一起出行，而且按规矩做落地核酸和每日核酸的，结果有一天突然码就黄了 。投诉一圈也不知道为什么黄的，就告诉一个「大数据赋码」，告知我们需要「三天两检」之后才能绿过来。觉得非常离谱的我们开始一大圈的投诉，最后是一个叫做「健康365」的公众号客服帮我们解决了问题。那是怎么解决的呢？既然是要三天两检才行，他就直接帮我复制了前一天的核酸结果，这样我就有了三天两检，健康码就绿了…… 毕竟这里是年终总结的部分，就不过多吐槽这个事情了，如果对具体过程有兴趣的话可以看另一位同行朋友写的博客：又去了一次长沙。 如果问我以后有机会还会再去长沙玩吗，我一定还会去的，毕竟湖南省博我还没去成🥺 ","date":"2022-12-31","objectID":"/posts/2022-summary/:12:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"其他 最后再强行贴上一张我觉得很好玩的照片 🤪 0X05 分享万物 这个模块是用来满足我个人分享欲的极其重要的点，打算分享一下今年买到的满意的东西、玩到的满意的游戏、读到的喜欢的书之类的东西～ ","date":"2022-12-31","objectID":"/posts/2022-summary/:13:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【电子产品】Eypc 7551p 试问哪个男孩子能拒绝一颗 32C64T 3.0Ghz 却只卖 1000 块的 CPU 呢？好了不开玩笑，我装这台机器单纯就是因为想要学习 K8S、MySQL 集群和网络知识等。毕竟 MacBook 单热管压着的 i7 再加上区区 16G 内存并不能跑的动几个虚拟机。所以我就干脆整了这个第一代 Eypc 处理器，再配上 4 根 16G 的 reg ecc 内存，前前后后也就三千出头就配上了 32核64线程64G内存 的服务器 🤓 ","date":"2022-12-31","objectID":"/posts/2022-summary/:14:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【日用品】喜德盛自行车 我这就是个纯纯的入门款，为了上下班代步，毕竟我们这个上班时间小区楼下的共享单车早就被抢完了。再加上之前买的电动滑板不让进电梯，所以就只能自己买一辆自行车喽。自从买了自行车之后每天上下班通勤加一起也就半个小时 ，还能在路上顺便买个早饭，还是挺舒服的。而且还骑着这个小车完成了我的第一次 100KM 成都环城绿道的骑行～ 现在它已经不长这样了，我给它加了一对副把用来增加长时间骑行的舒适度，还把前后轮胎都换成了更细的半光头胎，这样骑行城市道路会省力一些。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:15:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【电子产品】DJI Action 3 某天我第 N 次想买运动相机，就想去大疆官网看一下之前评价不错的 Action 2，发现当天刚刚发售 Action 3。接下来就是等 Tim 评测的日子，心想只要 Tim 的评价不错我就直接冲到大疆实体店里去买。 没两天就等到了评测，再加上那段时间心情非常非常差，正好借机让自己开心一下，然后我的头上也就长出了一颗摄像头 🤯 Action 3 其实画质并没有很好，但是它的存在让我更愿意录制视频了，这一点比它的画质收音都更重要。以前我是不喜欢录视频的，因为我的 iPhone 12 mini 如果录视频的话很快就没电了，而且存储占用也是个很大的问题。更别说拿相机录了，画质虽好但设备真的太大了。现在用 Action 3 即使是关机的时候也可以一键开始录制，自身又带有磁吸，可以随手吸在任意一个金属面上，即使是手上拿着也并不会很麻烦，所以我也就更愿意拿它去记录我的生活。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:16:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【折腾】网络改造 以前其实也没太在意过家里的网络状况，毕竟自己又不玩网游不搞 pt 的。但是自从买了相机尤其是运动相机之后，每次往 NAS 里备份数据都得等上大半天，所以想着干脆把家里的网络环境升级一下。具体的升级项目如下，仅供参考 设置光猫调整为桥接模式，拨号由 J4125 软路由负责 原来的 AX5400 路由器关掉 DHCP 等功能，仅做 AP 用 软路由上配置统一的大梯子，有必要时单独打开，方便设备出海 软路由再配置 DNS 等常用服务，大幅提升网络体验 软路由上还配置了诸如 AdGuard Home 等服务 为 NAS 和 homelab 添加 2.5G 网卡，从此内网传输速度上了一个大台阶 电信把本来 300M 的宽带直接给免费升级到 1000M 了，原地起飞 👏 整体的体验最明显的就是往 NAS 里备份照片和电脑数据快了很多。以前纯无线网的时候电脑上传只有 70MB/s，现在最多跑到 230MB/s 还多点，速度上直接变成三倍了。不过这里不得不吐槽一下我这台 MacBook Pro，好歹也是 2020 年买的当时最新的设备，结果不支持 Wi-Fi 6 协议，而且想自己换张网卡都不行，最后只能在桌子上备个 type-c 的 2.5G 有线网卡，真的太蠢了。另外一个感受就是打开网页什么的明显变快了，因为我自己配置了大半天的广告过滤规则和 DNS 缓存，所以还是在带宽不变的情况下实现了上网速度的提升～ ","date":"2022-12-31","objectID":"/posts/2022-summary/:17:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【日用品】COOLMAX 无聊逛知乎上关于户外的话题，看到有人在推荐凯乐石的 COOLMAX T恤，看他说的还不错的样子就听去买了一件来试穿。价格比优衣库的普通 UT 要贵上一点点，但也就 120 块钱的样子。本来以为也就像之前买的速干衣一样普普通通，结果这个面料穿起来特别舒服，排汗速干的效果也特别好 ，强烈推荐给有速干衣需求的朋友去尝试一下这个面料。这应该是我今年买过最满意的一件衣服了，年后打算再买两件 🤣 虽然我自己很喜欢凯乐石的衣服和户外装备，但还是强调一下 COOLMAX 并非凯乐石的技术，其他很多品牌也都有采用该面料的衣服可以选择～ ","date":"2022-12-31","objectID":"/posts/2022-summary/:18:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【游戏】sifu 师父 师父是我今年玩过最好玩的 ACT 游戏 了，虽然体量不大但是打得是真过瘾。整个游戏虽然画面上并不是真实取向的，但是看着比较舒服，最值得说道的是里面的动作。不论是赤手空拳的搏斗、长棍短棍的动作还是挥舞手中的匕首，甚至是飞过去一板砖，都非常流畅真实。不仅是战斗系统，这个游戏的美术设计和关卡设计也非常值得一提。印象最深的就是美术馆的那关，从美术设计上说真的是漂亮的离谱（词穷了）；从关卡设计上说，时而第三人称时而横版 3D 的玩法也足够有趣（继续词穷）。 本来以为我这种没有 PC 的玩家跟这个游戏无缘了，结果某天突然看到他登陆 Switch 了😱 立刻买了回来当天晚上就趴床上打了大半天，确实是非常棒了。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:19:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【游戏】双人成行 双人成行是今年才通关的，作为一款「配置要求极高」的游戏，好在我可以流畅运行它。为什么说这款游戏配置要求极高呢？因为它是个纯双人游戏，必须要有朋友跟你一起玩才行🐶不仅如此，你的朋友还得会玩游戏尤其是会用手柄才可以，否则他的角色就会出现以下：走路撞墙上、跳跃掉坑里、拿脸接子弹等等症状。 这款游戏应该是自旷野之息和马里奥奥德赛之后最让我感到惊讶的游戏 了，真的是太有趣了。他的双人游玩并非是魂斗罗那种多了一个人之后内容完全不变的，也不是马里奥奥德赛那种一主一辅两位玩家。而是两个人有完全不同的游玩内容，两名角色所掌握的技能也是互补的，只有两位玩家互相配合才能前进。所以如果你用某角色通关之后，其实完全可以再交换角色开二周目，体验另一个角色的玩法。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:20:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【游戏】神界：原罪 2 神界原罪我最早是在 Gamker 那里听说的，但是迫于没有 PC 就一直没有机会玩到。直到今年我装了那台 32C 的服务器，自然也就顺手买了张老显卡拿来玩玩老游戏喽，所以也就终于玩到了这款神作。具体这款游戏的介绍那就还是去看拉我入坑的 Gamker 视频吧。 整个游戏最吸引我的其实并不是「角色扮演」的部分，也不是「宏伟剧情」的部分，而是战斗。游戏本身的战斗系统非常有趣也比较有深度 。 弓箭手如果站在高地上则会获得攻击距离和伤害的加成； 远程直线攻击比如射箭和火球术之类的可能会被敌我之间的障碍物挡住； 远程投掷武器即使敌我之间有障碍也可以从障碍上方丢过去； 地图着火之后再加上雨水则可以灭火，还会生成大雾进而降低雾中命中率； 敌人在大雾或是水中的时候，单体的雷属性技能可以秒变 AOE； 角色走在冰面上可能会摔倒； 所以可以先丢油瓶让敌人减速 -\u003e 再丢火系技能引发大火/爆炸 -\u003e 丢水系技能留下积水和大雾 -\u003e 丢闪电技能带来大型 AOE。不过不只你能这么干，NPC 当然也是可以这样折磨你的 😄 ","date":"2022-12-31","objectID":"/posts/2022-summary/:21:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【训练】暂停实验室 有一段时间我一直觉得很难控制自己的情绪，也觉得对身边的很多事情提不起兴趣来，正好想到想到之前关注的 UP 主推荐过这么一个训练营，也就自己花钱去报名了。可以简单给各位介绍一下，我参加的是 EBP 情绪训练，整个周期是连续 22 天的课程，每天会有一个小任务交给你，你自己去完成就好。我的 EBP 训练营体验下来主要是进行「正念」，有整年的呼吸、品尝、书写、行走等等。 当时我报名参加的时候有这么两种想法：一个是「万一能改善我的情绪和心理问题，即使只能改变一点，那几百块也是划算的」，另一个是「就当拿自己做个心理学实验，不管有没有用都还挺有趣的」。所以我就义无反顾得报名了这个训练营。完整的体验下来我是觉得有效果的 ，能感觉到自己对生活的感知更加敏锐了，也更能控制自己的情绪了。总的来说我觉得花个几百块钱能给自己带来一些这样的变化是值得的。 各位有需要的话可以扫下面的二维码去了解一下，如果用我的邀请码报名了的话可以联系我把我的返现佣金再转给你，这样可以再给你省下一些钱～你说是不是广告？当然不是啦，谁打广告还把返现的佣金送出去 🤣 ","date":"2022-12-31","objectID":"/posts/2022-summary/:22:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【书】大师之书 这本旷野之息设定集真的是太～棒～了！整本书 400 多页全彩印刷，里面包含了所有旷野之息里出现过的人物、怪物、武器、物品等等，清晰度极高比 Switch 上的渣渣画质不知道高到哪里去了 。 每次拿出来随便翻上几页，就总能看到一些以前从来没有注意到过的游戏中的细节，然后又双叒叕想去海拉鲁大陆当流氓 拯救公主了。 如果你是一个非常喜欢旷野之息的玩家，那我强烈推荐买一本回来收藏。不过毕竟是精装全彩高清印刷的 400 多页的设定集，还是要卖两百多块钱呢，都快能买一张旷野之息的卡带了 🤣 ","date":"2022-12-31","objectID":"/posts/2022-summary/:23:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【书】你想活出怎样的人生 这本书看完说真的觉得收获很大吗？并没有，里面讲的很多事情都是自己这个二十大几岁的人都懂的。但是知易行难，很多事情自己知道怎样是对的，更知道当前这样做是错的，但还是做出了错误的选择。我是觉得这本书虽然并不一定能给我带来多大的改变，但是如果某天我又作出明知是错误抉择时想起这本书里的故事，也许就避免了一次错误。 要说读完这本书最大的感受，其实是感叹自己从小没有受到这么高质量的教育 。 ","date":"2022-12-31","objectID":"/posts/2022-summary/:24:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【书】What If 这本书非常有趣，里面充斥了「如果人体内的 DNA 瞬间消失会怎样」「如果把海水抽干会怎样」「如果地球膨胀到太阳那么大会怎样」这种问题，并且用非常严谨和科学的态度回答了它们。如果你平时喜欢看科普类的内容或者自己也会想这些奇奇怪怪的问题的话，那我强烈推荐你去读一读这本书，真的很有趣。它还有个姊妹篇叫做 how to ，副标题是「如何不切实际的解决实际问题」。如果我小学的时候有这么两本书，我一定能天天翻天天看，一直把书翻烂为止（我真的好想把现在自己拥有的好多东西拿给小时候的自己啊 😭）。 不知道读过这本书的朋友有没有这么一种感觉，看这本书的时候总感觉自己在刷知乎 🤔 ","date":"2022-12-31","objectID":"/posts/2022-summary/:25:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"【书】三体 是的，我有罪，我现在才开始看《三体》，甚至黑暗森林都还没看完，我有罪😭 不过被大家吹上天的作品确实是好看，这应该是我看过最长的一部小说了，强烈推荐对科幻感兴趣又没有看过的赶紧去看（除了我真的还有这样的人吗）。不过从我自己的感觉，三体这套书对读者还是有那么一点点挑的，大刘应该默认了读者都是掌握了基本的理工科、天文和一些其他相关知识的。如果对这些完全不了解的话，看起来还是会有些吃力，不过仍然强势推荐一波，真的很好看。 0X06 总结\u0026展望 又到了给总结写总结的套娃时刻，不过今年的年终总结确实是有点长，也配的上单独有一个总结。总的来说呢就是今年过的比较一般，中规中矩，对明年整体看好，但又没有完全看好🤣毕竟现在疫情这么一放开，大家怎么说也要感染上个一二三四次，到时候 2023 会成为咳嗽元年也说不准呢。 对明年的展望也就列在这了，等明年年底的时候再来看看有多少是实现了的 工作效率较今年有明显提升 把无聊刷B站的时间全部拿来读书（不是戒掉B站） 多出去徒步，把今年疫情少去的补回来 我要拍星空，我要拍星空，我要拍星空 开足马力，多写博客，写博客还挺好玩的 展望不是计划，喜欢看我年度计划的可以私聊我（我也没想到，去年被人催更年度计划来着）🧐 一万多字的年终总结到这里也就结束了，如果真的有人能看到这儿可以来微信找我领取一份神秘大礼口头奖励 🎁 最后祝看到这里的所有朋友们，新年快乐～ ","date":"2022-12-31","objectID":"/posts/2022-summary/:26:0","tags":["Summary"],"title":"2022 年度总结","uri":"/posts/2022-summary/"},{"categories":null,"content":"0X00 简单用法 内容比较少，主要是介绍一下 xargs ，直接开始吧～ 首先假定各位能够熟练运用基础的管道操作，能够理解 ps aux | grep nginx | awk -F ' ' {print $1} 这样的命令。 我们日常使用管道的时候肯定会用到 ls | grep xxx | grep xxx ｜ rm 这样的操作，想从一堆文件里筛选出自己需要的文件并将其删除/移动/复制等。但其实这样的操作是不行的，因为前面管道传过来的是数据流但是 rm 命令却并不能处理它。如果想实现这种效果就可以用 xargs 将其进行转换了，如下图所示使用 ls | grep bbb | grep \"[3-5]\" | xargs rm 就可以完成任务。 此处 xargs 的功能就是将前面的数据流逐行分解并一个个丢给 rm 去执行。不过通常的管道都是一个命令执行一次，配合 xargs 之后就是做一个循环了，将管道前面的的输出逐行取出作为参数交给下一个命令去执行。我这里画了一个简易的图，也许能够便于理解。 其实如果只是找到文件并删除的话，还有更简单的方法啦，例如 find . -regex \"\\./test_bbb_[3-5]\" -delete 现在虽然解决了删除的问题，但其实并没有解决复制的问题。因为你写了 ls | xargs cp target_dir/. 之后它会翻译成 cp target_dir/. ./aaa ./bbb ./ccc 这样的命令，显然是不对劲的。所以 xargs 还提供了宏功能，使用类似 find . -regex \"\\./test_bbb_[3-5]\" | xargs -I {} cp {} target_dir/. 这样的命令就可以将前面的输出拆分后准确的塞入到后面命令的指定位置，实现想要的功能了。 0X01 参数们 首先是关于「到底执行了什么」的参数，有两个分别是 -p 和 -t，前者用于把即将执行的命令打印出来并询问是否执行（交互式），后者则是单纯的打出执行的命令。用法就是 ls | xargs -p rm 和 ls | xargs -t rm 这样简单。 其次就是上面介绍到的 -I 参数，宏命令，正如上面所说的那样，是设计一个模板并将其替换。⚠️需要注意的一个点是，{} 这个符号并非语法，而是你想用什么就用什么，比如 ls | xargs -I HONG_MING_LING cp HONG_MING_LING xxx/. 也是可以的，只是 {} 比较少出现在命令里并且也比较易于理解而已。 另外两个参数是 -L 和 -n，前者表示「每多少行作为一组参数」，后者表示「每多少列作为一组参数」。下图所示，第一段是将每三行作为一组参数，所以可以看到执行的是 sleep 1 2 3 和 sleep 4 5 这两个命令；第二段是将每行列作为一组参数，所以看到执行也是分了五步；最后一段是将每行的两列作为一组参数，所以执行的是 sleep 1 2、 sleep 3 4 和 sleep 5 。 还有最后一个重要的参数是 -P，这个参数是用来控制并发的。前面介绍了 xargs 会将参数丢给后面的命令，逐个执行一遍。那么来看一下这个命令 find . -name \"*.png\" | xargs ./compression_png.py，是要将目录下所有的 png 文件压缩一遍没错了。但是如果这个脚本是单线程运行的话（大多数情况下 Python 脚本并不会真的并行执行）你的多核 CPU 就没有用武之地了，只会在压缩每个文件的时候抽取一个幸运核心来工作，其他核心就休息。想要并行的话改脚本当然是麻烦了，可以给 xargs 加上一个 -P 参数就实现多线程工作：find . -name \"*.png\" | xargs -P 8 ./compression_png.py，这样一来就会同时运行 8 个进程，也就真的是并行运行了～ ","date":"2022-12-27","objectID":"/posts/xargs-simple/:0:0","tags":["Linux","Shell"],"title":"xargs 基本用法","uri":"/posts/xargs-simple/"},{"categories":null,"content":"0X00 虚拟化 首先虚拟化 Virtualization 它是一种技术，通过软件技术虚拟一张网卡（例如 Linux Bridge）、虚拟一个磁盘分区（例如 vmdk 文件）甚至直接虚拟一整台电脑（例如 VMware/VirtualBox）出来都是虚拟化技术的实装。 虚拟机就是通过虚拟化技术虚拟了一整套电脑所需的硬件，例如CPU、内存、磁盘、网卡等等，然后将它们拼在一起就是一台虚拟机了。 0X01 容器 区分这些的重点就在容器 Container 这里了。最早接触容器的时候很多人都说简单理解成轻量化虚拟机，不需要开关机、不需要操作系统、甚至是秒级启动的。当时我就死活不理解，明明是虚拟机为啥不需要操作系统不需要开关机的呢，其实但凡当时多解释几句，也就不会有这种困扰了。 tips: 这里说的容器仅仅指代 Linux 容器 容器的本质是进程 。在 Linux 环境下创建一个进程，如果我们将它与其它进程隔离开，让它发现不了其它进程、再分配虚拟网卡、临时的文件系统等等等等，那它其实就是一个「容器」了。所以容器的本质是进程 ，这样以来就说的通了。而且为什么不需要操作系统？因为它运行在一个现有的操作系统之上。为什么不需要开关机？因为它只是个进程。为什么是秒级启动？依旧因为它只是个进程～ 关于 Linux 是如何将一个进程隔离开的，可以参考下面这一系列文章 Docker基础技术：Linux Namespace（上） Docker基础技术：Linux Namespace（下） Docker基础技术：Linux CGroup Docker基础技术：AUFS Docker基础技术：DeviceMapper 0X02 Docker Docker 就简单了，它只是容器技术的一种实现，类似于 KVM 只是虚拟化技术的一种。不仅 Docker 是容器技术，还有 LXC、cri-o、containerd 等等一些。 0X03 总结 最后进行一个总结：虚拟化技术是通过软件虚拟出硬件来的技术；虚拟机是虚拟化的集大成者，将一堆虚拟出来的硬件组装成一台电脑；容器技术的重点是将进程隔离出来，使之独立运行；Docker 则是容器技术的一种实现 。 ","date":"2022-12-16","objectID":"/posts/virtualization-container-docker/:0:0","tags":["Virtualization","Container","Docker"],"title":"虚拟化、容器、Docker","uri":"/posts/virtualization-container-docker/"},{"categories":null,"content":"0X00 Linux 的进程关系 既然想搞清楚容器的单进程模型，那自然需要先复习一下 Linux 下的基本进程关系了。你说你用的是 Windows Container？不懂，不会，打扰了😢 我们知道 Linux 下会有一个 PID=1 的进程来带动其他进程，以前 PID=1 的进程是 init 后来大家都在用 systemd ，这里就不多说了，只来回顾一下「孤儿进程」和「僵尸进程」这两个概念。 这里来模拟一个场景，默认 PID=1 的是 systemd 打开运行一个新进程 新进程 fork 了一个子进程出来 子进程持续运行 此时父进程终止了 子进程成为孤儿进程 （因为他的父进程挂了） 该子进程会交由 systemd 接管 当子进程结束之后 systemd 会替它「收尸」，也就是释放、回收资源之类的 还有另一个场景：某菜鸡程序员（可能是我）写了个程序，现在运行起来了。该进程会 fork 新进程，每隔一会儿就会 fork 一个，但是该进程并没有 wait/waitpid 这种操作（俗称管杀不管埋）。所以当子进程结束后资源并没有被回收，甚至 PID 都还占着。这种已经结束了但是没被回收的进程就叫僵尸进城 噢不，叫僵尸进程 。 需要注意的一点就是，孤儿进程并不是什么大问题，systemd 会解决它的；僵尸进程也 不都 有问题，任何进程在结束之后和被回收之前，都处于这个状态，真正要注意的是源源不断产生僵尸进程的那个进程，就算只是一直占着 PID 也不是个事儿嘛。 0X01 容器的本质 好了现在已经回忆起僵尸进程和孤儿进程这两个概念了，接下来回忆一下容器的本质。简单来说的话容器（以 Docker for Linux 为例）并非虚拟化，而是在宿主机上运行的一个普通进程而已，只是通过 Linux 自身的一些特性将其与宿主机环境隔离开了而已。当然了，既然不是虚拟化也就没有宿主机这种说法，这里只是图个方便才这么说的。如果你想更多的了解容器技术本身，推荐下面这一系列文章 Docker基础技术：Linux Namespace（上） Docker基础技术：Linux Namespace（下） Docker基础技术：Linux CGroup Docker基础技术：AUFS Docker基础技术：DeviceMapper 0X02 单进程模型 既然我们了解了上面两项知识，那么自然也就明白为什么容器里 EntryPoint 的进程是 PID=1 了。接下来我们假设你写了个普通的程序，在程序内部会 fork 一些子进程出来。然后将其搬到了容器里，那么你自己的这个程序运行之后，又 fork 了一堆子孙进程出来，现在你想让 PID=1 的进程负责收尸工作，那么 PID=1 的进程是哪个呢？就是你自己写的那个呀🧐 如果没搞错的话，一般自己写一个多进程的程序是不太会管僵尸进程子进程这些的，一切交给 systemd 就好了。但是现在 PID=1 的是自己写的那个程序，又没有这个收尸能力，所以才会有这么个「单进程模型」啦。 那怎么解决呢？有问题就有答案，你可以选择修改你的程序，让它监控子进程、僵尸进程，并且对其进行合理的回收♻️；也可以让每个容器只运行一个进程，多进程就干脆多容器。然后把相关联的几个容器间的网络、文件、数据都给打通，这样一来每个进程都是 PID=1，当它结束的时候直接容器就停止了，也就不会有回收资源的问题了。可是看起来虽然少了监控回收的工作，但是多了打通网络、打通文件系统、打通进程间数据共享的问题，还多了一大堆其他的隐藏工作。这时候知道的小伙伴就知道了，该 Kubernetes 上场了～ 如果你不知道 Kubernetes 的话，可以先简单将其理解成一个容器编排工具，它里面有一个重要的概念叫做 pod。pod 中可以运行一个或多个容器，容器之间共享同一个网络命名空间，也就是说同一个 pod 里的容器 A 可以用 127.0.0.1 访问同处于一个 pod 中的其他容器；pod 中的所有容器也共享一个 Volume，也就是说简单的文件共享也是有的；并且它们之间甚至可以使用 Linux 下标准的进程间通信，例如信号量这些。 0X03 最后 综上所述，我们得出几条结论 由于将进程隔离起来，导致容器内部并没有 init/systemd 这种负责收尸的老大哥； 由于 1 的缘故，我们说容器使用的是单进程模型； 虽然是单进程模型，但是并非真的只能运行一个进程，你想的话可以自己实现一个 systemd 一样的东西在容器里面玩起来（但是并不推荐）； 程序/业务真的复杂到这种程度的话，建议引入容器编排工具，例如 Kubernetes； 参考资料： 为什么说容器是单进程模型 被忽略的一点：Docker 的单进程模型 ","date":"2022-12-16","objectID":"/posts/container-single-process/:0:0","tags":["Linux","Docker"],"title":"容器的单进程模型","uri":"/posts/container-single-process/"},{"categories":null,"content":"0X00 🛡️叠buff 我是个 Kubernetes 纯新手，并不懂很多原理和概念，可能会有误导； 我只是将自己遇到过的问题列出来，如果你想找「权威」请看官方文档； 每个人的基础环境不同，Kubernetes 版本也不同，参考应该学会变通； 此处记录的仅为个人部署过程中遇到的问题记录，并非「教程」或「指南」； 0X01 正文 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:0:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"注意交换分区 部署 Kubernetes 的节点是不允许使用交换分区的，临时禁用可以 swapoff -a。然后在 /etc/fstab 中将交换分区的自动挂载给注释掉就可以了。 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:1:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"不使用 docker Kubernetes 已经不建议使用 docker 作为容器运行时了，可以考虑使用 containerd 或者 CRI-O。注意这里提到的 Dockershim 已经移除并不意味着不能再用 Docker 了，而是说 Kubernetes 并不会原生支持 Docker 了，以后 Docker 的地位和其他运行时的地位相同了。 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:2:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"Containerd 的默认配置 如果使用 Containerd 作为容器运行时的话，安装好 Containerd 之后要检查一下配置文件 /etc/containerd/config.toml。如果没有的话需要手动创建一个默认配置 mkdir /etc/containerd containerd config default \u003e /etc/containerd/config.toml ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:3:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"kubelet 疯狂重启 如果你还没有搞定 kubeadm init 的话，kubelet 疯狂重启是正常行为，它在等待 kubeadm。 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:4:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"需要 AppArmor 我之前手贱把 AppArmor 给卸掉了，自以为是得认为它会影响到 Kubernetes 的运行。其实它确实会影响，因为 Kubernetes 的运行时需要它的，所以后来看日志在报错就又装回来了。 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:5:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"正确配置 cni 网络插件 如果你是使用 Containerd 的话，可以参考 https://github.com/containerd/containerd/blob/main/script/setup/install-cni ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:6:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"容器运行时的冲突 如果要用 Containerd 的话就记得先把 docker 卸载干净，否则可能会导致一些冲突和不兼容的状况。 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:7:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"正确加载内核模块和内核参数 我当时使用的是 Ubuntu 22.04，默认没加载 br_netfilter 模块，所以需要使用 modprobe br_netfilter 临时加载，并且使用 echo br_netfilter \u003e\u003e /etc/modules 持久化加载，保证其重启后也是被加载的。可以使用 lsmod | br_netfilter 的方式检查是否已经加载了该模块。 然后就是内核参数，首先可以通过 sysctl -a | grep xxx 的方式检查当前参数是否设置正确了。如果没设置正确的话需要在 /etc/sysctl.conf 中进行修改。改好之后 sysctl --system 可以使改动生效。 net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:8:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"使用 crictl 命令 如果你想使用 crictl 命令查看容器/镜像，则需要修改其配置文件让它能够连接到你的容器运行时，配置文件在 /etc/crictl.yaml。如果你用的是 Containerd 的话配置文件可以写成 runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: true ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:9:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"关于认证文件 当 kubeadm init 成功之后，记得关注一下它打出来的内容，里面包含了你该如何让其他节点加入，还包含了如何从其他机器上管理 Kubernetes 集群。 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:10:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"get nodes 当使用 kubectl get nodes 命令查看到的所有节点都处于 Ready 并且过一两分钟也没有异常状态时，也就意味着你的 Kubernetes 集群已经部署完成了🎉 ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:11:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"关于 dashboard 刚刚部署了一个 kubenetes-dashboard 结果 401/403/40x 了？可以尝试使用端口转发的方案：https://github.com/kubernetes/dashboard/issues/5542#issuecomment-706395744 dashboard 打开了，但是没有 Token 登陆不了？那就创建一个 token 喽 https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md ","date":"2022-12-08","objectID":"/posts/k8s-deploy-tips/:12:0","tags":["Kubernetes","Container"],"title":"部署 Kubernetes 集群时遇到的一些问题","uri":"/posts/k8s-deploy-tips/"},{"categories":null,"content":"0X00 为嘛用 kind 作为一个纯新手想要学习 Linux、MySQL、Python… 的第一步往往都是先装一个来看看，当然 Kubernetes 也不例外。装 Linux 也许跟着教程在虚拟机里一会儿就装好了，尤其是现在很多发行版本都有图形化安装界面了，但是 Kubernetes 就不一样了，如果你去部署一套 K8S 集群的话极有可能会遇到一系列问题，包括但不限于： 交换分区没关，导致服务异常 搞不清楚 CRI-O/docker/containerd 之间的关系 刚装好的 kubelet 服务疯狂重启 cni 网络插件搞不明白，导致 kubeadm init 一直不成功 缺少内核模块导致的集群初始化异常 缺少内核参数导致的集群初始化异常 想用 crictl 却怎么都看不到 container 状态 这些在熟手眼里可能根本不是问题，但是对于第一次接触的人来说还是挺麻烦的。所以我们需要一种简单的方式来快速部署一个 Kubernetes 来看一看试一试，而不是上来就先部署一套三四个节点的真正意义上的集群。 Kubernetes 官方推荐了几种单节点/单机部署的方式，kind 就是其中一种。它是基于 docker 的部署方式，如果你还不熟悉 docker 的话建议先把 docker 学一学再开始 Kubernetes 的学习。如果你已经了解 docker 并能简单的使用它了，那接下来就开始安装 Kubernetes 吧~ tips: 建议在虚拟机中安装，分配 2C4G 然后随便装一个比较新比较常用的 Linux 发行版本就可以 0X01 安装 既然是基于 docker 的那自然要先安装 docker 了，常见三种安装方式： 直接 apt install docker ❌ 不推荐 按照官网操作：新增 mirror、更新 cache、安装 docker ✅ 推荐 使用官方脚本 https://get.docker.com/ ✅ 推荐 第二步是安装 kubectl ，它是用来管理 Kubernetes 集群的，类似于 docker 体系中的 docker client 吧。原则上是可以不装 kubectl 的，没有它集群也能正常工作，但没法操作集群的话那还怎么学呢。因为 kubectl 只是一个单纯的二进制文件，所以「安装」就仅仅意味着：下载、挪到 $PATH 中、赋予执行权限。 curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl /usr/local/bin/. 装好 kubectl 后就该装 kind 了，kind 方便就方便在它也是一个二进制文件。所以我们去 GitHub 的 release 页面找到自己平台最新的二进制包下载，然后依旧是赋权和移动位置就搞定了。 （可选）最后可以将 source \u003c(kind completion bash) 和 source \u003c(kubectl completion bash) 追加到 ~/.bashrc 最后，再手动 source ~/.bashrc 一下，就可以为这两个命令提供自动补全了。 到此为止 kind 就安装完成了，接下来可以简单介绍一下它的用法。 0X02 使用 首先可以使用 sudo kind create cluster 来创建一个单节点的 Kubernetes（后面可以接 --name 参数为集群取名，默认叫做 kind）。因为是基于 docker 的所以需要从 dockerhub 拉镜像，如果「网络通畅」的话速度会比较快，如果不通畅的话可以用更科学的姿势上网~ 也可以使用配置文件的方式来创建集群，例如下面的配置，使用 sudo kind create cluster --config CONFIG_FILE.yaml kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta1 kind: ClusterConfiguration metadata: name: config networking: serviceSubnet: 10.0.0.0/16 imageRepository: registry.aliyuncs.com/google_containers nodeRegistration: kubeletExtraArgs: pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1 - | apiVersion: kubeadm.k8s.io/v1beta1 kind: InitConfiguration metadata: name: config networking: serviceSubnet: 10.0.0.0/16 imageRepository: registry.aliyuncs.com/google_containers nodes: - role: control-plane 当然如果你不想管这么多配置的话也可以用「极简配置」 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane 还可以创建「多个节点」的集群，因为仍然是在同一台机器上，所以用了引号，并非传统意义上的集群。不过创建多节点集群必须要用配置文件了。 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker - role: worker - role: worker 其他 tips： 使用 kind get kubeconfig 可以获取当前的配置文件，可以给 kubectl 用 使用 kind delete cluster 可以删除默认集群，也可以接 --name 删除指定集群 可以同时创建多个集群，不重名就行，可以用 kind get clusters 查看当前存在的集群 使用 kind 还能做到：高可用集群、端口映射、指定 Kubernetes 版本、配置代理等等，不过具体的就要去看官方的简明文档喽 ","date":"2022-11-28","objectID":"/posts/kind-deploy-k8s/:0:0","tags":["Kubernetes","Container","Docker"],"title":"使用 kind 飞快的创建一个 Kubernetes 集群","uri":"/posts/kind-deploy-k8s/"},{"categories":null,"content":"0X00 这是一个描述 下面介绍几个我自己常用的小技巧，均可以在日常工作中给自己带来一些小小的便利🤪 0X01 临时 web 服务器 如果你电脑上装了 Python3 则可以使用 python3 -m http.server --bind 0.0.0.0 2333 这个命令在当前目录启动一个简单的 web 服务，监听在 0.0.0.0:2333 上。这样一来别人就可以访问你的 ip 来下载当前目录下的文件了。不过使用这个方法的时候要注意自己当前的工作目录哈，不要傻乎乎的在自己的 $HOME 下面用这个命令，小心别人下载你的隐私数据喔。 不过如果你搞不懂什么是「监听、0.0.0.0、端口」的话，还是先去搞一下计算机网络吧。 0X02 使用 pdb/ipdb 调试脚本 我们都知道用 pdb 模块可以逐行调试 Python 脚本，只需要在脚本里 import pdb 然后在需要打断点的地方加上 pdb.set_trace() 就可以了。但其实 Python 还有一个三方库叫做 ipdb 是 pdb 的升级版，应该各位也都知道吧，通过简单的 pip install 就可以装好。不论是 pdb 还是 ipdb 其实都有这个用法：不修改脚本内容，仅通过 python3 -m ipdb script_filename.py 来从头调试脚本。并且会在每次脚本结束之后从头开始，直到手动退出为止。 tips: pdb/ipdb 运行时可以使用 b line_number 来在指定行打上断点，然后再 c 跳到下一个断点处从而方便调试。 0X03 指定版本 我们都知道可以用 virtualenv 之类的东西处理 Python 的虚拟环境，但是还有这么一种简单粗犷的方式也可以凑合一用（尤其是同时存在 Python 2.7/3.7/3.8/3.9… 的这种环境）。 使用 python2.7 -m pip install requests 或者 python3.9 -m pip install xxx 可以指定具体的 Python 版本，如果你有多个虚拟环境也可以直接指定到 Python 的二进制文件，再通过 -m pip 的方式将所需的库安装到正确的位置~ 0X04 一行流 这个「一行流」比较骚气，相当于把 Python 当成普通命令来用，简单的说就是 python -c \"xxxxx\" 可以在一行命令里执行一个 Python 脚本。例如有下面这些用法（其实就是把脚本写成一行） 比如说使用 python3 -c \"import json; print(json.dumps(json.load(open('data.json')), indent=4))\" 这样的方法可以直接将 json 文件格式化输出。（当然了这个需求其实有 jq 命令可以做的更好，只是这里举个例子而已） 具体这个一行流的用法有多强，那就要看各位的脑洞了，唯一一个需要提示的就是 Python 其实是允许使用分号来终止一个语句的，类似 C 那样，所以才使得一行流可以实现~ ","date":"2022-11-15","objectID":"/posts/python-developer-cool/:0:0","tags":["Python"],"title":"属于 Python 程序员的小技巧","uri":"/posts/python-developer-cool/"},{"categories":null,"content":"0X00 正文 这篇内容可能是目前为止我所有博客内容里最特别的一篇了，今天不想写什么技术类文章，只是想发发牢骚，说说自己想说但一直没啥机会吐槽的事情。不知道各位有没有那种情况，其实自己是很乐观很开心的一个人，但就是偶尔会突然觉得很累，很烦，很焦虑。这些事情说大可能并不大，只是打个游戏可能某一关打不过去而已；说小可能也不小，甚至跟全国乃至全球数以亿计人口的健康相关。那接下来我就要开始吐槽了，也许会有些负能量，但把自己的「负能量」一股脑输出出来没准就能让自己变得更好呢 hhhhh tips: 这些内容不分前后顺序、没有主次关系、不一定符合你的看法、甚至你觉得我就是在胡说八道，但我不在乎~ ","date":"2022-10-21","objectID":"/posts/fucking-tired/:0:0","tags":["Life"],"title":"「累」","uri":"/posts/fucking-tired/"},{"categories":null,"content":"我就想随随便便看个综艺 本来我很早就看 今晚80后脱口秀 的，虽然我并不是 80 后。也挺喜欢听他们说脱口秀的，不管是现在已经转行的王自健还是后面一直在说的蛋蛋建国什么的。后来 80 后脱口秀就不播了，我也就没再关注过脱口秀了。知道某天偶然间发现了 脱口秀大会 这么个节目，还挺喜欢看的，就每次看到在更新了就会跑过去看。我一直会给自己的时间安排上学习、娱乐什么的，但是这个节目因为我看起来确实非常快乐，所以只要它更新了我基本上就会把当天的学习计划或者游戏计划都延期，来专门看这个节目。 但是现在这个第五季，就经常在网上看到有人评论说「李诞人不行，就知道复活自己公司的人」「一天到晚就知道说内部梗，不利于脱口秀发展」「比赛有黑幕」这些那些的。我就一个工作学习累了的小小程序员，就想躺在沙发上听听脱口秀，谁 TM 关心李诞复活谁了？谁 TM 关心脱口秀发展了？脱口秀发展的好了我是能多赚钱吗？还是说脱口秀发展不好了会让我爱情事业双受挫？ 我就是一个纯纯路人观众，我就想听听好笑的东西让自己笑一笑，虽然我也会想让自己喜欢的演员晋级，但说真的我并不在乎他能拿到什么大王不大王的，只是说晋级了就还能至少再听一场他的演出而已。真不理解这些人一天天的怎么就这么无聊，到处盯着别人的各种细节去看，还要吧儿吧儿的说你。跟你说「脱口秀大会这么黑幕，你居然还看得下去？」「难道只有我一个人觉得这个节目无聊？」。你 TM 看不下去就不看嘛，来逼逼啥；你 TM 觉得无聊就去找点有趣的事情做嘛，管我干啥。我就想在累了的时候笑一笑，结果还要被这种人突然恶心一下。 ","date":"2022-10-21","objectID":"/posts/fucking-tired/:1:0","tags":["Life"],"title":"「累」","uri":"/posts/fucking-tired/"},{"categories":null,"content":"我就想随随便便打个游戏 本来说我一个从来不玩手游的人，想看看最近的手游都发展的怎么样了，就下载了几个比较火的手游试了试。结果呢，有些游戏一打开就把我当个弱智，整个屏幕都是灰色的，就只高亮一个按钮，旁边写一大堆字，告诉我应该点这里。点了之后又告诉我该点那里、点那里、再点那里。一路点下来我发现已经完成了「新手教学」任务，给了我吧啦吧啦吧啦一大堆东西，但此时的我就是机械的点完了这些按钮，完全不知道他们是干啥的，甚至不知道自己干了啥。。。要不然就是一些竞技类游戏，自己发挥好了可能会被队友坑，自己没发挥好可能会被队友骂，有可能自己和队友都发挥好了但是敌人太强最后还是输了，结果搞得垂头丧气。。。还有些就是整一大堆「日常任务」和「周常任务」，说真的我白天对着工作的 TODO LIST 逐个清理完，晚上回家再对着游戏里的 TODO LIST 逐个清理，就觉得自己很蠢，完全不知道自己为什么这么折磨自己。 我就只是想找个游戏放松放松，目前发现除了主机游戏和部分单机手游之外，想要图一乐就只能是王者这种游戏的纯匹配模式，自己和队友都不在乎输赢而且也没有什么乱七八糟的任务。 ","date":"2022-10-21","objectID":"/posts/fucking-tired/:2:0","tags":["Life"],"title":"「累」","uri":"/posts/fucking-tired/"},{"categories":null,"content":"我就想随随便便出去逛逛 当时刚开始实习和刚刚转正的时候并没有什么钱可以出去玩，现在虽然也没有啥钱，不过偶尔出去逛逛的预算还是有的。但是现在这个傻逼防疫政策整的，说真的比起疫情我更害怕防疫政策。现在出去玩一趟在不被隔离的情况下，需要首先查询离开当前城市的防疫政策、到达目的城市的防疫政策，然后做核酸检测，还要在核酸检测报告过期之前出发。到了目的地之后又要三天两检七天三检巴拉巴拉的。即使你完全符合了当地的防疫政策，还有可能会被「大数据赋码」给整成黄码。接下来就要到处去咨询和投诉，最后还不一定能解决（真实遭遇）。最后你的游玩结束了，要回到自己的城市，还要看出发和到达两地的防疫政策，回到了自己的城市之后第一时间就会收到短信威胁（真实遭遇），如果不进行三天两检或者七天三检啥的，就又会让你变成黄码。 那如果不出省不出市呢？即使上个班也要检查核酸报告，坐个地铁也要看核酸报告，吃个饭也要看核酸报告。身边也出现过几次因为核酸报告没有准时出来，结果背着包在楼下溜达了半个小时一个小时，等到了核酸报告才进楼上班的；也有出现过因为错过了核酸检测时间，导致第二天没有报告，然后不让进地铁站，最后只能打车十几二十公里去上班的。现在路上看到没啥人的核酸检测点都不由自主的过去排队，用同事的话说就是「巴普洛夫看了会沉默，巴普洛夫的狗看了会流泪」。 当然了最他妈扯淡的是进入餐饮场所不戴口罩还他妈不让你进。我是真的不理解我为什么要戴着口罩进餐厅。 ","date":"2022-10-21","objectID":"/posts/fucking-tired/:3:0","tags":["Life"],"title":"「累」","uri":"/posts/fucking-tired/"},{"categories":null,"content":"好了我吐槽完了 真的吐槽完了？那肯定是没有的，只是现在已经晚上十一点半了，我打算睡觉了。毕竟如果一直吐槽的话，键盘被敲烂了可能都吐槽不完。 虽然我并不觉得会有人能从头看到这里，但是万一有的话，我祝你永远没有上面我的这种困扰，祝你幸福~ ","date":"2022-10-21","objectID":"/posts/fucking-tired/:4:0","tags":["Life"],"title":"「累」","uri":"/posts/fucking-tired/"},{"categories":null,"content":"0X00 介绍 阅读并了解 LVM 需要了解：Linux 基本操作、分区概念、文件系统概念 首先 LVM 的全称是 Logical Volume Manager 逻辑卷管理。传统的方式是将一个磁盘分成类似于 sda1/sda2/sda3 的分区，然后再将这些分区格式化成类似于 ext4/xfs 这种文件系统，最后将文件系统挂载到某个目录上。但是这种方式下对磁盘空间进行重分配是比较麻烦的，将新安装的磁盘融入到现有系统中也是比较费力的，这就是 LVM 需要解决的问题。 总结下来 LVM 拥有这些功能 LVM 可以方便的对现有逻辑卷进行压缩（初次分配多的空间不会浪费，可以压缩出来） 空闲的空间可以随时重新分配给逻辑卷（传统模式只能将空间分给最后一个分区，或者创建新分区） 新加入的磁盘也可以为其他逻辑卷扩容（传统模式并不方便为某个现有分区扩容） 可以将两块磁盘融合创建出一个更大的逻辑卷（两块 1T 磁盘可以创建出 2T 的分区） 特别需要注意的，RAID0 也可以将两个 1T 的磁盘合并为一个 2T 的，并且理论读写速度都会翻倍，但是这和 LVM 完全是两种不同的操作。 0X01 相关概念 开始使用 LVM 前需要先搞清楚它的几个基本概念 PV 是 Physical Volume 物理卷 — 从磁盘上分出来的物理分区 VG 是 Volume Group 卷组 — 多个 PV 组成的一个 Group LV 是 Logical Volume 逻辑卷 — 从某个 VG 里创建出来的逻辑卷（可以格式化够挂载） PE 是 Physical Extent 物理区域 — 是 PV 中最小的存储单元 LE 是 Logical Extent 逻辑区域 — 是 LV 中做小的存储单元 只是「简单用用」的话可以不管 PE/LE 这两个概念 0X02 测试环境 如果你想跟着练习一下的话，这里推荐使用虚拟机，新装一个随便什么 Linux 发行版（还是建议新一点，防止 LVM 版本过老）然后额外分配 2 块 10G 的虚拟磁盘就可以了（截图里是三块，因为我以为做完整个实验需要三块盘，结果最后没用上 😢）。 0X03 如何使用 我们已经知道 PV 就是一个物理分区、VG 是一堆 PV 的集合、LV 是从 VG 中划出来的逻辑卷了，现在开始用起来吧 其实这里说 PV 是一个物理分区是不严谨的，也可以不分区，直接对整体的块设备进行 pvcreate 的操作，后面为了方便我也会这么操作 ","date":"2022-09-22","objectID":"/posts/linux-lvm/:0:0","tags":["Linux","LVM"],"title":"LVM 的创建扩容与压缩","uri":"/posts/linux-lvm/"},{"categories":null,"content":"从零创建 首先我们要通过 LVM 从零创建一个文件系统，就需要一路创建 PV、VG、LV 这三种 这里可以看到，一共用到了 pvcreate/vgcreate/lvcreate 三个命令，最终创建出来的 lv 就已经是一个块文件了，接下来的格式化与挂载就不再赘述了，而且根据经验也可以猜到如果要删除的话就是 lvremove/vgremove/pvremove 了。 值得注意的一点是 lvcreate -L 100M -n LV_NAME VG_NAME 这行命令，表示创建一个 100M 的 LV，名字叫做 LV_NAME，从 VG_NAME 这个 VG 创建。 另一个值得注意的一点是，创建出来的 LV 是一个链接，是为了给 dm-0/dm-1 这种不好记的文件「重命名」，这样一来我们后面格式化、挂载之类的就可以用 LV_NAME 来操作了。 ","date":"2022-09-22","objectID":"/posts/linux-lvm/:1:0","tags":["Linux","LVM"],"title":"LVM 的创建扩容与压缩","uri":"/posts/linux-lvm/"},{"categories":null,"content":"扩容 上面我们只创建了一个 100M 的分区，其实现在我们来给它扩容。首先我们可以看到 VG 明明有将近 10G 的空间，现在才用了 100M 显然是浪费的，先给这个 LV 扩容到 9G 再说 上图中可以看到一共执行了这么几个步骤 检查当前 VG 的状态，看到还剩 9.9G 的空间处于 free 状态 将 LV 扩容至 9G （扩容命令跟创建差不多，只是没必要指定 VG 了，毕竟 LV 就已经和 VG 绑定了） 再度检查 VG 的状态，发现空间被划走了 使用 fdisk -l 看到那个块文件/分区的空间变大了 那如果这时候空间还不够用呢？这就体现出 LVM 的优势了，我们可以将一块新盘加入到 VG 中，继续给 LV 扩容 上图中可以看到一共执行了这么几个步骤 创建一个新的 PV 将新的 PV 加入 VG 检查 VG 状态，发现 VG 扩容成功 将 LV 扩容至 18G 检查 LV 是否扩容成功 ","date":"2022-09-22","objectID":"/posts/linux-lvm/:2:0","tags":["Linux","LVM"],"title":"LVM 的创建扩容与压缩","uri":"/posts/linux-lvm/"},{"categories":null,"content":"压缩 接下来就该给 LV 压缩了，例如同一个 VG 下的两个 LV 其中一个空间告急需要从另一个中挪一点空间出来，就可以这么操作（其实跟扩容没有区别） 0X04 PE 和 LE 最后解释一下 PE(Physical Extents) 和 LE(Logical Extents) 两个术语。其中 PE 是 PV 的「最小存储单元」，LE 是 LV 的「最小存储单元」。 需要注意的是 PE 是创建 VG 的时候指定的，而并非创建 PV 的时候。而且 LE 的大小就是 LV 所在的 VG 的 PE 大小（这里有点绕，稍稍停下理解一下）。默认情况下 PE 是 4M，也就是说默认情况下创建出来的 LV 容量都将是 4M 的整数倍，当然也就可以解释下面这种「我明明创建的 99M 为啥变成了 100M」诡异情况了 所以我们从原理上来说，创建 LV 的时候并不是「指定 LV 的容量为多少 M/多少 G」，而是「指定 LV 的容量为多少个 LE」 0X05 参考资料 LVM - Debian Wiki LVM - ArchWiki（有中文） ","date":"2022-09-22","objectID":"/posts/linux-lvm/:3:0","tags":["Linux","LVM"],"title":"LVM 的创建扩容与压缩","uri":"/posts/linux-lvm/"},{"categories":null,"content":"0X00 前言 设计一个系统，不论是 Web 还是其他的什么形式，通过用户名和密码认证也是一个再正常不过的事情了。但是如何保存密码却是一个值得讨论的问题，相信各位最开始的一个有用户名和密码的程序多半也是用明文存储的密码吧 🤓 这里总结了六种比较常用于密码存储的方式，接下来可以逐一进行简单的分析以帮助我们更好的保护用户的密码 一清二白：明文存储密码，直接存 password 掩耳盗铃：使用 BASE64 之类的编码，存储为 cGFzc3dvcmQ= 盘古之法：使用早已不再安全的 md5 之类的摘要算法，存储为 5f4dcc3b5aa765d61d8327deb882cf99 祖宗之法：使用也已经不再安全的 sha-1 之类的摘要算法，存储为 5baa61e4c9b93f3f0682250b6cf8331b7ee68fd8 凑合能用：使用现代的安全的例如 sha-256 之类额算法，存储为 5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8 现代手段：采用系统化的多次迭代的加盐哈希方式加密，例如常见的 PBKDF2 有兴趣的话可以把我上面的这几个 md5/sha-1/sha-256 拿来到 CrackStation 尝试解一下，就知道弱密码究竟有多危险了。 不过在开始之前要先进行一些简单的科普，以便于能够比较好的理解后面的东西。 ","date":"2022-07-08","objectID":"/posts/security-password/:0:0","tags":["Security","Password","Hash","Encrypt"],"title":"如何使数据库中的密码更安全：哈希、加密和加盐","uri":"/posts/security-password/"},{"categories":null,"content":"名词解释 拖库：简单来说就是有不怀好意的小老弟获取了你数据库的访问权限，甚至将其 dump 然后下载下来了 撞库：当你有某个库的用户名和密码明文了，其实可以拿着用户名密码去其他系统也尝试登录，因为很多人的用户名和密码在各个系统都是一样的，比如同一个邮箱注册的密码也用的完全相同 碰撞：一般是说 hash 碰撞，在有一对原文和对应的 hash 值后再找到一个 hash 值相同但原文不同的的方法 盐：加盐是一种在进行散列前向原文指定位置插入内容，将散列前的原文变得很长从而使彩虹表等方式失效的行为 ","date":"2022-07-08","objectID":"/posts/security-password/:1:0","tags":["Security","Password","Hash","Encrypt"],"title":"如何使数据库中的密码更安全：哈希、加密和加盐","uri":"/posts/security-password/"},{"categories":null,"content":"密码破解 常见的几种破解密码的方式有：暴力破解、用字典的暴力破解、彩虹表、社会工程学。 其中暴力破解最为暴力（废话噢），就是把所有可能的密码组合一个个的尝试一直尝试到天荒地老。如果是一个 8 位密码且只由大小写数字构成的话也有 62^8=218340105584896 这么多种组合，就算每秒钟能算出来十万个也需要七十年才能完全遍历一遍； 其次是用字典，字典就是将常用的弱密码、生日这种存起来，暴力破解的时候不再尝试所有组合，而是从字典中逐个取出来尝试，速度较快但是成功率降低了，毕竟只尝试了可能性最大的一小部分密码； 彩虹表也是类似于字典，但是字典每条只是一个密码，彩虹表是密码明文与 hash 之后的 key-value 对，当数据库被拖/泄露了之后，可以尝试拿泄露出来的 hash 值来反查密码原文； 社会工程学听起来好像很野，但是谁还没有过用生日和手机号作为密码解密的尝试呢 🤣 0X01 一清二白：明文存储 明文存储实现起来当然是最容易的，但是但凡能够访问数据库的就都可以知道用户用的是什么密码，这显然不太行。当然最严重的还是万一被拖库了，坏人老弟就可以随便登录任何一个用户了，还可以顺便拿去其他系统撞库，到时候用户受损失的可能不止当前这一个系统 任何时候都不要明文存储密码 任何时候都不要明文存储密码 任何时候都不要明文存储密码 0X02 掩耳盗铃：使用编码而非加密/摘要算法 还别说，真的有人用 BASE64 作为「加密算法」应用在用户的密码字段上，但凡了解过一点 BASE64 的应该都知道这东西是没有任何保密功能的，因为但凡有一点技术功底的都能很快猜到某个字符串是经过了 BASE64 编码的，只要猜到了那就跟明文一模一样了。 任何时候都不要明文存储密码 任何时候都不要明文存储密码 任何时候都不要明文存储密码 0X03 盘古之法：例如 md5 经常会有人说「我密码用了 md5 加密」，这里再重申一下 md5不是加密算法，sha-1 和 sha-256 也都不是，他们都是摘要算法 。 我们都说 md5 不再安全了，但是我们要搞清楚是为什么不安全，以及那里不安全。2004年的国际密码学会议（Crypto’2004）王小云证明了MD5可以被碰撞，至此，MD5不再安全。现在又十几年过去了 md5 碰撞已经可以做到在普通电脑上以比较快的速度实现了，但是也仅限于「碰撞」二字。所以可以注意到在网站上下载软件的时候防止调包的算法也确实从 md5 换成 sha-256 了。 但是 这里提到的不安全也只是基于碰撞的，如果想通过碰撞来尝试登录一个使用 md5 保存密码的系统那么首先需要这个数据库已经泄露出去了才有可能。不过既然都泄露了数据库，那么黑客肯定就可以尝试用彩虹表来反查了，这效率要比搞碰撞高得多。 所以说单独使用md5 确实是一件非常不安全的事。 0X04 祖宗之法：例如 sha-1 那 md5 不行的话 sha-1 可不可以呢？如果你的系统安全性要求 md5 不行的话，sha-1 其实大概率也不行。因为在证明 md5 可以被碰撞后第二年 sha-1 也就找到了有效的攻击方法，直到 2017 年也成功碰撞了。 因为 sha-1 和 md5 本质上都是散列函数，所以上面说的 md5 的东西也几乎都适用于 sha-1 了。从而也间接证明了单独使用sha-1 也是一件不安全的事。 0X05 凑合能用：例如 sha-256 好消息是sha-256还没有被碰撞成功，所以用来校验下载的文件是否正确现在普遍使用 sha-256 了。但是也只是没有被碰撞而已，彩虹表也还是能在网上搜到一些，而且在 cracker 的圈子里肯定还流传着更大甚至大得多的彩虹表，所以说如果你被拖库了那其实也是挺危险的。 0X06 现代手段：例如 AES/PBKDF2 那现代化的安全性很高的加密手段呢？比较常见的是小标题上的 AES/PBKDF2 这种，前者是对称加密后者的核心仍旧是散列算法。 首先来说 AES，AES 是一个对称加密算法，如果想用在密码保存上的话就相当于给密码加密之后存储，所以用于加密密码的密码一定要谨慎保管 ，否则当被人拖库之后还泄露了这个密码那就跟明文保存无异了。那没有泄露这个关键密码就一定安全了？那可不是噢，如果这个人自己注册了该系统，那他必然知道自己的密码明文是什么，现在还获得了密文，接下来就可以尝试攻击来找到这个关键密文了。（所以说千万要做好各种防护，被拖库是很恐怖的） 然后是 PBKDF2 了，这玩意儿的全称是 Password-Based Key Derivation Function 2 ，多少有些长了。这个方法除了需要明文以外还需要我们指定：散列算法、盐、重复次数这三个重要参数。这里是 Python 的一个示例，散列算法用了 sha-256 配合超长的盐和超多的重复次数进行了一次计算。 其中散列算法可以用 md5/sha1/sha256 这种，比较推荐使用 sha256； 盐则需要每个密码都随机生成一个不同的盐 ，而且盐也一定要长，过短的盐和相同的盐是没什么意义的； 重复次数可以多一些，迭代次数多了自然会更安全一些 0X07 总结 总结下来其实可以发现没有哪种方法是绝对安全的，只是破解的难度不同、泄露密文后的严重性不同罢了。而且别人破解你解你密码也是有成本的，如果说只是一个论坛的密码你用了 md5 + salt 的话其实基本也够安全了，毕竟破解这个系统获得的收益也没有那么大。但是如果涉及到了金钱，比如银行的密码，只有短短 6 位数字就可以保证你的安全，这种情况数据库那边对密码的加密肯定就是不怕复杂了。而且高安全性的系统往往也不止要一个密码就行，还有 OTP 的二次验证、有短信验证码、有人脸校验、还有声纹指纹虹膜校验。 0X08 参考资料 发现我参考的资料比我写的又好又全又早，感觉自己没有写的必要了 😪 加盐密码哈希：如何正确使用 没知识真可怕——应用密码学的笑话之MD5+Salt不安全 ","date":"2022-07-08","objectID":"/posts/security-password/:2:0","tags":["Security","Password","Hash","Encrypt"],"title":"如何使数据库中的密码更安全：哈希、加密和加盐","uri":"/posts/security-password/"},{"categories":null,"content":"0X00 前言 本篇文章是这篇「使用 git stash save 将暂存区命名」的重置版。因为根据 Google 的统计数据我得知某些问题的关键词搜索出来之后我的博客排行会比较靠前，所以把最容易被各位点击到的文章做了个重置计划，改进之前的一些不足，争取能够说的更清楚一些，也能节省各位一点点时间，希望能真正的帮助到从搜索引擎点进来的各位~ 0X01 极度精炼的使用说明 可以使用 git stash save \"message\" 的方式为 stash 起来的变动命名，方便后面再次使用。 stash 起来过后可以使用 git stash list 来查看已经被 stash 的列表，这里可以看到已经有两条了，值得注意的是 stash 的 id 每次都在更新，最近 stash 的是 0，1 就是上次 stash 的，以此类推 如果需要从新应用某个 stash 的改动，可以使用 git stash apply STASH_ID 的方式，例如使用 git stash apply 1 就可以重新应用 id 为 1 的这个 stash。如果要丢弃掉某个 stash 的话使用 git stash drop STASH_ID 就可以了 ","date":"2022-06-15","objectID":"/posts/git-stash-remake/:0:0","tags":["Git"],"title":"git stash 命名 / git stash 用法「重置版」","uri":"/posts/git-stash-remake/"},{"categories":null,"content":"0X00 换个方式定义函数 本篇内容不严格区分 function 与 method 🥹 我们都知道在 Python 中如何定义一个函数，只需要 def foo(arg_1, arg_2, *args, **kwargs) 就足够了。知道的稍微多一些呢可能知道「Python 中万物皆对象，所以函数的调用也只是调用了函数对象中的 __call__ 方法」，所以我们可以尝试用这种方式调用一个函数 def say_hello(): print('hello, world') say_hello.__call__() # output # hello, world 既然可以这样调用了，我们也就可以用类似的方法来定义一个假的 function，可以发现我们自定义了随便一个类，但是只要它实现了 __call__ 方法就可以被当做函数一样调用 class Foo: def __call__(self): print('hello, world') foo = Foo()foo() # output # hello, world 0X01 callable 根据上面的方法可知我们可以用 hasattr(obj, '__call__') 来判断某个对象是不是函数，事实上我也确实在同事的代码里看到过这样用的。其实 Python 内置了一个名为 callable 的函数可以用，不过跟 hasattr(obj, '__call__') 并不完全一致 class Foo: pass class Bar: def __call__(self): pass func = lambda x : x print(hasattr(Foo, '__call__'), callable(Foo)) # output: (False, True) print(hasattr(Foo(), '__call__'), callable(Foo())) # output: (False, False) print(hasattr(Bar, '__call__'), callable(Bar)) # output: (True, True) print(hasattr(Bar(), '__call__'), callable(Bar())) # output: (True, True) print(hasattr(func, '__call__'), callable(func)) # output: (True, True) 测试代码的第一行 Foo 类因为没有实现 __call__ 方法所以 hasattr 返回的是 False，而它是一个类，调用它就会实例化一个对象出来，所以它是可调用的，所以 callable 就返回了 True；第二行 Foo 类也没有实现 __call__ 方法所以 hasattr 返回的是 False，而且它又只是个普通对象，不是一个 class 所以导致 callable 也返回了 False；第三行因为 Bar 类实现了 __call__ 方法所以 hasattr 和 callable 都返回了 True；第四行也同理；第五行本是一个函数，所以也都返回了 True。 需要注意的是官方文档提到「callable 返回了 True 的不一定真的能调用成功，但是返回 False 的一定不能成功」，比如你强行给某个类设置了一个 __call__ 但是又不是函数，可能就会出现这样的问题。不过你非要这么写的话，小心被同事打死噢 🤔 class Foo: def __init__(self): self.__call__ = '???' foo = Foo()print(hasattr(foo, '__call__')) print(callable(foo)) foo() # output # True # True # Traceback (most recent call last): # File \"hello.py\", line 8, in \u003cmodule\u003e # foo() # TypeError: 'str' object is not callable 相关的官方文档：https://docs.python.org/3/library/functions.html#callable ","date":"2022-05-23","objectID":"/posts/python-callable/:0:0","tags":["Python"],"title":"Python 内置函数：callable","uri":"/posts/python-callable/"},{"categories":null,"content":"0X00 可以被强制转换的自定义类 但凡写过 Python 的人应该都用过int()这个函数了，而且也都知道这个是将其他类型转换成int类型的内置方法，稍微用的多一点的还会知道这个方法如果传入不能被强制转换的数据时会抛出TypeError的异常。那你知道如何让自己定义的类可以被强制转换吗？ #!/usr/bin/env python3 class A: def __int__(self): return 233 a = A() print(int(a)) # output # 233 而且按照官方文档来说的话，如果你的class定义了__int__()方法，则int(your_obj)则会返回__int__()的值，如果定义了__index__()则会返回__index__()，如果定义了__trunc__()也会返回__trunc__()。当然是有优先级的，优先级`int index \u003e trunc`，可以使用如下代码分别注释这些方法测试一下 #!/usr/bin/env python3 class A: def __int__(self): return 1 def __index__(self): return 2 def __trunc__(self): return 3 a = A() print(int(a)) 0X01 int 的第二个参数 那你知道它其实还能接收第二个参数吗？其实 int() 方法可以接受第二个参数的，也就是用于进制转换的参数。换言说就是可以用内置的int()方法将其他进制的字符串数据转换成10进制 #!/usr/bin/env python3 print(int('12345')) # 将字符串格式10进制数字转为整型 print(int('12345', base=10)) # base 默认就是10 print(int('12345', base=8)) # 8进制的12345转成10进制 print(int('FFFFF', base=16)) # 16进制的转成10进制 print(int('0XDEADBEEF', 16)) # 当然可以带对应的0X前缀 print(int('011111', 2)) # 将2进制的数字转成10进制 # output # 12345 # 12345 # 5349 # 1048575 # 3735928559 # 31 0X02 hex、 bin 等其他转换方法 上面提到了进制转换，这里也就顺便说一下这两个方法好了。其中hex可以将整型数字转成0x开头的16进制字符串，bin可以将整形数字转成0b开头的2进制字符串 #!/usr/bin/env python3 print(hex(12345)) print(hex(3735928559)) print(bin(12345)) print(bin(23333)) 还有几个之前从来不知道，这次写博客才在官方文档看到的用法，不仅可以控制大小写还能控制是否展示0X这种标记 In [1]: '%#x' % 255, '%x' % 255, '%X' % 255 Out[1]: ('0xff', 'ff', 'FF') In [2]: format(255, '#x'), format(255, 'x'), format(255, 'X') Out[2]: ('0xff', 'ff', 'FF') In [3]: f'{255:#x}', f'{255:x}', f'{255:X}' Out[3]: ('0xff', 'ff', 'FF') In [4]: format(14, '#b'), format(14, 'b') Out[4]: ('0b1110', '1110') In [5]: f'{14:#b}', f'{14:b}' Out[5]: ('0b1110', '1110') In [6]: '%#o' % 10, '%o' % 10 Out[6]: ('0o12', '12') In [7]: format(10, '#o'), format(10, 'o') Out[7]: ('0o12', '12') In [8]: f'{10:#o}', f'{10:o}' Out[8]: ('0o12', '12') 官方文档参考： https://docs.python.org/zh-cn/3/library/functions.html#int https://docs.python.org/zh-cn/3/library/functions.html#hex https://docs.python.org/zh-cn/3/library/functions.html#bin https://docs.python.org/zh-cn/3/library/functions.html#oct ","date":"2022-05-22","objectID":"/posts/python-int/:0:0","tags":["Python"],"title":"一些由 int 方法引出的小知识点","uri":"/posts/python-int/"},{"categories":null,"content":"0X00 最常见的持久化方式：挂载出来 相信各位学习使用 Docker 的时候都会出现过好不容易用 Docker 启动了个数据库容器，然后发现只要容器消失之后数据也就一起消失了的情况。然后通常来说会使用这么一个方法来解决问题：将宿主机的某个目录挂载到容器里，这样一来那个数据库容器就可以将数据内容和配置持久化地存储下来了。一般会使用这么一个方法将某个目录挂载到容器内部 docker run -dit --name new_container --mount type=bind,source=/Users/shawn/Downloads/test/test_dir,target=/test_dir alpine 。 下面图中首先创建了一个名为 this_container 的容器，并将其 /test_dir 目录和宿主机的 /Users/shawn/Downloads/test/test_dir 目录绑定到一起了。然后在容器里向 /test_dir 写入文件之后发现在宿主机的对应位置也是可以看到的，并且文件不会因为容器的生命周期结束而被删除（如果数据在容器内部的话当容器被删除后自然也就不在了）。 这种方式其实更适合宿主机和容器需要共享一些文件的时候使用，例如我本地的开发环境在容器里但是代码又在宿主机上的情况就很适合这种方案。当然也可以将数据文件、配置文件等通过这种方式挂载出来。一般使用 Docker 部署各类服务的时候，都会将配置文件通过这种方式共享出来，方便我们在宿主机上修改配置然后在容器内部生效。 0X01 另一种持久化的方式：卷 说来惭愧，我一直以为只有上面一种数据持久化的方式，直到昨天看书的时候才知道 Docker 还有一种叫做 Volume 的存在。如果说上面说的将数据挂载出来到宿主机的方式有点类似与宿主机与虚拟机共享目录的话，那通过 Volume 持久化文件就类似于给虚拟机挂载一个虚拟磁盘了。 我们首先可以通过 docker volume ls 的方式检查一下当前环境是不是存在已经创建好了的 Volume，通常来说如果你的 Docker 环境已经用过一段时间并且创建过各种不同的容器的话那很有可能已经有存在的 Volume 了。接下来我们通过 docker run -dit --name volatiner --mount source=bizvol,target=/vol alpine 来启动一个容器，后面 mount 的参数就是将名为 bizvol 的 Volume 挂载到容器的 /vol 目录上。这里值得注意的一点是即使你没有手动创建这个名为 bizvol 的 Volume 也是可以正常启动这个容器的，Docker 会为我们自动创建这个 bizvol Volume 的。接下来再使用 docker volume ls 就可以看到这个被创建好了的 Volume 了，我们尝试着向其中写入一点点数据，然后删除这个容器，可以发现这个 Volume 还是健在的，此时我们再启动一个新的容器使用这个 Volume 是没问题的，甚至启动多个容器同时挂载都是可以的，这些容器里都能正常读写该 Volume 的数据。 关于 Volume 有如下几个命令，这里简单介绍一下 docker volume create # 创建 Volume docekr volume ls # 查看当前存在的 Volume docker volume inspect # 检查 Volume 的详细信息 docker volume prune # 删除未被容器或服务使用的全部 Volume docker volume rm # 删除指定 Volume ","date":"2022-05-12","objectID":"/posts/docker-file-persistence/:0:0","tags":["Docker","Volume"],"title":"Docker 容器中的文件持久化","uri":"/posts/docker-file-persistence/"},{"categories":null,"content":"0X00 前言 \u0026 简介 说起来但凡各位用过 Linux 就应该用过 ssh 了吧，所以怎么使用 ssh 去连接一台服务器、怎么去配置 key 登陆而非密码、怎么允许/禁止 root 用户使用这种问题就不再过多讨论了，这篇文章来介绍一下 ssh隧道 这个东西。 说起来各位在生活工作中肯定遇到过这么一个情况，如图所示：我（A）自己想要访问一台机器（C）但是A和C是不通的，这时候有一台中间的机器（B），A可以访问到B且B和C也是通的（比如说梯子的应用场景）。r当然了你在中间的B机器上开一个 OpenVPN 肯定是可行的，但是一旦你的A连上了 VPN 之后所有流量也就都从B上走了，绝大多数情况下我们其实并不是很需要一个 VPN 而是需要一个轻量化的解决方案。这时候 ssh 隧道就是我们的一个很好的比较轻量化的解决方案，首先它不需要你在客户端和服务器上安装额外的软件（如果你连 ssh 都没装那就不说了）；其次它不需要配置文件，为数不多的配置直接写在命令行里；最后它不需要额外的守护进程，你不用了就直接 Ctrl + C 干掉当前进程就行了。 ssh 隧道其实本质上来说就是转发，那我们来依次介绍一下这四种隧道（转发）方法 0X01 动态转发 动态转发是在本地建立一个通往另一台机器的隧道，然后网络监听在本地的某端口，使用 socks5 协议。使用对应的协议和端口的出口流量都会被转发到另一台机器，再由它访问后转发给你。比如你在墙内是无法访问 wikipedia 的，但是你“恰好”有一台在海外的服务器，那么你就可以通过这种方式建立一个简单且临时的小梯子。 命令格式是：ssh -D local-port username@tunnel-host -N，比如说 ssh -D 1080 shawn@123.1.2.3 -N -D 是绑定后面的本地端口 -N 是说非交互环境，不需要执行命令，挂着就好 这样一来就可以在浏览器上配置代理了，配上 socks5://127.0.0.1:1080 之后就可以访问更广阔的互联网了 YOU ARE FREEDOM !!! 这种动态转发其实平时用到的机会并没有很多，因为开发过程中用代理还是比较麻烦的，并不是所有软件/工具都支持独立配置代理，直接配置在浏览器或者操作系统上的话又很容易误伤到其他的进程。如果想要更细致的管理还是需要用下面的本地和远程转发方案。 0X02 本地转发 本地转发也是在本地建立隧道，监听在本地端口，一切发往本地该端口的流量都会根据配置转发到另一台机器上。比如我前几天工作中遇到的一个情景：开发机是 macOS 的，客户用来联调的服务器在他们内网需要 VPN 才行，但是他们用的 VPN 客户端没有 macOS的。所以我在 macOS 上装了个虚拟机，虚拟机起连上了他们的 VPN 也就可以访问联调环境了，但是我的 macOS 还是不能直接访问。这时候就可以用 ssh 隧道来解决问题了。 命令格式是：ssh -L local-port:target-host:target-port username@tunnel-host，比如说 ssh -L 8080:123.123.123.123:80 shawn@123.1.2.3 -L 指的是建立一条本地隧道 local 这样一来我再curl -XGET http://127.0.0.1:8080的时候，流量就会顺着 ssh 的隧道一路直接到最终客户的联调环境了。这种本地转发的情况是非常常用的，这样一来本来没有通的两个机器也就通过这样一条隧道连接起来了。最重要的是我们只占用了本地机器的一个端口，并不影响其他任何的系统配置和任何进程。 0X03 远程转发 远程转发是在中转的服务器上建立隧道，跟本地转发最大的不同是：本地转发只是本地用户自己建立了一条隧道给自己用，但是远程转发可以将端口共享出来给多台设备使用。还是说上面我的那个联调环境，如果说只有我一个人自己联调就用本地转发是刚刚好的，而且不用再登录到远程环境上去，也不需要远程用户比较高的权限。但是说如果我们全公司都在进行这个联调，那就可以用远程转发，在中转服务器上把自己的端口开放出来让所有用户一起使用。 命令格式是：ssh -R local-port:target-host:target-port -N local，例如 ssh -R 8080:123.123.123.123:80 -N local -R 建立一条远程隧道 remote 这样建立起来的隧道就可以给其他机器使用了，现在所有可以访问中转机的机器都可以通过中转机的 8080 端口访问到客户的联调机。所以说这里的远程转发和本地转发除了命令执行地不同、可用的机器不同以外就没有什么比较大的区别了。 0X04 其它参数 这里再介绍几个额外的参数 -f 后台运行，这样 ssh 隧道就不用占用你当前的 shell 了 -C 压缩内容，原则上就是用 CPU 换带宽 -g 开放本地端口，如果本地转发的时候加了这个参数，那就跟远程转发差不多了 ","date":"2022-04-04","objectID":"/posts/ssh-tunnel/:0:0","tags":["Linux","Shell","OpenSSH","Network"],"title":"使用 ssh 命令建立网络隧道","uri":"/posts/ssh-tunnel/"},{"categories":null,"content":"0X00 先胡说两句 兄弟们我的年更文章它又来了～如果你看到这篇文章的时候发现发布时间是 2021-12-31 23:59:59 但你的手机电脑告诉你还没到这个时间，不要慌，一定是你穿越了。 我也不知道为啥莫名其妙就开始写年终总结了，而且今年已经是第三年了。人确实是一种很奇怪的物种，领导老师父母让你总结一年的时候暴躁不堪，根本不想写；但是自己突然来了这个念头之后就非常认真的写了一大堆，告诉我不是我一个人有这种“病”🙄 这次的总结打算换一种方法来写，大概分成了这么几个部分：去年的年度计划情况、年内做得不错的事、没做好的事、可以分享给各位的东西。 0X01 翻车的年度计划 2019 年度总结中我说“取法其上，仅得其中；取法其中，仅得其下”，但是今年就不一样了，今年的年度计划基本可以用四个字来形容：我是废物 😢（其实没这么离谱，听我慢慢道来） 今年就不给大家详细一个个的看我的年度计划了，简单说一下吧： 阅读两个有兴趣的开源项目源码【完全没看】 动手写两个自己练手新技能/知识的小项目【几乎没写】 了解 5 个专业技能【就学到了俩】 养成 4 个良好习惯【也只坚持了俩】 读完 24 本专业无关的书【读了 17 本】 平均每月发布两篇博客【只完成了 1/3】 了解几个奇奇怪怪领域的业余知识【确实了解了几个】 多出门走走-拒绝成为肥宅【平均每月一次，完成】 考一个手动挡驾照【是的我才考驾照】 买一台 PS5【原价压根买不着】 算下来年度计划完成了有一半吧，而且还有一些原本不在年度计划里的重要的事、好玩的事、刺激的事，所以整个年度计划虽然优点拉胯但是也没有完全拉 🤪 0X02 整的不太行 这个 2021 下来整的不太行的地方其实还挺多的，就比如上面说到的：完全没看的源码、没咋写的个人小项目、同样没咋写的博客 😅 不过其实除了这个以外，年初的时候还打算给自己减重到 73kg 结果现在好像还比年初重了一点点🤏 也说要打通健身环、挑战舞力全开全曲库五星，结果今年还是在玩旷野之息、smash、Halo、Forza 🎮 不过话虽这么说，我倒是没有很自责，因为前些天算了一下如果严格按照年初的计划来执行，可能每个工作日晚上需要两个多小时的时间，周末也是需要投入不少时间进去才能完成整个年度计划。假设真的每个工作日都给自己安排两个多小时的事情，那自己就几乎没有自由时间了。所以今年的年度计划我打算削弱一点，按照工作日每天 60 到 90 分钟，周末加倍来安排，看看成效会不会比今年更好一点。 0X03 哎呦，不错哦 当然今年也没有上面总结的那么惨了，不仅学到了些专业技能、业余还好，也还是读了十几本书，也考了驾照，并没有说一直随随便便玩了一年～ 而且今年收获了一些来自朋友的肯定和感谢，这可能是让我这一年比较开心的最重要的东西了。 工作上的事倒是没有什么太大的变化，感觉现在的工作基本已经把握住了，明年得想办法让工作效率再提升一些（不是说可能有同事看我才这么说的，是确实觉得自己的效率有点低，这个问题亟待解决）。 说起来今年最大的变化就在于出去徒步了，虽然都只是一些弱鸡级别的，但是还是挺有趣的🧗 今年印象比较深刻的几次经验是： 孟屯河谷的星空露营，那天晚上架着相机拍星空拍从七八点钟天黑一直到凌晨一两点钟，才拍到了几张说得过去的照片 乐山蛮王洞探险，一路爬行上坡终于第一次真正意义上的进到山洞里（有点像是旷野之息的复苏神庙），印象深刻的还有王师傅给我拍的除了我的影子哪儿哪儿都清晰的我的剪影照片 在石棉县城吃的便宜量大且好吃的烧烤和深夜的共享电瓶车车王争霸赛 王岗坪那个无敌的索道：30 度倾角越过一个山峰，你以为结束的时候越过一个山峰转成 50 度倾角继续上升，你以为结束的时候再越过一个山峰转成 70 度角继续爬上，单程有 20 多分钟速度又快风景又好，真的特别棒 还就是“飞拉达”真的太好玩了，强烈推荐尝试一波，成都附近的话平乐古镇就有～不过离谱的是我居然从开始到最后结束的时候，脚底一点点都没有抖，还以为自己会紧张害怕的。真的要去玩的话推荐给手机准备一个拉环，这样就不至于不敢给一起的朋友拍照了 🤔 最后就是都江堰的熊猫谷，小熊猫居然是散养的！！！ 0X04 万物皆可分享 想了想全年下来可以推荐给各位的东西有这么几个： ⛺️露营：跟一个徒步团或者自己冲都行，选一个天气合适的时候找一个光污染极少的地方，几个朋友整整烧烤看看星星，再扛着三脚架拍一些星空照片，还是非常棒的～ 🚶徒步：第一次徒步实际上是在 2020 年，但是今年才是次数比较多的一年。虽然我的徒步线路一只都没有多高的难度，最高难度也就是海拔 4000 米的徒步或者 16 公里的穿越，如果在徒步圈子里的话这种其实跟入门级差不了多少。打算明年去走两次 20 公里以上的线路的，挑战一下自己～虽然徒步走下来还是挺累的，但是在路上的感觉还是很棒，听起来有点离谱但确实是一种很棒的放松方式。如果各位有兴趣的话可以多多尝试一下。 🧗飞拉达：飞拉达其实就是一种高度保护下的攀岩运动，只要胆子大谁都能过。本来我报名之前还是有点虚，担心自己到了目的地之后又退缩什么的，不过实际上到了之后特别淡定。同队的十几二十几个人全都顺利通过了，而且没有任何人遇到意外情况，所以说还是挺安全的～ 如果你也想尝试一下趴在悬崖上拍照的感觉，想体验一下挂在峭壁上蹦跶的感觉，那还是很值得一去的。 📷摄影：今年初给自己买了台相机开始学习摄影（别问了，还是个菜鸡），你要说拍到了多震撼的大片吧倒也没有，但是手里拿着相机的时候确实会更主动的去发现生活中的美，自然也就收获了很多值得纪念的瞬间～你要说“手机不也可以拍照吗，而且效果也很好”的话，确实没必要反驳，但是相机能给到的其实不只是所谓仪式感和更好的效果，对我来说更重要的是“它能让我更主动的发现生活中的美好”。 🎮XBox：本来今年是要买一台 PS5 的，但是这都一年过去了还是价格很高（而且几乎没有独占游戏），所以选择买了一台 XBox Series S 来玩，效果居然格外的好。2500 块钱你连一块 GTX1660 都买不到但是却可以买到一台最高 2k/120 4k/60 的、至少未来 5 年不过时的、带有 500G 固态硬盘的、附送一个价值 400 快手柄的游戏机。如果你再出一个 3A 的钱还能买到 3 年的 XGP，可以在未来三年内免费玩到游戏库内各种微软、EA的游戏和其他工作室的各类大作，包括但不限于：Forza、Halo、双人成行、上古卷轴、八方旅人…… 当然还有其他的，不过最重要的最想推荐出来的基本就是这些了，各位感兴趣的都可以跟我交流 hhhhh 0X05 隐藏篇幅 My4xNDE1OSAyNjUzNSA4OTc5MyAyMzg0NiAyNjQzMyA4MzI3OSA1MDI4OCA0MTk3MSA2OTM5OSAzNzUxMCA1ODIwOSA3NDk0NCA1OTIzMCA3ODE2NCAwNjI4NiAyMDg5OSA4NjI4MCAzNDgyNSAzNDIxMSA3MDY3OSA4MjE0OCAwODY1MSAzMjgyMyAwNjY0NyAwOTM4NCA0NjA5NSA1MDU4MiAyMzE3MiA1MzU5NCAwODEyOCA0ODExMSA3NDUwMiA4NDEwMiA3MDE5MyA4NTIxMSAwNTU1OSA2NDQ2MiAyOTQ4OSA1NDkzMCAzODE5NiA0NDI4OCAxMDk3NSA2NjU5MyAzNDQ2MSAyODQ3NSA2NDgyMyAzNzg2NyA4MzE2NSAyNzEyMCAxOTA5MSA0NTY0OCA1NjY5MiAzNDYwMyA0ODYxMCA0NTQzMiA2NjQ4MiAxMzM5MyA2MDcyNiAwMjQ5MSA0MTI3MyA3MjQ1OCA3MDA2NiAwNjMxNSA1ODgxNyA0ODgxNSAyMDkyMCA5NjI4MiA5MjU0MCA5MTcxNSAzNjQzNiA3ODkyNSA5MDM2MCAwMTEzMyAwNTMwNSA0ODgyMCA0NjY1MiAxMzg0MSA0Njk1MSA5NDE1MSAxNjA5NCAzMzA1NyAyNzAzNiA1NzU5NSA5MTk1MyAwOTIxOCA2MTE3MyA4MTkzMiA2MTE3OSAzMTA1MSAxODU0OCAwNzQ0NiAyMzc5OSA2Mjc0OSA1NjczNSAxODg1NyA1MjcyNCA4OTEyMiA3OTM4MSA4MzAxMSA5NDkxMiA5ODMzNiA3MzM2MiA0NDA2NSA2NjQzMCA4NjAyMSAzOTQ5NCA2Mzk1MiAyNDczNyAxOTA3MCAyMTc5OCA2MDk0MyA3MDI3NyAwNTM5MiAxNzE3NiAyOTMxNyA2NzUyMyA4NDY3NCA4MTg0NiA3NjY5NCAwNTEzMiAwMDA1NiA4MTI3MSA0NTI2MyA1NjA4MiA3Nzg1NyA3MTM0MiA3NTc3OCA5NjA5MSA3MzYzNyAxNzg3MiAxNDY4NCA0MDkwMSAyMjQ5NSAzNDMwMSA0NjU0OSA1ODUzNyAxMDUwNyA5MjI3OSA2ODkyNSA4OTIzNSA0MjAxOSA5NTYxMSAyMTI5MA== 【打赏 9.99 元即可解锁隐藏内容】 0X06 总结(总结) 没错这是一个递归的总结，给总结再写个总结的人应该不多，但我算一个。 明年的计划还没做完，但是有一个大体的方向：适当减少专业和业余学习的安排，但是要 100% ","date":"2021-12-31","objectID":"/posts/2021-summary/:0:0","tags":null,"title":"2021 年终总结","uri":"/posts/2021-summary/"},{"categories":null,"content":"0X00 前言 如果秉承着「能用就行」的原则，那么这篇文章提到的东西基本都没什么卵用；如果秉承着「写更好的代码」的原则，那么这里提到的东西也许对你有所帮助。 内容主要取材自 Effective Python ，主要是作为自己学习后的一个输出而总结的这篇博客 0X01 使用 % 的 C 风格格式化 首先是沿用自 C 风格的使用 % 进行的字符串格式化方法： \u003e\u003e\u003e name = 'Shawn' \u003e\u003e\u003e job = 'developer' \u003e\u003e\u003e text = 'I am %s, a %s' % (name, job) \u003e\u003e\u003e text 'I am Shawn, a developer' \u003e\u003e\u003e height = 123.456 \u003e\u003e\u003e text = 'My height is %4d.' % height \u003e\u003e\u003e text 'My height is 123.' \u003e\u003e\u003e text = 'My height is %4f.' % height \u003e\u003e\u003e text 'My height is 123.456000.' \u003e\u003e\u003e text = 'My height is %4.8f.' % height \u003e\u003e\u003e text 'My height is 123.45600000.' 这种写法在写惯了 C 的人身上比较常见，比较熟悉而且也比较简单。不过这种写法有几个问题，首先就是当百分号右侧的变量数量发生变化或者类型发生变化的时候，程序很有可能因为类型转化出现不兼容的情况 （当然了，本来是 %s %4d 对应字符串和数字，现在两个都是字符串了当然就出错了）。如果要解决这种问题的话，必须每次修改都要检查百分号左右的占位符和具体数值是否能对应的伤，而且一旦占位符多了之后还很容易看花眼。 还有一个问题就是填充数值的时候通常需要对具体的值进行一些处理，比如保留某几位长度之类的，这样一来表达式可能会很长，从而显得很混乱 。 coffee_price_list = [ ('Americano', 15), ('Latte', 25), ('Cappuccino', 30) ] for index, (name, price) in enumerate(coffee_price_list): content = '%d. %-12s ----- %.2f' % (index, name, price) print(content) # output 0. Americano ----- 15.00 1. Latte ----- 25.00 2. Cappuccino ----- 30.00 我们来看 for 循环里面那行，是不是确实看起来乱乱糟糟的，这还只是三个占位符，如果更多的话就会更混乱了。 第三个问题是如果要用同一个值来填充多个位置，那就需要在右侧重复多次 （废话之：你想要几个就得写几个）。我们假设你有一个保证书模板，只需要填入姓名、错误和保证内容就可以生成出例如「我XX再也不YY了，我保证以后ZZ」的十万字长文。但是整篇文章里出现了大量的空位，需要填入这些 XX/YY/ZZ 怎么搞呢？你可能需要在后面写上不计其数的 '--------%s-------%s-------%s-------%s' % (xx, xx, yy, zz, zz, yy) 这种东西（别跟我说你要用 replace 那是另外的内容，这里只讨论字符串格式化😅）。 当然了这个问题也不是无解，我们使用 dict 来替换平时用的 tuple 就可以了，就是类似下面这种用法（虽然我从来没真的在代码里见过谁这么写） value_a = 'aaaaaaaaaaaaaaaa' value_b = 'bbbbbbbbbbbbbbbb' value_c = 'cccccccccccccccc' content = ''' %(val_a)s, %(val_a)s, %(val_a)s %(val_b)s, %(val_b)s, %(val_b)s %(val_c)s, %(val_c)s, %(val_c)s %(val_a)s, %(val_b)s, %(val_c)s ''' % { 'val_a': value_a, 'val_b': value_b, 'val_c': value_c } print(content) # output aaaaaaaaaaaaaaaa, aaaaaaaaaaaaaaaa, aaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbb, bbbbbbbbbbbbbbbb, bbbbbbbbbbbbbbbb cccccccccccccccc, cccccccccccccccc, cccccccccccccccc aaaaaaaaaaaaaaaa, bbbbbbbbbbbbbbbb, cccccccccccccccc 虽然这种写法解决了多次重复使用的问题，但是加重了第二点也就是代码更冗长 了，因为不仅要给变量做格式化，还要给每个占位符再设定一个 key 且为其匹配好。 最后就是因为每次都需要把 key 至少写两次（占位符那里一次，后面的字典里一次），甚至因为 value 过长还可能再把变量提出去单独定义一下，就会导致整个表达式非常长，比较容易出现 bug 且定位 bug 比较复杂。 0X02 另一种方法：format函数与方法 format 是我平时用的最多的一种方法了，比较常规的方法是调用str 对象的 format() 方法，例如下面这样 name_1 = 'shawn' name_2 = 'bluce' print('hello {}, hello {}.'.format(name_1, name_2)) print('hello {:\u003c10}, hello {:\u003c20}.'.format(name_1, name_2)) print('hello {1}, hello {0}.'.format(name_1, name_2)) print('hello {1}, hello {0}, hello {0}, hello {1}.'.format(name_1, name_2)) # output hello shawn, hello bluce. hello shawn , hello bluce . hello bluce, hello shawn. hello bluce, hello shawn, hello shawn, hello bluce. 第二种可能比较少见，不过规则比较简单，就是在花括号里写一个冒号，冒号右边可以用 C 方法格式化变量。第三四种就比较常见了，可以通过 index 来规定位置。 这种方法还有一些更高级的用法，例如在花括号里访问字典的 key 或者访问列表中的下标（好像也没见人这么用过） data = [ {'name': 'shawn', 'gender': 'M'}, {'name': 'bluce', 'gender': 'F'} ] print('{data[0][name]} and {data[1][name]} is {data[0][gender]} and {data[1][gender]}'.format(data=data)) # output shawn and bluce is M and F 0X03 更好的方法 f-string 插值格式字符串 在 Python 3.6 中引入的这个特性可以解决上述提到的问题，语法要求格式化的字符串前面加上一个f做前缀，就类似于之前的b/r这种。这里也同样支持前面 format 那里用到的格式化方法，例如 f'{name_1:\u003c10}, {value:.2f}' 这种。 一个简单的例子 name_1 = 'shawn' name_2 = 'bluce' print(f'hello {name_1}, hello {name_2}') # output hello shawn, hello bluce 我们现在再回过头来看一下最开始提到的四个问题：第一个 ，如果需要调整顺序，那么百分号左侧的正文要改，右侧的值也要改，就要改两次。现在没有百分号也就不再区分左右了，如果调整顺序那么就只调整一次就行，方便了很多。第二个 ，如果对填进去的值稍作处理可能会导致整个表达式变得很长。现在因为省略了百分号右边的内容，所以整个表达式还是精简了不少的。第三个 ，当某个变量/值要用多次的时候就需要左右共写两次那么多。用 f-string 方式的话，如果确实需要调用多次且每次都要进行修改（例如保留小数或是转成大写之类的），则可以考虑将其提取出去单独赋值，然后在格式化的时候用新值来代替，还能更加符合字符串格式化的语义。第四个 是说如果使用 dict 的话会使代码变多，现在不用字典了当然也就没有这个问题了。 下面是几种用法，看起来 f-string 并没有代码量少很多，是因为这个例子并不能很明显的体现出代码量少的优势，但是已经体现出可读性和维护性的优势了。如果一眼看过去，明显是 f-string 的用法最简单清晰明了。 data = [ {'name': 'shawn', 'score': 78.5}, {'name': 'ami', 'score': 89.0}, {'name': 'jack', 'score': 92.0}, {'name': 'amber', 'score': 99.5} ] print('------------- style_1 -------------') for item i","date":"2021-10-12","objectID":"/posts/python-string-format/:0:0","tags":["Python"],"title":"Python 中格式化字符串的几种方式","uri":"/posts/python-string-format/"},{"categories":null,"content":"0X00 背景 前两段属于并不那么重要的“故事”部分，如果切实需要一些数据恢复的经验和方法的话可以直接跳到0X02部分，给个一键三连就好（哦不对这不是B站视频，那你白嫖好了）。 故事大概是这样的，之前我有一台 Synology 的成品 NAS 用来存储我自己的照片、视频、音乐、电脑数据备份和喜欢的电影，不过因为它是单盘位的用了大概三年之后它就几乎满了，我也就开始准备升级 NAS。后来经过一番调研发现自己的需求其实不太适合用 Synology 的成品，因为它性价比低的同时附带的很多软件功能我都不需要，相比于自建 NAS 所需要的计算机知识我也正好具备，就选择自己搭建一台 TrueNAS 出来。准备的配置是CPU Intel G6400 + RAM 8G X 2 + SSD 120G + SSD 240G + HDD 8T X 3 + HDD 4T，其中双核四线程的 CPU 给这台 NAS 用已经是完全没有问题了，同时因为使用 ZFS 也就配备了 16G 的内存，8T X 3 作为主要数据存储并且搭配了 240G 的缓存盘；另外那块 4T 是从老群晖上拆下来的，作为独立存储使用；最后最小的 SSD 用来装系统。当时还整理了一篇博客来记录这件事（最近有一点点小改动）。 本来我那个 4T 的盘拆下来之后就一直没用，直到前几天我突然想把它也塞到我现在的NAS里去，正好把 OMV 换成 TrueNAS，再加一个缓存盘，重构一下自用 NAS 的架构从此就不再管它了。然后我就把重要数据打包一股脑塞到了那个小硬盘里准备重建 NAS（此时我自己的照片、视频、备份文件全部都在这个小硬盘里）。等了几天需要用的 PCI-E 转 m.2 的扩展卡和 PCI-E 转 SATA 的扩展卡都到了之后就开始重新装机。 把扩展卡和 SSD 之类的都装好之后，我特意把小硬盘的线给拔了，防止在重装系统或者其他的时候误伤到它。然后开始一路顺利的安装好了 TrueNAS 并且配置好了 3 X 8T 的存储池之后关机，准备接上小硬盘电源开始往大存储池里恢复数据。 0X01 事发 拆开机箱盖、拆开侧面版、给小硬盘插上线、装好侧面版、装好机箱盖、插上电源网线、开机、坐到笔记本前、打开 Firefox 开发者版、打开 TrueNAS 管理页面、输入用户名密码、登陆、找到存储池、格式化刚刚接入的硬盘…… woc，woc，woc 我干了啥！！！ 当我意识到我格式化了装满了重要数据的 4T 硬盘之后，本来整个人都很困，结果一秒钟之内就像喝了一百杯 Espresso 一样清醒了。当时脑子里的意识流差不多就是“卧槽完了、卧槽没了、卧槽出大事情了”，愣了几秒钟之后意识到了一个重要的事“刚刚格式化的是机械硬盘”，然后第一时间卸载了挂好的硬盘并且关机断电拆机拔线。 拔掉线之后冷静了一下觉得数据应该还能找回来，赶紧开始想办法。想办法的过程中又想到“我磁盘分区貌似是 xfs 来着，这个估计还难搞”，想到难搞之后整个人又是一愣🤣 然后我开始东找找西找找，看看有没有什么好办法可以恢复数据的。当时第一时间是想到找线下的数据恢复公司，但是找数据恢复公司的话就非常贵了，而且又非常麻烦，就打算想想办法有没有可能自己把数据恢复回来。又想到 DiskGenius 可以恢复，但是去官网看了一下这玩意儿好像还有点贵（官网售价 468 元人民币），当然如果肯定可以恢复回来 468 还是值得的，不过我还是想去找找看有没有便宜一点的方案。然后也陆续找了一些其他软件，要么就是没听过的小公司搞的，要么就是特别特别贵的。就在打算去买 DiskGenius 的时候发现了一个叫做 万兴恢复专家 的软件，当时第一反应是“这种名字一般都不靠谱，就像 21 天精通CPP一样不靠谱”。不过在调查了一下这个软件和公司之后，我发现背后的这家“万兴科技”是上市公司，且旗下有亿图和墨刀两大将，就打算去试试看了。 软件下载下来之后扫描了很长时间，虽然我费尽心思下载到的46G让子弹飞这种都没有扫描到，但是我最最最最重要的照片们几乎都扫描到了，然后我就眼睛都没眨的付费买了永久授权的这个软件，因为才160+块钱对于这么多重要数据来说真的不值一提，然后用了很长一段时间把数据恢复了回来。不过值得说的是，我的几十部电影无一幸存，但是自己拍的照片的幸存率超出 90%（估计的），这个我也没搞明白是为什么。 现在我已经把重要的数据都整理出来了，所以才有心情来整理这篇文章。说是一篇文章，但是前面这段可能对于看官来说都是废话，我也只是想记录一下这么个经历才把它写出来的。最后来总结一下遇到类似情况的时候应该怎么做吧。 0X02 总结 首先，首先，首先，数据恢复的最佳时机永远是在数据丢失之前做，换句话说就是 不要把自己的重要数据搞丢 。然后如果数据万一真的丢了，那么我总结了几点这次的经验也许对大家有那么一丢丢的帮助 还是强调不要删掉自己的重要数据，在删除东西之前最好 冷静 几秒钟，格式化磁盘之前最好 冷静 几分钟 如果万一删除了或者格式化了，请第一时间先弹出、卸载磁盘，保证 不要再像磁盘中写入任何数据 找到一个你信得过的数据恢复软件，接上存储设备先扫描看看（大多数收费的数据恢复软件扫描也是不要钱的，所以你可以先看看究竟能恢复出来多少数据，可以根据这个数量决定是不是要付费购买软件） 使用数据恢复软件将数据恢复出来 不要放在恢复的磁盘里 ，应该单独找个地方存储这些恢复出来的数据 什么？你说盗版软件？这种事情就不至于还用盗版软件了吧，别人的软件帮你找回了你最重要的数据，如果是万兴的年费版的话才99块钱，难道你最重要的数据连 99 块钱都不值？ 最后需要注意的几个小问题 如果是误删和“快速格式化”的话，找回来的几率都还比较大 如果是复写性质的格式化，那基本上也就别想恢复的事儿了 即使数据恢复回来了，通常情况下文件名也都彻底乱掉了 如果误删或者格式化之后又写入过文件，写的越多越没戏 如果你有 100 个文件，恢复率 99% 的话可能会丢一个 如果 100 个文件压缩保存，恢复率 99% 的话就彻底废了 最后的最后还有嘿嘿，如果你想要彻底毁掉你的硬盘，不想给别人拿着你硬盘恢复数据的机会的话 格式化的时候不要选择“快速格式化” 使用专门的工具全盘复写几次 上一条做不到的话就往磁盘里塞电影，塞满删掉塞满删掉连续个三四次 （毕竟你卖二手电脑，二手相机的时候肯定不会想成为下一个冠希哥吧 好了，我能想到的就只有这些内容了，最后希望大家永远永远永远不会用到这里面数据恢复相关的经验～ ","date":"2021-07-21","objectID":"/posts/data-recover-experience/:0:0","tags":["Data Recover"],"title":"记一次惨痛的数据恢复经验","uri":"/posts/data-recover-experience/"},{"categories":null,"content":"0X00 前言 首先声明这篇博客针对的是中级 Linux 用户，如果你还不清楚 Linux 中的基本权限机制 user/group/other 和 rwx 的话需要先去了解一下对应的基础内容才行。既然标题上写了是“不那么基础的权限”，也就能看出来虽然内容不是很基础，但是也不会很高深。 另外，这篇博客里提到的好多内容都是并不复杂的东西，但是非常零碎，也许你用了十年 Linux 还是不知道其中的一些小知识点，不过也没什么，毕竟这些知识点的使用率真的很低。 如果你看完了这篇博客有那么一点点收获，那我也算是完成目标了；如果你看完后发现所有的内容都是你以前就知道的，那我只能说你对 Linux 权限这部分的掌握超过了大多数人。因为我确实给周围好多人分享过这些内容，从刚实习的朋友到比我工作经历多很多综合实力也强很多的人，几乎没有谁是完全了解这些内容的。（所以说虽然这篇博客并不难，并不是什么高深的知识，但是我比较有信息让你从中获得那么一点点的收获） 本博客不涉及某个命令的具体用法，只起到一个让你“知道自己哪里不知道”的作用。如果想要仔细了解某个命令或者某个机制，可以自行搜索相关资料。 0X01 root 究竟是谁 “root是谁？”这个问题听起来很蠢，但是实际上好多人并没有思考过这个问题。我们都知道 Linux 中有一个叫做 uid 的东西，其实 root 用户指的就是 uid为0的用户 ，而非用户名为root的用户，我们可以通过 id 命令来查看用户的 uid。 而且值得注意的一点是 uid 是会被回收利用的 。也就是说你创建了一个用户，系统分配了 1002 这个 uid 后，如果删掉这个用户紧接着再创建一个新用户是会敷用 1002 这个 uid 的。假设你删掉了系统中的一个老用户，又创建了个新的用户，那么万一 uid 是重复的就有可能导致这个新用户拥有之前老用户的权限，这是一种很危险的操作。 0X02 一分钟带过的 rwx 然后进入正题，也就是真正意义上的权限：rwx/ugo 机制。不过这里就不细说了，假设你已经掌握了基础的这套权限机制。如果你之前不知道 rwx 是三个二进制位，从而用 111/101/110 来表示的 rwx/r-x/rw- 的话现在也知道了。 0X03 基础 rwx 之后的一级：ACL 我们会发现基础的 rwx/ugo 权限机制只能将权限分配给三种人：文件所属用户、文件所属组、其他。如果你想专门指定给某个用户某个权限的话，是做不到的。那么 ACL 就是来解决这个问题的。 用于操作 ACL 的命令有两个 getfacl/setfacl，分别用来读取和设置 ACL 权限。其中 getfacl 可以比 ls -l 看到更详细的权限信息。而且你可能注意到权限位最后的 + 了，这里是加号的时候 ls -l 出来的权限位和我们平时看的就不一样了，想要看的清晰些最好就使用 getfacl。 这里具体的展示方式各个发行版本可能会有所不同，有些在没有设置 acl 的时候后面什么都没有，有些则会留一个 . 在那里。而且 acl 在某些发行版上还需要额外安装，默认是不带的。所以需要具体发行版本具体分析。 0X04 suid sgid 和 sbit suid 和 sgid 之前先来看一下这一小段代码，这段 C 代码的唯一功能就是在 /root 下创建一个文件。 然后将这段代码编译成二进制程序，再给它赋予相应的用户和权限 这里就看到了一个“奇怪的现象”：这个二进制程序会在 /root 创建一个文件，但是执行这个二进制程序的用户并没有向 /root 写入文件的权限（第三行可以看到有一个红色的 [1] ，是指的上一条命令的返回值），那是怎么成功的呢？就是因为上面的两行命令：sudo chown root.root create_file_2_root 和 sudo chmod +s create_file_2_root。 这两条命令分别是：将文件分配给 root 用户；为文件设置 suid。那么这个 suid 究竟是做什么用的呢？ **设置了 suid 的程序，在执行过程中会临时变更为文件所属人的用户状态。**也就是说上面的 create_file_2_root 命令在执行过程中是 root 用户的身份，既然是 root 用户那向 /root 写入个文件也就没什么大不了的了。 所以 suid 的意思就是 set uid ID upon execution，那么 sgid 也就显而易见的是 set group ID upon execution 了。 唯一需要注意的就是，设置 s 权限的时候，需要已经拥有 x 权限。如果没有设置 x 就直接给了 s 权限，则在 ls 的时候会发现 S 会变成大写的标识，表示当前没有执行权限。 其中 suid 只能使用在二进制可执行程序上； sgid 不只是二进制程序，也可以用在目录上； 设置了 sgid 后，用户在此目录下的有效用户组将变成该目录的用户组； 设置了 sgid 后，若用户在此目录下拥有 w 权限，则用户所创建的新文件的用户组与该目录的用户组相同（默认情况下自己创建的文件所属人和组都是自己，如果与目录相同就自然会有风险） 还有一个跟 suid/sgid 不是很很相关，但是又比较相关（诡异的描述 hhhhh）的sbit 叫做 the restricted deletion flag or sticky bit，也就是用来标记限制删除的 bit。一旦文件被设置了 sbit 后就只有文件所属人和 root 才有办法删除了。好多发行版本的默认情况下 /tmp 这个目录就被设置了 sbit，因为你肯定不想你自己创建的临时文件被其他人删掉嘛。 当然，suid/sgid/sbit 都可以通过 getfacl 命令查看到。 补充一句，这个只能为目录设置，为普通文件设置暂时没有任何意义和用处。 0X05 umask 默认权限 umask 是配置在用户上的，用于指定用户创建的文件/目录的默认权限的功能。也就是说每个用户可以有自己的 umask，用来指导自己创建的新文件、新目录所具有的权限。 新文件默认权限 = 类型最大权限 (666/777) - umask（按位减而非按数字减） 最大权限问题其实我们可以简单理解成：文件为 666，目录为 777，因为目录默认必须有 x 所以比文件多 1。 具体算法可以按照如下来尝试计算，但是我总觉得自己哪里搞得不对，不过又怎么算都是对的，这里我对自己的正确性持保留意见，如果发现什么不对劲的地方可以随时联系我改正 。 umask 0022： 文件： r w - r w - r w - 666 - - - - w - - w - 022 ——————————————————————————————— r w - r - - r - - 644 目录： r w x r w x r w x 777 - - - - w - - w - 022 ——————————————————————————————— r w x r - x r - x 755 0X06 删除文件引发的权限问题 接下来就到了一个相对来说有趣的地方了，首先我们来带入一个场景，看看这个 为什么这个文件我已经是所有人了，且权限已经是 777 了还是不能删除呢？这就涉及到了一个子问题“目录是什么”。 都说 Linux 万物皆文件，其实目录也是一个文件，只不过相对特殊一点而已。这个文件里主要存储了 inode 和 filename，所以我们对一个目录里的文件进行改名和删除的时候需要和创建一样的 w 权限。只有拥有了对一个目录的 rwx 权限才可以进入到目录里并且删除一个文件。所以上面这张截图里不能删除文件的原因是 当前用户并没有当前目录的写入权限，所以不能删掉这个文件 。 如果给用户添加一个对目录的写入权限，那就可以删掉这个文件了。 接下来再看下一个例子，我们可以看到这个目录下有两个文件，我已经可以删除其中一个了，那为什么不能删除另一个权限完全一样的文件呢？ 答案在 setattr 这里。 Linux 中还有一个 attr 的东西，故名思义就是文件的属性了。我们可以给文件设置多个属性，例如下面的这些： A：文件或目录的 atime (access time)不可被修改(modified), 可以有效预防磁盘I/O错误的发生； S：硬盘I/O同步选项，功能类似sync； a：即append，设定该参数后，只能向文件中添加数据，而不能删除； c：即compresse，设定文件是否经压缩后再存储。读取时需要经过自动解压操作； d：即no dump，设定文件不能成为dump程序的备份目标； i：设定文件不能被删除、改名、设定链接关系，同时不能写入或新增内容； j：即journal，设定此参数使得当通过mount参数：data=ordered 或者 data=writeback 挂 载的文件系统，文件在写入时会先被记录(在journal中)。如果filesystem被设定参数为 data=journal，则该参数自动失效； s：保密性地删除文件或目录，即硬盘空间被全部收回； u：与s相反，当设定为u时，数据内容其实还存在磁盘中，可以用于undeletion。 其实还不止这些，但是我们通常会用到的也就这两个：a 和 i。其中被设置了 a 的文件只能被追加新内容，不能被删除或者复写已经存在的内容；而被设置了 i 的文件则不允许被删除，即使是天王老子来了也不允许被删除（除非你把它的 i 属性彻底）。所以说 a 属性通常会用在重要的日志上，从而保证日志不会被复写；i 属性通常会","date":"2021-06-02","objectID":"/posts/linux-unbasic-permission/:0:0","tags":["Linux","Permission"],"title":"Linux 中不那么基础的权限","uri":"/posts/linux-unbasic-permission/"},{"categories":null,"content":"0X00 前言 在正式开始之前我们先要搞明白一个事情，那就是「函数」和「方法」到底有什么区别。首先来看一下在 Python官方文档里的定义。 函数：可以接受零个或几个参数并向调用者返回一些值的一系列语句。 function : A series of statements which returns some value to a caller. It can also be passed zero or more arguments which may be used in the execution of the body. 方法：在类里定义的函数。 method : A function which is defined inside a class body. If called as an attribute of an instance of that class, the method will get the instance object as its first argument (which is usually called self). 但是一般大家并不会很认真的区分「函数」与「方法」，而且就算不区分也并不会对平时的交流甚至编码造成任何影响（起码我没有因为不认真区分它们导致交流出现分歧或者代码出现 bug 的时候）。所以这里列出来也只是提个醒，防止有人并不是很清楚这两个名词表示的含义。其实我就是因为要写这篇博客才去搜了一下它们到底有什么区别，以前都是管 Python 里的叫「方法」，管 C 里的叫「函数」，不知道有没有人也是有这种不良习惯的🤣 既然搞清楚了，那么这篇文章后面就用「函数」来称呼好了，因为这些特性与是否定义在类里没有任何关系。（估计这是我第一次也是最后一次认认真真区分这两个东西了 hhhhh） 0X01 万物皆对象 都说 Python 中万物皆对象，那当然函数也是对象，我们可以用下面这段代码来验证一下 #!/usr/bin/env python3 def hello(): print('hello, world') print('id is ', id(hello)) print('type is ', type(hello)) if isinstance(hello, object): print('石锤，就是对象') 运行结果可以看到定义好的函数有自己的 id，type() 的输出结果也证明了它对象的身份，最后的 isinstance() 就更是石锤了。 id is 140617210113904 type is \u003cclass 'function'\u003e 石锤，就是对象 0X02 函数的本质 那么既然函数也是对象，那么一个函数和一个普通的对象有什么差别呢？或者说函数之所以是函数，它的本质是什么呢？这里首先看一个 Python 内建函数 callable，看名字就能猜到这个函数能用来检测传进去的参数是不是可以调用的。下面这小段代码可以看到 callable 的用法 #!/usr/bin/env python3 def hello(): print('hello, world') foo = 3.1415926 bar = 'linux' print('foo callable: ', callable(foo)) print('bar callable: ', callable(bar)) print('hello callable: ', callable(hello)) 运行结果可以看出来，只有 hello 也就是这里唯一的函数是可以被调用的。 foo callable: False bar callable: False hello callable: True 这么看来只要我们搞一个 callable 的对象出来，就可以向函数那样调用喽？是的，只要我们给自己定义的类实现一个 __call__ 方法，那么这个类实例化出来的对象就是 callable 的。例如这样 #!/usr/bin/env python3 class SayHelloClass: def __call__(self): print('hello, world') say_hello_obj = SayHelloClass() print('say_hello_obj is callable: ', callable(say_hello_obj)) say_hello_obj() 我们从执行结果可以看到，这个类实例化出来的对象在 callable 这里返回了 True，且被我们通过与调用普通函数相同的方式成功的调用了。所以我们可以说函数和普通的对象本质区别就在于是不是 callable 的，而是不是 callable 的则取决于类有没有实现一个 __call__ 方法。 say_hello_obj is callable: True hello, world 0X03 可以被传递 既然函数都是对象了，那可以被当做参数或返回值传来传去也没什么特别的了。尤其是将函数作为参数传递，这在 Python 里有一个非常重要的概念是装饰器，东西很多就不展开说了。 0X04 *args 和 **kwargs def foo(xxx, yyy, zzz, *args, **kwargs) 这种函数定义方法不一定经常用，但是肯定见过不少了。就拿这个函数举例好了 #!/usr/bin/env python3 def foo(xxx, yyy, zzz, *args, **kwargs): print(xxx, yyy, zzz) print(args) print(kwargs) print('------------------------------------') foo(1, 2, 3, 4, 5, 6) print('------------------------------------') foo(1, 2, 3, 4, 5, 6, 7, 8) print('------------------------------------') foo(1, 2, 3, 4, 5, 6, a=7, b=8, c=9) print('------------------------------------') 这里可以很容易看明白这两个参数的作用，但是有一个小问题你可能也许大概不知道，或者没尝试过 #!/usr/bin/env python3 def foo(xxx, yyy, zzz, *args, aaa, bbb, ccc): print(xxx, yyy, zzz) print(args) print(aaa, bbb, ccc) foo(1, 2, 3, 4, 5, 6, aaa=7, bbb=8, ccc=9) 执行结果如下，这种把 *args 夹在中间的做法偶尔也会用得上。 1 2 3 (4, 5, 6) 7 8 9 关于 *args 和 **kwargs 还有一个小知识点，多数人可能都知道。定义函数的时候 args 和 kwargs 两个名字只是大家习惯使用的，真正让语法生效的是前面的星号，原则上这两个单词随便用什么都可以，不过源于「代码是写给人看的」这个理念，还是建议任何时候都是用 args 和 kwargs 这两个普遍使用的单词拼写。 0X05 🪆套娃 套娃就很好理解了，也就是说可以在函数里定义函数，而且在函数里定义的函数还可以 return 到外面去。 #!/usr/bin/env python3 def get_function(func_name): def say_hi(): print('hi world') def say_hello(): print('hello, world') def say_abaaba(): print('a ba a ba a ba') if func_name == 'say_hi': return say_hi elif func_name == 'say_hello': return say_hello else: return say_abaaba func_1 = get_function('say_hi') func_1() func_2 = get_function('say_hello') func_2() func_3 = get_function('say_你好') func_3() 这种用法要注意每次调用 get_function 的时候都会定义 say_hi/say_hello/sai_abaaba 这三个函数，如果外层函数频繁被调用或者内部函数耗时耗资源比较多的话要慎用这种方式，尽可能考虑将子函数挪出去。或者可以这么说：除非你非常确信需要将其做成子函数并且理解其会怎样工作，否则就不要使用子函数。 0X06 λ lambda 说起 λ 这个符号，应该挺多人第一次见都是在 CS 里吧，当时我还以为这个字是入口的「入」，让我从那里进去呢😅 说正事，Python 中的 lambda 函数被称为「匿名函数」，当时刚接触的时候觉得这个名字取的真烂。后来才明白烂的不是名字，而是当时看到的那个示例 add = lambda a, b: a + b result = add(3, 5) 这个示例完了之后就告诉我说「匿名函数讲解完了」，我当时人都傻了，心里还在想这不是有名字吗？这名字不就是add吗？然后又回去翻看书上的例子，才觉得这个名字取的确实没问题。（当然这个例子是我自己写的，书里可不会有","date":"2021-05-26","objectID":"/posts/python-function-feature/:0:0","tags":["Python"],"title":"Python 中函数的特性","uri":"/posts/python-function-feature/"},{"categories":null,"content":"2024 年更新了 NAS 方案 2024 年更新了 NAS 方案 2024 年更新了 NAS 方案 0X00 背景 本来家里有一台群晖的 DS118 单盘位机器装了一块 4TB 红盘，用了三年后存储空间已经告急了。再加上买了相机，有很多照片需要存储；还有两台 MacBook 的 Time Machine 需要备份，而且还有不少下载的电影时不时想要回顾一下，空间就非常紧张了。虽然这台群晖的体验确实是挺好的，不过因为我这款性能太弱了，每次上传照片的时候创建索引都会卡死很久，期间几乎所有操作都是无效的，而且因为是单盘位既没有数据冗余也不能扩展空间，就想着是时候给家里的 NAS 升个级了。毕竟再过不了多久这块磁盘也就被塞满了，到时候再研究迁移就会导致中间断层一段时间，还是不太好。 一般来说自己家里的 NAS 有几种方案，这些方案各有其优劣，都有适合和不适合的群体，可以根据自己的实际情况进行选择。 上下文提到的 FreeNAS 同时表示 FreeNAS 和 TrueNAS 群晖：简单易用、几乎不需要任何计算机专业知识、体积小功耗还低、软件套件强大、软套件强大、软件套件强大 ，但是比较贵。群晖就有点像是电脑届的苹果：你花了更多的钱不需要付出什么额外的东西就能获得 80 分的使用体验，但是如果你对它不满意想要改造一下以便获取 90 分乃至 100 分的体验，那是很难的。就比如你想给群晖升级硬件配置，几乎是不可能的（有些机型允许升级内存）。系统虽然提供了 ssh 连接，但是由于定制化过高也导致我们不敢进去改一些配置； 黑群晖：不推荐 ，没有别的理由，只是因为盗版； 威联通等：比群晖便宜，易用性可能比群晖弱一点，不过也很适合不怎么具备专业知识的用户。如果想省心还想省钱，可以考虑用威联通之类的来替换群晖； 自建 TrueNAS：需要自己购买硬件攒机，需要自己安装操作系统进行配置，各种基础存储以外的功能都需要自己手动安装配置。所以不适合没有计算机专业知识储备的同学（当然你也可以先在虚拟机里尝试一下）。而且需要注意的是 FreeNAS 基于 FreeBSD，并不是Linux、并不是Linux、并不是Linux ，所以即使有一些专业知识储备的同学也要注意到这一点。不过正式因为 FreeNAS 基于 FreeBSD 所以也就带来了完整且原生的 ZFS 支持。硬件方面毕竟是自己攒机装系统，性价比什么的当然就很高了； 自建 openmediavault：同样需要自己攒机装系统，所以性价比依旧很高。但是 OMV(openmediavault) 是基于 Debian 的，所以普适性更强一些。虽然系统性能上貌似不如 FreeNAS 不过用起来也还不错，因为我自己对 Linux 比较熟悉就也选择了这个方案； 自建纯 Linux：这个自由度是最高的，但是也是最折腾的。不仅要自己装机装系统，还得配置各种服务，其他方案上一键的 Samba 服务在自建 Linux 下都要配置一会儿。不过纯 Linux 的自由度是最高的，也能选用自己最熟悉的发行版本； Windows server：不推荐，理由和黑群晖相同，除非你愿意出钱买 Windows server 授权（$501） 路由器插硬盘：这种方式适合对数据安全性要求低且功能要求更低的用。因为几乎只支持上传下载，而且可定制的功能还特别少，不过好在成本最低。如果只是插上去用其他设备看个电影什么的，路由器插硬盘的方案还是可以试试的 综合下来最后我选择了自建 openmediavault，因为群晖给我的很多功能我都用不上，比如说在线 Office 和 Video Station 等，反倒是性能太弱有点接受不了；TrueNAS 的话毕竟是 BSD 我没什么把握在出问题的时候可以修复它；纯 Linux 太折腾了，不想花那么多时间在上面。 0X01 硬件方案 硬件我是先选的机箱，需要小一点还得能塞进去三块硬盘；CPU 选便宜点的就行，再弱也不至于是群晖那种；内存是从朋友那儿搞来的；电源选了个大牌子的普通电源，功率比较小的也没问题；主要是硬盘，选了三块 PMR 的 8T 企业级硬盘（非氦气） 类型 型号 价格 CPU 奔腾 G6400 2C4T 629 内存 铭瑄 DDR4 8G X 2 散热 ID-COOLING IS-30 89 主板 七彩虹 H410M-M.2 PRO V20 459 电源 振华 铜皇 450W 铜牌 259 机箱 先马趣造 i’m + 防尘罩 348 系统盘 铠侠 m.2 NVMe 256G 249 数据盘 西部数据 PMR 7200rpm HUS72 8T 256M X 3 2700 其中还有在买了之后京东保价了的，最后算下来花了差不多 4500 块钱。转过去看了一下群晖的 DS420（四盘位2020年的型号）空机器就已经这个价格了，性能还远不如我这个，所以性价比来说还是很高了。不过这里还是要提示一下，自己组装需要一定的计算机专业知识和技能，否则就算装机装好了后面的软件也够头痛的。 **震惊！！！震惊！！！震惊！！！**我 900 一块买的硬盘现在淘宝已经快 3000 了，一块顶我前两周的三块了，矿老板牛批！！！ 0X02 软件方案 软件上我选择了 openmediavault 方案，因为基于 Debian 还算比较熟悉。三块磁盘组了 RAID-5 阵列，虽然 RAID-5 没有那么靠谱，但是三块企业盘组 RAID-5 还是可以的。RAID-5 是磁盘越大越多越不靠谱，我这种三盘还行。然后远程访问是用的 Samba 协议，macOS 自己的 AFP 已经放弃且全面转到 Samba 了；电视手机和 iPad 都可以轻松访问，macOS 的 Time Machine 也可以直接用，很方便。 RAID-5 最少需要三块盘（最好是容量相同的），拿其中一块的容量用来做奇偶校验，也就意味着 n 盘的 RAID-5 存储空间有 n-1 那么多。安全性的话是允许坏一块盘，如果坏两块的话，那数据就没了 扩展的功能，比如系统监控我用的是NetData，下载用了 AriaNg 和 Transmission，照片库用的是PhotoPrism，然后所有服务均使用 Docker 进行部署。其实各种服务可选的有很多，可以自己找适合自己的来部署。一般常见的就是需要部署一个文件管理器、一个下载工具、一个照片库和一个电影库，如果还有其他需要的话可以在 GitHub 或者其他平台找找看，比如也可以自己在家里搭一个 Gitlab/Gogs 或者虚拟化平台之类的方便开发测试。 其中电影库还没考虑好用什么方案，Plex不太方便、Radarr又不太好用、Kodi又需要给客户端全装一遍。 0X03 数据迁移 数据迁移方案其实很简单，新 NAS 装好后开个 Samba 然后在老群晖里挂载一下，然后登录到群晖的管理界面上复制粘贴就行了。唯一需要注意的是，复制的时候硬盘疯狂读写有点吵，可以选择出门前复制，玩一天回来就搞定了。 或者不方便挂载后复制粘贴的话，使用 scp/rsync 之类的也没问题。 0X04 综合体验 现在用了一周多的时间了，大面上体验还算不错。现在内网访问时候的传输速度在 120～140mb/s 的水平，不管是备份恢复数据还是直接看存在里面的高清电影（4k 50～70G）都没有任何问题，一般只有在拖动进度条或者刚刚打开电影的一瞬间会卡一秒钟的样子，毕竟还是需要缓存一下。 先说说小问题： 管理界面真的不好看，如果说群晖的 DSM 是 Windows 10 的话那 openmediavault 的管理洁面就像是 Windows 2000； 初次配置太麻烦了，毕竟很多服务都要自己搞，没有一些专业知识储备的话一整个周末都可能弄不好； 7200 转的企业盘跟 NAS 专用的红盘确实比不了，声音比较大，不过不读写的时候还是很安静的； 体积大，虽然先马趣造已经算是不错的小机箱了，但是跟 4 盘的群晖比还是大了很多； 再来说说优点： 性能充足，即使是我一次上传上万张照片，后台用 tensorflow 做分类，CPU 都没有爆满，跟不说那双通道 16G 内存了； 自由度高，64bit X84 4C4T 的CPU配合 16G 内存可以运行绝大多数常见服务，需要什么服务用 docker pull/start 一波就有了； 扩展性起飞，如果你不在乎机箱大小的话，完全可以配个 4 条内存 8 个盘位； 便宜、便宜、便宜 0X05 注意点 用了这段时间下来，总结了几个点需要注意的 NAS 新手且没有什么计算机专业知识储备的，不建议自建、不建议自建、不建议自建 。除非你只是玩玩，了解一下，否则万一不小心数据搞没了就是天大的麻烦。数据无价、数据无价、数据无价 自建 NAS 是需要一定硬件和软件动手能力的，而且也会消耗比较长的时间，如果觉得有兴趣再考虑这种方案，否则建议用现成的群晖或者威联通这种 内网穿透可以考虑用 frp 自建 NAS 要善用 docker，也不是说多吹捧 docker，只是说在你服务崩了一个的时候不至于牵连到系统 NAS 即使有了 RAID 也还是建议将最重要的数据有额外备份，冗余不等于备份、冗余不等于备份、冗余不等于备份 ","date":"2021-04-19","objectID":"/posts/nas-build-2021/:0:0","tags":["NAS"],"title":"我的家用 NAS 方案","uri":"/posts/nas-build-2021/"},{"categories":null,"content":"0X00 什么是 ARP 首先纠正一种说法，ARP就是“地址解析协议”，所以严格来说不应该说“ARP 协议”，因为ARP Address Resolution Protocol就已经包含了Procotol了，说“ARP 协议”就相当于是“地址解析协议协议”，很鬼畜。 我们知道网络中寻找其他机器需要用到对方的 ip 地址，但是在局域网内两台机器之间通信是不用 ip 地址直接通信的，而是要用到 mac 地址。而且我们一般说的交换机也是二层交换机，现在假设两台电脑插到一个交换机上去，通过 ip 能找到对方吗？显然不能。因为交换机只支持到链路层，然而对于链路层来说它并不知道 ip 是个什么，ip 需要到再上一层的“网络层”才能发挥作用，所以在这种情况下就需要用到 mac 地址来通信了。 当一个数据包需要被发送到某一 ip 地址的机器上去时，最后一步就需要找到 ip 地址对应机器（严格来说是网口）的 mac 地址，从而进行通信。那么在这一步里“通过 ip 地址找到 mac 地址”的解析协议就被称之为：地址解析协议，也就是ARP: Address Resolution Protocol了。 0X01 ARP 的工作 我们知道 arp 是将 ip 和 mac 地址进行转换映射的，那肯定不是每次都要去转换的，要留个缓存的嘛。我们可以使用 arp -a 来检查当前机器上的 arp 高速缓存表，命令在 Linux 和 macOS 上可用，Windows 上不确定各位可以试试看。我这里是防止隐私问题把一部分数据给人工打码了，正常输出的就是缓存表里每个 ip 对应的 mac 地址。 ? (192.xxx.xxx.1) at 4:xx:xx:xx:xx:0 on en0 ifscope [ethernet] ? (192.xxx.xxx.103) at (incomplete) on en0 ifscope [ethernet] ? (224.xxx.xxx.251) at 1:xx:xx:xx:xx:fb on en0 ifscope permanent [ethernet] ? (239.xxx.xxx.250) at 1:xx:xx:xxx:xxx:fa on en0 ifscope permanent [ethernet] ARP 的工作流程也是很简单的，当机器 A 想给 10.0.0.4 发一条数据的时候的时候： 检查自己的高速缓存表，发现没找到（有点类似与 DNS 先找 /etc/hosts 的操作） 机器 A 发出广播“你们谁是 10.0.0.4， 告诉我你的 mac 地址” 因为是广播，所以 BCD 都会收到此条数据 CD 因为不是 10.0.0.4 所以忽略 B 发现是在找自己，回消息“我是 10.0.0.4，我的 mac 地址是 1:0:2:7f:af:fa” A 收到了回馈，并将 10.0.0.4 -\u003e 1:0:2:7f:af:fa 记入自己的缓存表 所以 ARP 概括成一句话就是“广播一条消息：如果你是这个 ip 地址的拥有者，那么请回答你的 mac 地址”。 0X02 ARP 的数据包 一个 arp 数据包有 42 个字节，分成两部分：数据部分和链路层封装的首部。 首部：以太网目的地址、以太网源地址、帧类型； 数据：硬件类型、协议类型、硬件地址长度、协议地址长度、op、发送端以太网地址、发送端 IP 地址、目的以太网地址、目的 IP 地址（其中以太网地址就是指的 mac 地址）。 在整个数据包中我们最需要关注的是下面这 6 个字段以太网目的地址、以太网源地址、发送端以太网地址、发送端 IP 地址、目的以太网地址、目的 IP 地址。哎是不是发现有些数据有重复，比如发送者的 mac 地址就重复了？回忆一下 OSI 模型，因为工作在链路层的程序接受到的数据已经被链路层拆解了，也就是说 arp 程序拿到的数据是没有首部的，所以想要判断这个数据到底是从哪儿来的就需要在数据里存一份才行。 现在我们来简单看一下这几个字段: 以太网源地址、以太网目的地址：这个是首部要用的，要知道从哪儿发到哪儿。这里需要注意的是，广播的时候，目的地址是 ff:ff:ff:ff:ff:ff ，这个地址在 mac 地址中表示广播； 发送端以太网地址、发送端 IP 地址：这个是数据里的部分，作为 arp 发出去后，别人响应的时候就是从这里取的地址返回给你 目的端以太网地址、目的端 IP 地址：同样是数据中的部分，作为 arp 响应的时候，这里的“目的端”指的就是发送 arp 广播请求的那个机器 我们来看一个实例好了，我自己笔记本配置的网关是192.168.0.1，那我们来看一下这个找网关 mac 地址的流程。上面这张图是我发送出去的广播，下面这张图是网关给我的回复。（各位有兴趣的话可以下载一个 Wireshark 来抓包看看，挺有意思的） 这里可以看到这么几个值得注意的点： 发送 arp 请求的时候，首部的接收者是 ff:ff:ff:ff:ff:ff，即广播地址； 发送 arp 请求的时候，虽然不知道目的 mac 地址，数据里仍然携带了，只不过标记为了 00:00:00:00:00:00； 响应时并非单纯的将请求数据包中的 00:00:00:00:00:00 替换成自己的 mac 地址，而是将自己的数据仍旧放在“发送者”的位置上。 不过仔细观察一下这个协议会发现一个问题：它好像非常不安全。 0X03 ARP 欺骗 正常来说有人在吼“谁是 xxx 这个 ip 地址的拥有者，告诉我你的 mac 地址”，应该只有一个人回复或者干脆没人理。但是如果这时候网络里出现了一个小老弟，他很不老实，明明自己不是这个 ip 的拥有者却一直再回复“我是，我的 mac 地址是 xx:xx:xx:xx”，那会发生什么？如果广播要找的是一台普通的机器，那么在这个小老弟的干扰下可能他就会网络不稳定，或者干脆断网；如果这个小老弟打算假冒自己是网关（也就是别人在广播找网关的时候，他站出来疯狂回复），则可能导致整个局域网挂掉。 网断了还是小问题，如果这个小老弟冒充自己是网关，并且在自己本地做了个转发，将流量再转到真实网关上去会怎么样？局域网内其他机器并不会断网，但是却因为都以为这个小老弟是网关所以一切流量都会发送到这个小老弟这儿来，再由他转发到真网关。这时候这个小老弟就监听到了整个局域网内所有的数据包。如果这时候你的流量是不加密的，比如登录了一个 http 的站点，那小老弟就可以用上面提到的 wireshark 等工具抓到你的 htt 流量并轻而易举的看到你的登录密码，就算你没登录也能轻而易举得看到你的 cookie 从而模仿你登陆。即使你加密了，用上了 https 协议，也不能完全不泄露隐私。比如你登陆了某 PxxHub 正找视频呢，被人监听到流量抓包了，虽然他不能登录上你的号，但是他可以看到你所有的 http 请求的 url 哇，这样一来你看的所有视频这个小老弟都会知道。下次见到你直接跟你聊起你前一天晚上看的视频，岂不是很恐怖😱 上面提到的：假装自己是网关的操作，就叫做“ARP 欺骗”；后面把自己假装成网关后再做转发，从中监听并篡改流量的行为就叫做“中间人攻击”。 那就没有什么解决方案了吗？当然有 0X04 防护 ARP 欺骗 防护的话有两种常见的方案：从网关上防护和从主机上防护。从网关上防护的一个方案叫做 DAI: Dynamic ARP Inspection 动态 ARP 检测，从主机上防护主要就是“不要连来路不明的 Wi-Fi，且自己的 Wi-Fi 密码要足够复杂”，给个人设备装防火墙。 其中 DAI 的大致工作是这样的：如果你有一台具有 DAI 功能的路由器（不太可能，因为这玩意目前只在一些高级的企业级路由器上才会有）那么这台路由器可以做到 ARP 欺骗的拦截。首先我们知道连接到网关的机器，网关当然知道他的 mac 地址对吧，再加上 DAI 这个功能可以搞到他的 IP 地址，这样一来如果他发出的 arp 响应不对就会被网关发现直接 drop 掉，甚至还可以对这个地址进行“惩罚“。 针对自己主机的防护我们能做的主要就是装 ARP 防火墙，防火墙可以绑定正确的 ip 和 mac地址，可以识别网络中的 arp 扫描行为。 所以简单来说：把路由器换成带 DAI 的企业级路由器、不要乱连 Wi-Fi、把自己 Wi-Fi 密码设置地复杂点、装个安全软件（这个安全软件，说起来真头痛，反正注意鉴别，别到时候攻击没来自己先引狼入室了🤣）。 ","date":"2021-01-16","objectID":"/posts/arp/:0:0","tags":["Network","ARP"],"title":"地址解析协议 ARP","uri":"/posts/arp/"},{"categories":null,"content":"0X00 什么是网桥 普通的桥就是连接本来不通的路的基础设施，网桥就是用来本来不通的网的设备。但是这里介绍的不是真正意义上的物理设备，而是在 Linux 上创建的虚拟设备。Linux 上不仅可以创建虚拟网卡，也可以创建虚拟网桥，所以说我们在 Linux 环境下学习网络知识确实是一个不错的选择（除非你有钱买一堆物理设备🤣）。 桥接器（英语：network bridge），又称网桥，一种网络设备，负责网络桥接（network bridging）。桥接器将网络的多个网段在数据链路层（OSI模型第2层）连接起来（即桥接）。 – Wikipedia 0X01 网桥有什么用 网桥的本质就是将互不连通的网卡接到一起，原则上只有这么简单的功能，具体在这个功能之上能做什么事就看我们自己了。 如果用过 Docker 的话可以看一下自己本地的网络拓扑，实际上 Docker 的容器间通信和外网访问也是用到了 bridge 的。 最直观的一种操作就是：用一台双网卡机器串在两台主机之间，用来监听甚至修改数据包。乍一看这好像是搞坏事要用的技术手段呐，的确。如果有人拿一个双网卡的小机器（比如树莓派）串在了你的光猫和路由器之间，那么连接到路由器的所有外网请求都会经过这台小机器。如果你的流量不加密，比如用http协议或者ftp协议这种，那别人是可以看到中间的所有内容的，包括登录的密码之类的。所以这也是“不要随便连接来路不明的 Wi-Fi” 一大理由。 当然也不只是做坏事，现在的好多 WAF（Web Application Firewall）都是可以支持 “透明模式部署” 的，也就是说把这种安全软件所在的机器串在你的网关和 web 服务器之间，就可以在不破坏网络拓扑的情况下做到安全防护。 如果你想用这种方式来做软路由、绕开学校的网络共享限制，或是做流量分析都是可以实现的。 0X02 实验环境 如果要跟着做一下的话，需要简单配置一下环境： 首先我们需要三台虚拟机，有些人用Virtualbox、VMware Workstation、有些人又在用VMware Fusion甚至有直接用 KVM 的，每个的操作都不太一样，我们最终都是需要有一套这样的环境。 三台虚拟机，server_1 有一张网卡，配置静态 ipv4 地址为 192.168.1.1/24，server_2 有两张网卡，分别与 server_1 和 server_3 相连，server_3 有一张网卡，配置静态 ipv4 地址为 192.168.1.2/24。当然如果虚拟机软件不能让两台机器直连的话可以通过在机器间加路由器或者交换机的方式将其连接起来。 此时 server_1 和 server_3 的 ip 虽然在同一网段，但是应该是不通的。 如果这时候你的网络已经通了，那证明虚拟机和虚拟路由器配置有误。有一点是需要确认的：如果你用到了虚拟路由器的话，路由器要禁用 DHCP 或是给两台路由器配置不同的网段例如 1.1.1.0/24 和 1.1.2.0/24 0X03 如何配置网桥 好的我们开始配置网桥，其实过程很简单，总共分四步：创建网桥，将 server_2 的 ens33 网卡连接到网桥上，将 server_2 的 ens34 网卡连接到网桥上，将网桥启用。 首先在 server_1 上执行 ping 192.168.1.2，此时网络是不通的，不要停，一直叫他 ping 就好，一会儿通了可以瞬间看到效果。 在 server_2 上执行 ip link add name br0 type bridge # 创建一个名为 br0 的网桥 ip link set dev ens33 master br0 # 将 ens33 接到 br0 上 ip link set dev ens34 master br0 # 将 ens34 接到 br0 上 ip link set dev br0 up # 启用 br0 这时网络应该通了 ip link set dev ens33 nomaster # 将 ens33 与 br0 断开连接 ip link set dev ens34 nomaster # 将 ens34 与 br0 断开连接 ip link set dev br0 down # down 掉 br0 ip link del br0 # 删除网桥 ","date":"2021-01-09","objectID":"/posts/network-bridge/:0:0","tags":["Bridge","Linux","Network"],"title":"在 Linux 中使用网桥 bridge","uri":"/posts/network-bridge/"},{"categories":null,"content":"0X00 前言 一年前的这个时候还在想，2020 年这个年号听起来很科幻，然而 “科幻” 这个词还是还是太瞧不起 2020 了，这一年简直是“魔幻”。 最近这几年来，过得最不舒服的应该要数今年的前两个月了，疫情刚刚爆发，武汉封城，人也不出门，就在家里当咸鱼。本来就焦虑的人们每天看到的新闻也只能让大家更加焦虑，每天看到的都是 “口罩、防护服没有了”，“武汉封城”，“急寻密切接触者”…. 不过好在也有火神山雷神山这种好消息，也有后面武汉解封的振奋，还在今年的最后一天还看到了“国产疫苗批准上市，免费接种”这种令人愉悦的消息。如果要真是像🇺🇸️那么搞，真不知道还有没有命在这儿写 2020 年度总结了😵 0X01 2020 年度计划 ❗️ 写 31 篇博客完成率：63% ❕️ 欣赏 50 部电影完成率：82% ✅️ 坚持 5 个小习惯完成率：100% ✅️ 阅读 20 本书完成率：100% ✅️ 了解 4 个新领域完成率：100% ✅️ 提升 5 个专业技能点完成率：100% ✅️ 减重 15 斤到 150 斤 完成率：100% ✅️ 换一份新工作完成率：100% 其实总结下来看的话，这个年度计划表我还算是完成度比较高的了。疫情期间所有计划都是完全停止的，一点进展没有，甚至“减重 15 斤以达到 150 斤”这事儿难度还加大了，毕竟成天在家里坐着，想吃什么还自己做来吃，不胖才怪😅。虽然后面想起来疫情其实并没有直接影响到我计划表里的任何一项，但是当时整个人都是晕糊糊的，所以那两个月也就这么晕糊糊得过去了。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:0:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"写博客 这部分是达成率比较差的，主要是两部分原因：首先是今年自己的提升确实不多，太难整理了；还有一个是因为好多自己学到的东西学习资料都已经写的太好了，感觉自己再复述一遍不仅浪费时间还没有意义。所以下一个年计划里，博客这部分已经被我重构了。明年换个思路来写博客，不知道会不会比今年好一些。 其实坚持写博客这件事还是挺有意义的，不论是“学习最好的方法就是教别人”，还是“好记性不如烂笔头”，总之确实坚持着写了不少。在写的过程中还是有不少收获的，找工作的时候别人也会把持续维护的原创博客作为一个比较大的加分点。当时面试的问题都有些是直接从我博客里抽出来问的，看看是不是真的在认真写，是不是真的学到了东西； ","date":"2020-12-31","objectID":"/posts/2020-summary/:1:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"欣赏电影 其实好多人跟我说 “就看一电影，还至于列到年度计划里？”，其实我觉得还是可以列进来的。就比如我自己在这条计划里有一个备注是 “不少于 30 部豆瓣 top250”，因为有好多优秀的电影认真看是能提升自己观影品味的，能提升自己的艺术趣味，就算只是当成看一部普通电影也是很不错的。 但是观看这种电影本身“不一定”是一个很有趣的事，因为有些电影本身太老了，或者当时很新颖的拍摄手法和观点如今已经很常见了。就算有些老电影隔了这么多年还能让人沉浸其中，但是决定把两个小时的时间放在一部 1957 年的电影上还是需要一定决心的（比如 十二怒汉）。 本来观影这部分是很容易达成的，毕竟有那么多优秀电影可以看，每年也有那么多院线电影可以选择，但是因为疫情原因，导致今年几乎没去电影院看院线电影，所以也就没能最终达成计划。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:2:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"坚持小习惯 其实真的只是小习惯，有些是能让自己变得更好的比如 “少喝可乐” 这种，还有些是改掉自己平时的一些坏习惯，比如 XXXXX（这我肯定不能说啊）。其实这种每年养成几个小习惯还是比较容易的，然而日积月累下来，每年四个坚持五年就是十几二十个优秀的习惯，就算坚持的时间再短，养成的习惯再少，也总归是一种让自己变得更好的方法嘛。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:3:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"阅读 其实这一项应该不算 100% 完成度。因为有几本书虽然在计划表里，但是真正看起来之后发现超无聊，讲得都是些废话或者完全不能认同的，就随便翻翻过去了。明年再选书的时候还是要多看看书评，而且看到觉得不想再看下去的书就即使止损。 不过关于看书倒是有两个点可以记录下来（如果有人看到这篇文章的话才算 “分享出来” ，我就姑且叫 “记录” 吧 🤣）： “读自己专业领域的书，不叫读书；读与自己专业无关的书，才叫读书”。这句话是从 Tengusan 那儿听来的，好像是马未都说的。受这句话启发，我就把明年的读书列表里所有跟专业相关的书都踢出去了，要读的是一些“奇怪”领域的书：天文、医学、医药、物理、建筑… 看书不一定要看和自己观点一致的，但是一定要看 “不胡说” 的。比如你认为 “外星生命一定存在”，但是作者认为 “外星人不存在”，那这本书可以继续看下去，完全没问题，求同存异嘛。因为这样不仅可以让我们认识到其他人的观点、理解别人的想法也有可能让我们发现自己观念的局限性，从而改正自己的观念或者扩展自己的思维，最起码还能开阔自己的眼界。然而如果作者表达的观点都是 “我没见过外星人，所以没有外星生物；我周围所有人的工资都超过 2000 块钱，所以世界上没有人会饿死了” 的这种明显的逻辑问题，或者纯粹的抬杠扯淡，那就没必要继续看下去了。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:4:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"了解新领域 这个是我年度计划里最有趣的部分了。就算哪天我不再看电影、不再写博客、甚至不再玩游戏了，也还会保留这一项。今年列表里是 “做饭、摄影、游戏设计和视频剪辑”，虽然都只是了解了星星点点，不过也还是很有趣。 尤其是游戏设计，真正的 “游戏玩家” 可以来了解一下，不是了解编程去实现一个游戏，而是了解设计一个游戏的玩法、关卡、机制等，真的很有趣。比如 “玩家在枪林弹雨中奔跑时会偷偷降低所受的伤害，以此让玩家觉得自己精于闪避”， “敌人掉落物品时会扫描玩家背包，给出玩家需要的东西，以此让玩家觉得自己运气爆棚” 等等。 当然了，一年了解四个新领域，也只是、只能是了解到皮毛中的皮毛了，不过这并不重要，了解了皮毛以后再有兴趣深入也有个方向嘛～ ","date":"2020-12-31","objectID":"/posts/2020-summary/:5:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"专业技能 这个就不额外多说了，毕竟自己还只是一个初级开发者，还是得学习一个。以后我想买的从 PlayStation 到 Tesla 都要靠专业技能来赚钱买呢，不继续提升专业知识的话都不说 Tesla ，甚至爱玛电动车都买不起呦。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:6:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"减肥 减重 15 斤这事儿，身边有朋友说我还挺厉害的，我倒是觉得还挺正常的。就很简单呐，少吃点，多动动，自己就瘦下来了。我以前每次吃面🍜️都是点三两（大份）的，现在都是吃二两（中份）的了；以前每次吃中餐都是两三碗米饭🍚️，现在改一碗了；然后偶尔拿出自己的 Switch 玩玩健身环呐、舞力全开呀，真的是自己就瘦下来了，没那么难的。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:7:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"换工作 最后一个规划就是 “换一份工作”，这个也顺利达成了。毕竟作为一直之前只有过一次面试、一次工作经验的人，换工作还是个挺大的事儿呢，所以还专门记录了这第一次换工作的经历。 0X02 值得记录的时刻 准备了几道年夜饭的菜（虽然就一家三口 hhh） 玩到了塞尔达-旷野之息（有塞尔达玩的时光真的太幸福了） 第一次自己的开源项目有真正的用户 找到一份心仪的工作 一次惊险刺激的徒步之旅 新工作被通知提前转正 今年发生的事情很多，但是回忆起来真正发生在自己身上的事情又很少，所以也没有几个真正值得记录的时刻。照着自己过去一年的滴答清单了好半天，也没凑够整整 10 个值得纪念的时刻。 ","date":"2020-12-31","objectID":"/posts/2020-summary/:8:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"做菜 首先今年的年夜饭是第一次在自己家里吃，以前都是回老家跟一大家人，这次自己家里我还准备了几个菜，效果不错。而且其他有人来做客的时候我下厨炒的几个菜评价也都还挺好的，不管是家里人还是我自己满意度都还算可以； ","date":"2020-12-31","objectID":"/posts/2020-summary/:9:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"塞尔达 有塞尔达玩的时间，真的是最幸福的时间之一，尤其是游玩的前 100 个小时，是真的太幸福了。不只是游戏设计的优秀，还有任天堂对孩子一样的好奇心的奖励，让玩家真心觉得自己是幸福的，是被重视的。如果让我说我玩过最优秀的游戏，那自然是塞尔达无疑了。但是塞尔达也不是适合所有人，如果到现在你还没玩，又想试试的话还是建议买实体版，万一不喜欢还能出掉回血，喜欢的话就直接把实体收藏了； ","date":"2020-12-31","objectID":"/posts/2020-summary/:10:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"第一个用户 自己的一个小开源项目有用户这事儿还是挺骄傲的，虽然没几个 star ✨️但是居然有人愿意用，我就已经很开心了。之前一直只是嘴上说着支持开源，实际的做法也就只是使用一些开源软件而已，这次也算是真正开源精神的一次实践吧。明年要继续完善一些 feature，还有几个 bug 要改一改，欢迎各位用 iTerm2 的朋友来试试看呐，顺便再点个 star 酒更好了🤣 ","date":"2020-12-31","objectID":"/posts/2020-summary/:11:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"新工作 新工作呢，现在这份工作综合来说还是挺满意的，不仅涨了工资还能学到新的东西，认识更厉害的人，后面估计会继续在新公司待挺长一段时间了； ","date":"2020-12-31","objectID":"/posts/2020-summary/:12:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"惊险刺激 经验刺激的徒步之旅是之前朋友推荐去的，结果在山上又大雾又高反又迷路，还上演了一波 “古墓丽影” 式的救援。而且在大雾的山坡上迷路的时候，突然发现一辆以前冲出马路的报废车，还是是真的刺激。大家在筹划这些有危险的出行之前，还是要做足功课，要不然真的出现这种特殊情况，还是挺危险的。不过这次经验还是很棒，而且很刺激，估计能记在脑子里很久很久了 hh； ","date":"2020-12-31","objectID":"/posts/2020-summary/:13:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"提前转正 最后一个就是前两天的新员工的工作总结了，总结完当场被通知表现的不错，提前转正了。虽然 2020 的开头实在不怎么样，但是这也算是为这个 2020 年画上了个比较漂亮的句号吧～ 0X03 总结 \u0026 2021 2020 年已经到此结束了（我就不信仅剩这么一点点时间还能出什么幺蛾子），如果单纯从我自己的角度来说的话，我倒还是过得不错的。新工作不仅涨了工资，也能学到更多的东西；经历的一些事情虽然没有好的结果，但也让自己有所成长；看过的书虽然忘了不少，但也还是能无形间给自己提供力量。 之前听 Tengusan 分享过一句 Ansel Easton Adams 的话：“我们不只是用相机拍照。我们带到摄影中去的，是所有我们读过的书，看过的电影，听过的音乐，爱过的人”。我想不只是拍照，生活也是，我们每个人的生命不只是我们自己，还有所有我们读过的书，看过的电影，听过的音乐，爱过的人。 最后，介于 2020 年这么操蛋，那我觉得 2021 年再怎么样也要比 2020 年更好的，大家加油 ～ ","date":"2020-12-31","objectID":"/posts/2020-summary/:14:0","tags":["Summary"],"title":"2020 年终总结","uri":"/posts/2020-summary/"},{"categories":null,"content":"0X00 前言 假设有这么一个银行（肯定要假设，如果真有这么蠢的话这银行也早就倒闭了）的网上银行站点，他提供了一套的 API，通过GET https://api.xxbank.com/transfer?amount=3000\u0026to=shawn可以向 shawn 转账 3000 块。虽然用 GET 来修改数据是挺蠢的但是好像也没什么大问题是吧。 好的，现在你正在这个银行的网站上沉迷于数自己余额的零，这时候我给你发来了一个链接。你开开心心的看完了发给你的页面，然后关掉了它，回过头来发现自己账户少了 3000 块钱！！！怎么回事呢？ 其实是我发给你的链接有“毒”，页面里所有的图都是\u003cimg src=\"https://xxx.xxx.xxx/xxx.jpg\"/\u003e，但是有一张图裂了你没发现，裂了的那张图是\u003cimg src=\"https://api.xxbank.com/transfer?amount=3000\u0026to=shawn\" /\u003e。这里有两个问题需要注意，第一个就是这个 url 并不是图片，所以图片必然会裂掉；第二个就严重了：因为是 img 标签，所以浏览器会去 GET 回来，这一 GET 没拿到图不要紧，却发起了我预谋的转账请求，你的钱就没得喽。 当然如果你多刷新几次，每次刷新就是 3000 块钱，嘿嘿嘿 0X01 什么是 CSRF 故事讲完了，那么到底什么是CSRF呢？ 跨站请求伪造（英语：Cross-site request forgery），也被称为 one-click attack 或者 session riding，通常缩写为 CSRF 或者 XSRF， 是一种挟制用户在当前已登录的Web应用程序上执行非本意的操作的攻击方法。跟跨网站脚本（XSS）相比，XSS 利用的是用户对指定网站的信任，CSRF 利用的是网站对用户网页浏览器的信任。 「维基百科」 说起 CSRF，不知道大家有没有听过一种“POST 请求比 GET 请求更安全”的说法，也许这种说法的来源就与 CSRF 相关，具体的我们后面会说到。 0X02 如何防范 CSRF ","date":"2020-11-18","objectID":"/posts/csrf-simple/:0:0","tags":["Security","CSRF"],"title":"防范 CSRF","uri":"/posts/csrf-simple/"},{"categories":null,"content":"使用 Referer 字段 HTTP 协议中有一个 Referer 字段，是用来标记“发起这个请求的来源地址”，这样我们在后端做一个校验if http_request.head.referer == 'https://www.xxbank.com/'就可以了。这种做法的成本极低，只需要在后端新增一个校验，但是存在一个问题：不是非常靠谱。 为什么不是很靠谱呢？首先，这个字段并不是必填的，我们不能保证用户的牛鬼蛇神浏览器会传这个字段；其次，即使牛鬼蛇神传了这个值，也不能保证他是正确的。所以从这个角度上来说，虽然实现成本很低但是效果也并不太好。 ","date":"2020-11-18","objectID":"/posts/csrf-simple/:1:0","tags":["Security","CSRF"],"title":"防范 CSRF","uri":"/posts/csrf-simple/"},{"categories":null,"content":"使用 token 回传 首先我们要知道不要用 GET 去修改数据，不要用 GET 去修改数据，不要用 GET 去修改数据，然后再讨论接下来的问题。还是用转账这个 API，我们假设现在 API 是POST https://api.xxbank.com/transfer/，数据是{\"amount\": 3000, \"to\": \"shawn\"}。你可能会说“不对呀，还是可以在其他页面里用 Ajax 发送 POST 请求呀”，先别急，慢慢看。 我们要修改“转账”这个功能，本来只接受amount/to这两个参数的 API 现在接受第三个参数：csrf_token。那么这个csrf_token的值应该是什么呢？这个值又服务器生成，每次客户访问转账页面的时候带着返回个客户，然后客户表面上看起来这里有两个框框，填写金额和收款人，但实际上还隐藏了第三个框框，直面的值是自动填写的 csrf_token。现在用户点击“转账”后，发送的正确请求数据应该是{\"amount\": 3000, \"to\": \"shawn\", \"csrf_token\": \"2336666\"}了。 现在如果还有个小黑客用 Ajax 的方法发送请求，就不行了，因为这时候他只能填写amount/to这两个参数，第三个csrf_token是不知道的，所以防御成功。 ","date":"2020-11-18","objectID":"/posts/csrf-simple/:2:0","tags":["Security","CSRF"],"title":"防范 CSRF","uri":"/posts/csrf-simple/"},{"categories":null,"content":"双重 cookie 双重 cookie 并不是说有两个 cookie，而是说一个 cookie 要用到两次。当用户登录银行系统后，系统会发给他一个 cookie，这个完全没有问题对吧。但是采用了双重 cookie 验证的站点会在 cookie 里加入一个csrf_tcookie=xxx（当然你随便叫什么都可以）的值，接下来你在银行站点内发送的每一个请求都要取出这个csrf_cookie，拼在后面。 在不启用这个防护策略的时候，POST https://api.xxbank.com/transfer/就可以将钱转走，现在启用了这个策略，就必须得POST https://api.xxbank.com/transfer/?csrf_cookie=xxxxx才可以。然而由于这个值是放在 cookie 里的，所以只有该站点的请求才可以取到，其他站点的是不能取到 cookie 的。如此一来，csrf 的攻击也就失效了。（当然，这个的安全性是没有回传 csrftoken 这么强的） 0X03 参考 美团技术团队前端安全系列（二）：如何防止CSRF攻击？ 维基百科：CSRF ","date":"2020-11-18","objectID":"/posts/csrf-simple/:3:0","tags":["Security","CSRF"],"title":"防范 CSRF","uri":"/posts/csrf-simple/"},{"categories":null,"content":"0X00 前言 这篇博文概述了一次查询从一条 SQL 到拿到数据的过程，是掌握了基础的 CRUD 后想要进阶 MySQL 的一条必经路（当然我是说这个只知识是必经路，并不是我这篇文章）。如果有兴趣的话就继续看下去吧~ 我们首先看一下下面这张图，其实并不复杂。我们很多人都已经知道了其中的一部分，比如缓存、存储引擎、数据这些。 图源自：高性能 MySQL 我们先来看一下粗略的流程 首先客户端发送一条查询给服务器； 服务器检查缓存，如果命中缓存则直接返回；否则继续执行； 服务器解析 SQL、预处理、由优化器生成执行计划； MySQL 根据执行计划，调用存储引擎 API 执行查询； 将结果返回给客户端； 虽然每一条都比上面描述的、比我们想象的要复杂得多。但是因为这里只是基础篇，所以我们只是了解一下基础流程和原理就好，如果需要深入了解某部分的细节的话，可以查阅更详细更深层的资料。 0X01 客户端 \u003c-\u003e 服务器 首先我们需要知道的一点是：客户端和服务器之间的通信是半双工的 。这一点其实就能解释我们工作中遇到的一个问题：当我们主动发起一个查询请求后，并不能再次主动 cancel，只能等待查询结束。 大家学过计算机网络都应该知道，半双工就意味着要么发送数据、要么接收数据，并不能两件事一起做 还有需要注意的一点，我们“理所当然”地以为客户端发起连接，推送 SQL 到服务器，等待处理再将结果拉 回客户端。但是这样其实并不对，实际上是 MySQL 服务器在向客户端推 数据，再加上半双工的限制导致客户端在接收数据的时候只能等待数据全部推送完毕。 我们初步了解的时候并不需要学习整个客户端和服务器交互协议的详细内容，所以了解到这里就差不多了，我们知道“半双工”和“推拉”基本就能解决一些初级问题了。 然后一个与此相关的是“查询状态”。我们都知道show full processlist可以看到当前系统中运行的查询的状态，但是有些状态是不能望文生义的，还是需要了解一下具体到底是什么含义 Sleep：等待客户端发来请求；Query：正在查询或者正在将结果发送给客户端 ；Locked：被锁住了，正在等待表锁（存储引擎级别的锁并不会显示出来，例如 InnoDB 中的行锁）；Analyzing and statistics：正在收集存储引擎的统计信息，生成查询的执行计划；Copying to temp table[on disk]：正在查询，并将届国际复制到临时表中（通常是在 group by，或者 Union 或者文件排序），标记了 on disk就标识当前在磁盘上操作；sorting result：正在对结果排序；sending data：可能是多个状态间传递数据、或者在生成结果集或者在向客户端返回数据。 其他的都还好，重点就在与Query和sending data，之前很多人以为Query就是正在查询，sending data就是发送数据回到客户端，这其实是不对的。 0X02 查询缓存 MySQL 自带就有一个查询缓存，但是比通常我们自己写的缓存要严格得多，MySQL 自己使用哈希做的缓存。也就是说，当缓存开启的情况下，每次来了一条 SQL 都会去计算这个 SQL 的哈希，然后拿哈希去对比，如果命中缓存就校验权限，权限没问题就直接返回数据了。 这里需要注意的一点是，MySQL 会有一个机制来控制缓存的刷新，这也是我上面提到“严格”的第一处表现：只要这个表被更新过，缓存就集体失效。因为如果数据存在延迟的话，一致性就不能得到保障，单机都保障不了一致性的话也就没法用了。 这里还需要注意的一点，也就是“严格”的第二处表现：因为是用 hash 做的缓存，所以只要你的 SQL 跟上次不完全一致，即使只差了一个字节，都会导致 hash 不同，最终不触发缓存。 0X03 查询优化 首先查询优化阶段会将 SQL 语句进行解析，得到一棵“解析树”，然后在配合预处理器的配合下最终将合法的解析树交给 查询优化器 。查询优化器会将其转化成“执行计划”，然后将执行计划交给下一步的“查询执行引擎”。 毕竟这一步是叫做“查询优化”的，是因为在这个过程中可以对查询进行一些优化，比如下面几种： 当手动写了一个比较烂的 JOIN 时，可能内部真实的关联顺序并没有按照你的 SQL 执行，而是自己寻找了一个更好的关联顺序（MySQL 都看不下去了 hhhhh）； 等价变换：比如有人写了3=3 AND age\u003e18，就会直接在这个过程中把3=3给优化掉；还有这种(a\u003cb AND b=c) AND a=5就会被优化成b\u003e5 AND b=c AND a=5； 比如你在 InnoDB 表上给自增 id 建了索引，然后取MAX(id)，那优化器就会直接取到最大值（因为本来就是排好序的，没必要一个个去找）； 等等这些。当然这部分是整个查询里最复杂的部分，我自己了解的也不够多，所以只能说清楚这么多，再多的话我也是似懂非懂更不敢乱说了。不过这部分确实是比较重要的内容，大家有兴趣的话可以自行搜索更有深度的资料，或者直接去看《高性能 MySQL》就好了。 0X04 查询执行引擎 查询执行引擎拿到的是上一步传递过来的“执行计划”，这里的执行计划是一个数据结构。MySQL 只需要根据里面给出的指令逐步执行，执行的过程中会有很多操作需要调用存储引擎来实现（那可不嘛，要不数据从哪儿来）。具体调用会跟存储引擎相关，不同的存储引擎提供的 API 也不一样，这里就是 MySQL 主程序去调用存储引擎的 API 把最终的结果集查询出来。 0X05 返回结果给客户端 查询的最后一步当然就是将查到的结果返回给客户端，如果开启了缓存的话顺便再写一下缓存。 MySQL 返回数据是一个增量、逐步的过程。也就是说当生成第一条结果集数据的时候就开始返回了，这样可以保证服务端不用存储过多的数据，也可以让客户端更早得到结果。 ","date":"2020-11-16","objectID":"/posts/mysql-query-exec-basic/:0:0","tags":["MySQL"],"title":"MySQL 查询执行内幕-基础篇","uri":"/posts/mysql-query-exec-basic/"},{"categories":null,"content":"0X00 概述 我们经常想要简单测试一下某个页面的性能，或者 REST API 的性能，用 postman 这种程序虽然可以很快的模拟请求出来但是并不方便发送大量请求；自己写脚本虽然自定义程度很高，但是写起来总归还是有点麻烦，而且并不能很方便得输出我们需要的一些常用数据。那这种时候一般就需要找些专业的工具来做这种专业的事情了。下面两个就是平时比较常用的性能测试工具，可以针对某个/某些 url 做性能测试，并且输出相对完整的报告。 软件具体怎么装就自己想想办法吧，毕竟 Linux/macOS/windows 之间都不太一样，而且 Linux 上还有 apt/dnf/pacman 巴拉巴拉一堆包管理器，用法都不一样，没必要一个个找出来贴上。需要了解 API 性能的人肯定能很轻松装上这些工具了~ 0X01 apache bench 最简单的使用方式：ab -n 20 -c 2 https://example.org/回车，就会执行一次简单的测试并输出报告。这串命令会用GET请求访问https://example.com/，并发为 2，总共发起 20 次请求。需要注意，测试的时候域名最后已经加上/，因为测试是针对 url 的，而非 domain 接着我们会得到一个这样的输出结果。其中第一部分是服务器的相关信息；第二部分是本次测试的相关信息；第三部分是连接相关的信息；第四部分是请求耗时的相关信息，我们只是简单看一下请求的响应速度，所以重点看第三和第四部分。 第三部分内容是连接耗时的数据，分别统计了连接、处理、等待的最小、平均、中位、最大值。 作为简单的测试结果，第四部分是最值得关注的，工具统计出了所有请求的耗时时间分布。我的这条结果显示 50% 的请求在 592ms 内完成；90% 的请求在 839ms 内完成，100% 的请求在 1069ms 内完成（也就是说最慢的一次请求用了 1069ms）。 简单的GET请求就到此为止了。如果需要携带 Header（这太常见了，起码登录过后得带个 token 才能真正测试），那就携带新的参数-H 即可，类似于这个样子ab -n 10 -c 2 -H 'token: xxxxxxx' https://example.org/。如果需要携带多个 Header 参数，那就多接几个-H也没问题。 如果要发送 POST 请求，需要先把请求内容写在一个文件里，例如在/homes/shawn/post_data，然后通过-p /home/shawn/post_data就可以把文件内容通过 POST 请求发送出去了，比如这样 ab -n 2 -c 1 -p /home/shawn/post_data http://127.0.0.1:2333/ 0X02 http_load http_load可以测试多个 url，不用像 ab 那样只能同时测单一 url；但是问题是不能使用GET以外的其他方法，所以用途也比较单一，用法也比较简单。 常用 4 个参数： -p –parallel 并发进程数 -f –fetches 总访问数 -r –rate 每秒访问数 -s –seconds 访问总时间 首先要准备一个纯文本文件，类似这样/home/shawn/urlist的文件 https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/about/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ https://blog.just666.com/ 保证每行一个 url，接下来就是http_load -p 3 -f 10 /home/shawn/urllist以 3 的并发量发起 10 次请求，或者http_load -r 5 -s 10 xxx 以每秒 5 次的频率请求 10 秒 会得到类似这样的输出结果 总共发起了 19 次请求；并发为 1；大小是697449 btes；总共用了 10.0025 秒；平均每个连接是36707.8 bytes；平均每次请求 1.89 秒；平均响应是 20ms、最大 125、最小 8.2； 最后是各个状态码的数量，这里 19 个都是 200，一切正常；如果 50x，尤其是 502/503 这种就有可能是你频繁请求导致服务器不稳定了。 中间缺了一行msecs/first-response的解释？实话说我也不知道。manual 里没有这个解释，google 没找到官方的解释，二手资料全是复制自同一个人的，结果最初的那个老哥可能也没搞明白就没写。。。 ","date":"2020-11-04","objectID":"/posts/ab-http_load-performance/:0:0","tags":["Performance","HTTP","Tools"],"title":"使用 ab 和 http_load 进行简单的性能测试","uri":"/posts/ab-http_load-performance/"},{"categories":null,"content":"0X00 前言 上周在新公司接到的一个任务\"给 XXX 系统集成 LDAP 登录\"。我以前是没有了解过 LDAP 的，一听是集成登录功能，脑子里第一时间想到的就是之前做过的 SSO，但是经过一番搜索发现 LDAP 和 SSO 完全是两回事，而且发现还是不少人不了解其中的一项或两项技术，正好在这儿整理一下顺便分享给大家~ 内容都是比较基础的，主要介绍了什么是 SSO、SSO 的基本原理、什么是 LDAP、LDAP 的基本原理和 LDAP 如何与 SSO 关联。如果你对这些有兴趣那不妨继续看下去，如果你都了解了那就去看看其他文章或者休息一会儿好了~ 现在开始做一些技术背景预设：假设你公司有 10 个站点，分别是site_0.just666.com/site_1.just666.com.....，同时每个站点都有用户系统，也就是说用户可以登录到站点上去做些操作。 用户肯定不愿意访问每个站点的时候都要登录，比如你登陆淘宝之后再看天猫肯定不想再登陆一次了，如果天猫要你再登陆一次且密码还跟淘宝那边不通你肯定要骂人了。现在怎么办呢？SSO 和 LDAP 两种技术就是用来解决这个问题的。 0X01 SSO SSO全称是 Single Sign On，中文叫\"单点登录\"，听名字也猜得出来，就是从单一节点上登录。SSO 需要在传统的\"客户/服务\"之外再加了一个 SSO 服务，专门用来校验用户登录和管理 token。 客户点开站点 站点发现这次请求没有携带有效 token，将客户跳转到 SSO 的登录页面（并告诉 SSO 本次跳转的来源） 客户在登录页面输入用户名密码，点击登录（此时在和 SSO 服务进行交互） SSO 服务器对登录信息进行校验，通过后发放一个 token 并跳转回源页面（因为第 2 步告诉 SSO 跳转的来源了） 客户带着 token 再次访问目标页面，服务器拿到 token 服务器带着 token 去 SSO 那儿校验，SSO服务 反馈：“这个 token 合法，用户是 shawn” 服务器通过校验，当前用户以 shawn 的身份登入到站点中了 这样一来，原本 10 个站点需要配备 10 个用户登录模块的，现在只需要独立出来的一个登录模块了；后面再有新的站点，接入到 SSO 也是非常方便的；不仅如此，SSO 也可以做到让你登录一个站点后带着 token 直接访问同域的其他站点。 所以简单点来说的话，“SSO 是一种用来验证登录的服务”。 0X02 LDAP 上面 SSO 介绍完了看起来好像验证就完美了，完全不需要什么 LDAP 这种东西，但是实际上不是的。 LDAP 是一个树形结构的查询性能优秀的目录数据库，可以方便的管理公司员工和部门的结构，并且 LDAP 也并非基于 HTTP 协议，硬说的话 LDAP 其实可以是一个\"DB\"。只不过因为是\"目录结构\"的数据库，所以非常适合存储树形组织的各项数据。 所以这么说来 LDAP 从技术上并非无可替代，我们完全可以用 MySQL 这种关系型数据库来做个对应的表结构来存储相同的数据，再自己写一套简单的程序要进行查询。但是 我们使用 LDAP 最大的原因之一应该就是 wiki 上说的那样：“开放的，中立的，工业标准的应用协议”，因为他足够开放、中立且应用非常广泛，我们随便一个程序都可以很快的接入进来。 我这里就不过多介绍 LDAP 了，这东西三两句是真的说不清楚，这里也主要是让大家知道\"我还有什么不知道\"，这样后面学习也有个点可以看。（主要是我自己也没有摸的很明白，现在真是说把基本原理搞明白了然后正确的把程序接入进去了，万一我说错了什么误人子弟就不好了）。 0X03 联动 如果说把上面两个部分看完了，那么联动这里就很容易了。SSO 和 LDAP 两个东西一前一后，LDAP 可以作为 SSO 服务的后端数据库，就算是将这两个连动起来了。不过通常来说企业里也不太会联动着用，一般来说一个 LDAP 就够了，多个站点都要登录但是用的相同的用户名密码，也没有多麻烦。 0X04 参考资料 LDAP 概念和原理介绍 – WilburXu 我花了一个五一终于搞懂了OpenLDAP 单点登录 Wikipedia ","date":"2020-11-02","objectID":"/posts/sso-and-ldap/:0:0","tags":["SSO","LDAP","Auth"],"title":"SSO 和 LDAP","uri":"/posts/sso-and-ldap/"},{"categories":null,"content":"0X00 前言 都说“生活需要仪式感“，那对我来说的第一次跳槽还是值得记录的，用处肯定是没什么，但是总归是给自己一点点生活上的仪式感吧。而且脑子记住的东西可能过几年就忘记了，真正记录成文字的东西存活时间就会长久很多。（记录在网络上就算我自己把博客删了，都还会有垃圾站原封不动的抄走，简直不可能丢🤣） 本来说离职当天就来写这篇文章的，结果当天晚上同时请客去吃饭又去唱了歌，回家都凌晨两点了，隧作罢。现在离职手续全部都办好了，新公司的文件们也都准备好了，就等假期过后直接到新公司入职。所以也终于有时间坐下来不用考虑工作相关的任何事情来写一写自己的想法了～ 这不是日记，按蓝青峰跟朱潜龙的说法正经人谁写日记啊🤪 （狗头） 0X01 为什么离开 从我以实习生的身份加入创宇到最后离开一共是 1159 天，算下来是三年多一点点，时间也算是很久的了。毕竟我大学也就只有前三年是在学校的，后面就出来实习了。现在走呢也不是说因为公司有啥问题或者我被开除了，只是说我自己觉得是时候出去看看了，不想常年在一家公司里待下去 这次跳槽主要是这三个问题导致的： 💰️首先是“工资问题“；大家出来工作，99.99%的人都得是为了赚钱吧。我因为是实习转正的，所以薪资起点会比较低，即使后来的调薪比例都还挺高的也会导致最后的薪资并没有怎么多；所以说自己也想要换个地方拿到更高的薪资，要恰饭的嘛； 👨‍💻其次是“专业技能“；开发者嘛，想要自我提升的人还是很多的。但是因为我做的事情是针对少数人使用的内部系统，所以对个人的技术力需求和考验并没有什么。在工作中不太能接触到更高的东西和更宽的东西，自己私下时间学习到的更高更宽的知识又比较难应用起来。虽然在舒适圈是真的舒适，但是想要提升的话总有一天要打破舒适圈的，我觉得这一天该到了； 👀️最后是“眼界和见识“；我个人觉得我已经在同一个岗位做同一个业务时间够久了，也想要见到其他不同的行业和人们。 所以总结下来，就是说目前我：使用“Python“编写“Django“程序，然后运行在内网环境里，给“内部审核系统“提供稳定可靠的“web 服务“，每个月赚到“**“的工资。现在双引号里所有的东西我觉得都是时候可以改变的，对于技术栈来说的“T 型人才“的横和竖都是可以发展的了；对于工资来说就更是了，起薪低的情况下很难有大额上涨的，所以跳槽也是一个大家都理解的操作。 0X02 准备一份简历 换工作第一步当然是“写简历“了。我在这儿就不总结说怎么准备简历了，毕竟就一次换工作的经历，也没什么资格。不过在准备简历的过程中我觉得有些事情是值得被记录下来的。 首先是我花费在简历的时间，从 0 到 1 准备简历的时间加上后来改动简历的时间怎么也有 10 个小时了。这点本来我没太觉得有什么，直到后来跟朋友聊到这个才发现好多人并不会用这么长时间在简历上的。我自己是觉得简历这件事太重要了，花费 10 小时完全是值得的； 其次是“善用搜索能力“，多搜一下相关的简历，在 Google 上直接搜索“程序员 简历 filetype:pdf“就可以找到海量的 PDF 格式简历。这些简历都是我们的素材来源，可以多参考。（因为是搜来的，不能保证质量，所以需要自己有辨别能力）✅️❎️ 接下来是“P2P“，也就是点对点。如果不嫌麻烦的话应该做到一份简历对应一份工作，当然这样的成本是巨大的。所以一般来说应该是一份简历对应一种工作，比如说我会 Python 和 前端 并且都可以作为主力，那我想要找这两个工作的时候应该准备一份 Python 的简历，再准备一份 前端 的简历，万万不可写在一起。因为 Python 的面试官并不是很在乎你的前端能力，反之也是一样。不过也不是说一点都不写，还是要提一些，因为这样可以证明自己的工作能力、学习能力，如果用人方真的需要这种技能那就是个大大大大大的加分项了。 最后的是“朋友的帮助“。我前前后后找了五六七八个朋友帮我看简历，然后从设计、人事、技术、等不同角度给我提了四五十个修改点。是的没错，我就 A4 纸 1 页能有四五十个修改点。然后我会把这些所有的修改点逐一排列起来然后认真考虑吸取哪些内容，然后再一点点改自己的简历。改好之后再拿出去给大家看看，反复迭代两三次的样子。所以上面提到的十多个小时算下来也没有很过分了是吧，毕竟改了这么多个点。 0X03 面试经历 因为我简历是非常用心的，所以简历投出去之后的“邀请面试率“是很高的。后来面试的时候我针对技术面做了个小心思可以给大家分享一下，个人觉得这个经验还是值 5 毛钱的。 价值 5 毛钱的经验 ：我准备好简历后先投递了第一波简历，不一定是自己非常非常愿意去的公司，但是也不能说“我就试试又不去“（这样就太流氓了）。然后面试的时候也是要全力以赴，面试结束回家之后拿出一只笔在简历上把面试官问的所有问题全都勾画出来，尤其是当时没有回答出来或者自己觉得回答的不好的。然后等第一波面试结束后，如果有合适的那当然就去了，但是如果没有合适的，那就对着上面这四五六次面遇到的问题“各个击破“。因为技术面的时候面试官肯定是会对着你的简历找问题的，那你现在把几乎所有的问题都找出来了，只需要解决他们就可以了。等把所有问题都解决了，就再次投递简历～ 第二波面试的时候很多问题会直接命中，然后你就很容易成了呀 然后面试的时候我是很诚实的，遇到自己不懂不会的问题会说“这个我不太清楚，不过如果是我的话可能会这样设计/如果是我觉得 xxx 应该可以实现“。即使是自己不懂的原理，也可以在说明之后尝试猜测一下，这时候面试官会比较理解我的解题思路，即使不正确也是会有加分的。（当然不是乱说了，如果一点点都不懂的话就直接明说就好了）最最最重要的是，不要撒谎。往小了说，撒谎被发现可能就直接终止面试了；往大了说，就算成功入职也不会通过试用期，而且还会被同事嘲讽很久很久（是的，我们公司就来过这种，我们嘲笑了很久很久） 最后要注意的是“引导面试官“；面试官问了问题，你回答的时候对自己不了解的东西“一嘴带过“，然后尽可能往自己熟悉的领域聊（当然，不是说要跑题）。这样一来就会让面试官觉得你很强，不过这个算不上很重要的点了。 0X04 心仪的工作 接下来想说说我自己这次找到的比较心仪的工作。前面提到的“薪资、知识技能、眼界见识“这三个问题，能全部解决掉是最好的。所以我后面找工作的时候就是按着这三个点来突破的，如果这三个需求都能满足到的话就再好不过了。所以现在找到的新公司是很满意的，就坐等到时候入职看看具体情况了～ 找工作的时候很重要的一点是面试过程中 HR 或者技术面的面试官问的“你有什么问题问我吗“。这种时候是可以放心大胆问的，如果自己很在乎加班那就问问、如果自己很在乎个人提升那就多问问技术栈和技术深度。如果特别反感加班，那公司 996 的话就直接告辞，这样也算节省了自己和公司的时间是吧。 找工作我自己会有一个铁律：喜欢的不怕多问，多打探公司的情况，让自己放心的去；觉得不合适的公司，及时止损，不浪费自己和公司的时间。 0X05 总结 这些内容写出来主要是给我自己看的，当然如果尤其他同学看到了这篇文章并且其中的某一句话对你有那么一丢丢的帮助，那我就很开心了。 最后祝我自己在新公司能得心应手，如果有读者的话也祝你工作顺利，生活愉快🤓～ ","date":"2020-09-27","objectID":"/posts/first-job-hopping/:0:0","tags":["Life","Work","Other"],"title":"我的第一次跳槽之路","uri":"/posts/first-job-hopping/"},{"categories":null,"content":"0X00 What’s this 我们知道 MySQL 中存在“事务”这么个事物（我是故意拗口的，哈哈哈哈哈哈哈）；我们也知道事务“一荣俱荣，一损俱损”（要么事物内所有查询均生效，要么均不生效）。那么现在问题来了，银行数据库中有两个事务在同时进行，我们来看一下这两个事务 一条 SQL 就是一个查询，不一定是 SELECT xxx 才叫查询。 -- 事务 A shawn 转账给 bluce 100 块钱 BEGIN; UPDATE account SET money = money - 100 WHERE username = 'shawn'; -- 这行代码标记为 A-1 UPDATE account SET money = money + 100 WHERE username = 'bluce'; -- 这行代码标记为 A-2 COMMIT; -- 事务 B BEGIN; SELECT money FROM account WHERE username = 'shawn'; -- 这行代码标记为 B-1 SELECT money FROM account WHERE username = 'bluce'; -- 这行代码标记为 B-2 COMMIT; 现在有这么一个情况：shawn 和 bluce 两人账上都有 100 块钱，此时事务 A 和 B 同时开始执行，那么事务 B 所查询到的两人的账户余额究竟是多少呢？这个问题并没有一个确切的结果，因为 B 在查询的时候，转账正在进行中。不过要回答这个问题也不是做不到，就需要引入标题中提到的“隔离等级”这个概念了。 我们在 MySQL 中使用这四种隔离等级：READ UNCOMMITTED/READ COMMITTED/REPEATABLE READ/SERIALIZABLE。隔离等级的不同就意味着在同一个时间节点下查询到的内容可能不同，数据的可靠性也会变得不同。接下来我们来简单了解一下这四种隔离等级，并且通过一个测试数据库来验证一下。 0X01 How to try it 首先要建一个数据库来做测试，表结构和两条基础数据长这样，可以通过随便什么方法把表先建好。 CREATE TABLE `account` ( `id` int(11) NOT NULL, `username` varchar(10), `money` int(11) DEFAULT 0, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; INSERT INTO `account` VALUES (1, 'shawn', 100),(2, 'bluce', 100); 然后我们打开两个 MySQL 的终端连接（一定是两个），其中一个用来执行事务 A，一个用来修改隔离等级然后观察结果。 修改隔离等级的时候，可以选择“全局”或者“会话”两种级别，我们在测试阶段就只需要修改“会话”级就可以了。用的是SET session transaction isolation level read committed;这样的方法，其中最后的read committed就是其中一个隔离等级。 注意在会话级别修改隔离等级应该是在你 select 去验证的会话里执行，而非执行事务的会话。换个说法就是“隔离等级”是针对事务外的，而非针对事务。 0X02 READ UNCOMMITTED 首先是 READ UNCOMMITTED 这个最简单的隔离机制：“读未查询”。其实这个翻译是准确的，就是有点别扭，换个说法叫“可以读到未提交的数据”。可以直接理解成“没有隔离”。也就是说所有改动都是实时的，别人事务里改动也是实时的，即使事务还没commit我也能看到。 +----+----------+-------+ | id | username | money | +----+----------+-------+ | 1 | shawn | 100 | | 2 | bluce | 100 | +----+----------+-------+ 拿上面的数据库来举个例子，现在数据库里数据是这个样子的。首先我们打开数据库终端两个，分别叫做“甲/乙”好了，为了好区分。甲负责执行事务，乙负责检验效果。我们在甲终端里开始一个事务，但是不提交 mysql root@127.0.0.1:test\u003e BEGIN; Query OK, 0 rows affected Time: 0.002s mysql root@127.0.0.1:test\u003e UPDATE account SET money = money - 100 WHERE username = 'shawn'; Query OK, 1 row affected Time: 0.003s; mysql root@127.0.0.1:test\u003e UPDATE account SET money = money + 100 WHERE username = 'bluce'; Query OK, 1 row affected Time: 0.002s ```sh 然后在乙终端里将会话的事务隔离等级改为 `READ UNCOMMITTED`，用这条命令`SET session transaction isolation level read uncommitted`就可以了。接下来我们再从终端乙查询一下整张表的数据 +----+----------+-------+ | id | username | money | +----+----------+-------+ | 1 | shawn | 0 | | 2 | bluce | 200 | +----+----------+-------+ `` 发现转账已经成功了。问题也就出在这儿了，假设我们终端甲里执行的事物还有好多事情要做，而且后面执行过程中出错回滚了，那当前的数据其实就是错误的。这种读到脏数据的行为我们称之为 脏读 。就光是这两条语句，都有可能出问题的。如果我们在事物给 shawn 的钱 100 之后读，就能读到 shawn 没有钱且 bluce 有 100 块，那此时的钱就凭空消失了 100。再或者说，shawn 在系统里给别人转账，点击确定后自己钱没了，然后系统在把钱加到对方账户上的时候发现对方账户有问题，事务就回滚了，shawn 发现自己的钱从 100 到 0 再到 100，就很奇怪。命名是转账成功变 0，不成功就保留 100 的事情，结果居然在两个数字减反复横跳，就很奇怪。 记得最后在终端甲上 commit 一下，要不然一会儿数据对不上了 如果要解决脏读的问题，其实也比较简单，可以引入下面提到的 READ COMMITTED隔离级别。 0X03 READ COMMITTED 想要让数据更靠谱就只有牺牲资源，比如加锁。在这种隔离等级下不会出现上面的脏读现象，只有当事务commit之后事物外才能读到改动。“那岂不是这样就完美了？” too young too simple. 正如前面说的，这种操作是要加锁的，既然加锁就有可能出问题。首先是并发，有 100 个人都要给 bluce 转账，触发了 100 个类似于上面的查询，第一个开始的会取得 bluce 的排他锁，后面的就直接卡住了。等第一个释放掉之后，后面 99 个事务再抢。。。这样还算好的，如果其中一个事物不止做了这些，还有更复杂的操作，导致事务耗时比较久，那么后面的所有事务都会被卡住，并且有可能会导致事务超时。 还有一个问题，涉及到索引。UPDATE account SET money = money - 100 WHERE username = 'shawn' 这条 SQL 对应的 username 字段如果在 account 表里没有索引，会发生什么？MySQL 会不知道哪条才是username = 'shawn'，那咋办？锁表，准确的来说不是加表锁，是给表里所有行加行锁，等找到username = 'shawn'的时候再释放其他的。如果表里有 100w 数据，那就要给 999999 条数据做无意义的加锁解锁操作。 0X04 REPEATABLE READ 这个是 MySQL 默认采用的隔离级别，称为“可重复读”。既然有可重复读就一定会有一个对应的“不可重复读”，这里简单介绍一下怎么叫可重复读和不可重复读，还是以两个事务为例子。前面提到的两个隔离等级都是“不可重复读”的，这个很好理解，只是中文叫法有点奇怪，我们看一个“不可重复读”的例子（shawn/bluce 各有 100 块存款） -- 事务 A BEGIN; UPDATE account SET money = money - 100 WHERE username = 'shawn'; -- 这行代码标记为 A-1 UPDATE account SET money = money + 100 WHERE username = 'bluce'; -- 这行代码标记为 A-2 COMMIT; -- 事务 B BEGIN; SELECT money FROM account WHERE username = 'shawn'; -- 第一波查询 SELECT money FROM account WHERE username = 'bluce'; SELECT money FROM account WHERE username = 'shawn';","date":"2020-08-30","objectID":"/posts/mysql-isolation-level/:0:0","tags":["MySQL","Database"],"title":"MySQL 中的四种隔离等级","uri":"/posts/mysql-isolation-level/"},{"categories":null,"content":"0X00 使用 with as 语法 我们写程序经常会操作文件，我们都知道写文件要 open/write/close ，尤其是 close ，没有的话文件就会出问题（有些内容在缓存里，没写入磁盘）。不过我们现在写文件应该没什么人这样写了，都是用with open('filename', 'w') as f的方式来操作文件了。如果说这样做的好处，那多数人都会说“不用手动关闭文件了”，错肯定没错的。 上下比起来，上面的方式不仅多了一点点代码，而且随着中间逻辑代码变多，很可能会导致最后忘记 close，从而引发 bug。 0X01 这就是上下文管理器 上面 with xxx as xxx 的调用方式就是在调用上下文管理器。简单来说上下文管理器就是：在执行你编写的代码（with xxx as xxx后面那坨）之前，操作一波 ；再在你编写的代码执行完后，操作一波 。我们简单理解一下就是在 with open('file_name', 'w') as f 内层缩进的代码执行完成后自动帮你执行了f.close()（当然没这么简单，有兴趣可以去看一下 open 的源码，但是大体逻辑是这样的）。 0X02 自己实现一个 说了半天，咱们自己来实现一个上下文管理器好了。自己实现一个上下文管理器跟实现一个其他东西不太一样，不用继承任何东西，就像实现一个迭代器一样，只需要满足自己的协议（也就是上下文管理器协议）就可以。而且好在这个协议极其简单，实现一个类只需要满足满足两个方法就可以：__enter__ 和 __exit__。我们先来写个 demo 试试看。 #!/usr/bin/env python3 class CustomProtocolConnection: def __init__(self, host, port): print('假装开始连接, ', host, port) def __enter__(self): print('进入了 __enter__') return self def __exit__(self, exception_type, exception_value, exception_traceback): # 这个定义是固定的，必然接收三个参数 print('进入了 __exit__') self.close() # 没有定义，只是断开连接 return True def push_data(self, data): print('假装在推数据：', data) def pull_data(self): print('假装收到了数据：', 'hello,world') if __name__ == '__main__': with CustomProtocolConnection('127.0.0.1', '2333') as conn: conn.push_data('hello, world') conn.pull_data() 然后我们来看一下执行结果 首先进行了常规的实例化；实例化之后执行了__enter__；然后执行我们自己编写的代码块；退出代码块之后执行了__exit__。我们可以看到上面定义__exit__的时候带了三个参数，看名字也看出来了，第一个参数是异常类型、第二个是异常的值（也就是错误消息）、第三个是异常的错误栈。如果我们在 with xxx as xxx 下面的代码块中出现异常了，那么异常会被捕获并传递到__exit__这里来，你可以根据情况来处理。这里不再展示具体如何处理异常了，大家可以手动触发异常然后在__exit__里打上断点来调试一下，很容易就能知道这里是怎么用的了。 实现一个上下文管理器不一定非要是类，一个函数照样可以（废话，with open() as f 不就是一个函数嘛）。我们可以来编写这样一个函数 #!/usr/bin/env python3 import contextlib class CustomProtocolConnection: # 一个本来就有的类 def __init__(self, host, port): print('假装开始连接, ', host, port) def push_data(self, data): print('假装在推数据：', data) def pull_data(self): print('假装收到了数据：', 'hello, world') @contextlib.contextmanager # 用 contextmanager 装饰器使这个方法成为上下文管理器 def connect_2_server(host, port): conn = CustomProtocolConnection(host, port) print('进入了 __enter__') # 并不是真正的 __enter__ 方法，但是有同样的效果 yield conn print('进入了 __exit__') # 同理，并不是真正的 __exit__ 方法，但是又相同的效果 with connect_2_server('127.0.0.1', '2333') as conn: conn.push_data('hello, world') conn.pull_data() 一个带有 yield 的函数是一个生成器，不过因为 contentlib.contentmanager 装饰器的作用，使它现在是一个上下文管理器了。以 yield 为界限，yield 之前的内容可以理解成是 __enter__ 方法，后面的可以理解成是 __exit__ 方法，由 yield 返回的那个值就是我们 with connect_2_server as conn 的 conn 了。我们来看一下这坨代码的运行结果，可以看到跟上面的效果是一样的。这两种方式其实是适用于不同的场景的，第一种直接编写 class 的方式适用于从零开始实现一个东西，这种就可以直接将其定义为上下文管理器，用起来比较方便；第二种通过装饰器实现一个额外的 function 的方式适用于在现有的代码块基础上实现上下文管理器，可以做到不侵入现有代码还实现所需功能的需求。 0X03 什么时候用 现在搞明白上下文管理器是什么了，也知道自己怎么才能实现一个上下文管理器了，那么什么时候才会用到这种东西呢？其实我们的操作很多时候都是可以用到的，只不过这个东西从来都不是必须的，很多情况下大家都绕开或者用了更麻烦一点的方式实现了。 首先，就像上面的自定义协议的通信一样，先要连接最后断开的情况就可以使用这种方式；比如要发起一大波 HTTP 请求，但是需要登录，就可以用这种方式实现自动登录和自动注销；再比如做数据统计或者导出，开始前要将数据整理一波，统计导出结束后再将结果邮件发送到指定邮箱，这种也是可以的。 ","date":"2020-07-18","objectID":"/posts/python-context-manager/:0:0","tags":["Python"],"title":"Python 上下文管理器","uri":"/posts/python-context-manager/"},{"categories":null,"content":"0X00 objects 是个啥 想必所有用过 Django 的人都会用到 Django 自带的 ORM 进行数据库查询。那既然用过 Django 的 ORM 就来看一下这段代码好了， models.Stuent.objects.filter(name='Shawn') 这段代码是什么意思呢？很简单，就是查询到名字为\"Shawn\"的学生信息。具体来说， models 应该是一个放了多个 model 的文件，Student 是一个具体的模型，filter 是筛选，name='Shawn' 则是筛选条件。那么问题来了，中间那个 objects 是个啥呢？（你知道？知道还在这儿看啥，有这空看看其他文章，打打游戏看看电影不好吗🤣） 通过 type 可知，这个 objects 是一个 django.db.models.manager.Manager 的实例（或者是他子类的实例）。然后我们来看看这个Manager是个什么❓ A Manager is the interface through which database query operations are provided to Django models. At least one Manager exists for every model in a Django application. 从 Django 文档得知“Manager 是 Django 用来进行数据库查询的一个接口，在 Django 应用中每个 model 都需要至少有一个 Manager”。 0X01 Manager 的用法与自定义 我们正常来说用的 filter/exclude/first/last 这种查询都是用的 Manager，用法大家是都会用的，不过自定义的话就是另一回事儿了。我们假设有下面这个 model class Student(models.Model): name = models.CharField() age = models.IntegerField() gender = models.CharField() birthday = models.DateField() height = models.DecimalField() weight = models.DecimalField() remark = models.TextField() class Meta: verbose_name = '学生' 虽然我们没有手动指定 objects，但是其实已经从 models.Model 继承来了。如果我们非要手动指定的话，可以objects = models.Manager()。值得注意一点是，如果我们手动指定了 Manager 的话，Django 就不会再给我们一个可用的 objects 了。 下面我们来说一下关于自定义 Manager 的方法，和什么时候需要自定义 Manager。通常来说，没有必须要用 Manager 才能完成的操作，但是很多时候利用自定义 Manager 会帮我们节省很多时间和代码。我们考虑这么一种情况，上面那个 model 定义了一个学生信息表，我们在系统里需要非常经常地使用“身高超过 180，且体重在 70 到 80 之间，且成年的男生；身高超过 170，且体重在 55 到 65 之间，且成年的女生；当天过生日的所有同学”这三种筛选条件。当然，最简单的方法就是每次使用的时候都去 filter 一遍，这也没错，不过这样就太奇怪了。稍微有点编程经验的可能会写出如下代码： # 这函数名确实太长了，就不乱编了，三个函数对应上面三个查询 def get_student_1(): return Student.objects.filter(gender='M', height__gt=180, weight__range=(70, 80), age__gte=18) def get_student_2(): return Student.object.filter(gender='M', height__gt=170, weight__range=(55, 65), age__gte=18) def get_student_3(): today = datetime.date.today() return Student.objects.filter(birthday__month=today.month, birthday__day=today.day) 调用的时候每次都是 studnet_1_queryset = get_student_1()。尤其是如果要在这个基础上增加新的筛选条件，画风就会变成这样：student_x_queryset = get_student_1().filter(xxxx).exclude(yyyyy)。就…….也不是不行，不过看起来确实很奇怪，而且跟使用 Manager 的方式比起来也非常不 Pythonic。 如果使用使用 Manager 的方式编写的话就可以是下面这个样子： class SpecialStudentManager(models.Manager): # 自定义一个方便获取这三种数据的 Manager def student_1(self): return self.filter(gender='M', height__gt=180, weight__range=(70, 80), age__gte=18) def stuent_2(self): return self.filter(gender='M', height__gt=170, weight__range=(55, 65), age__gte=18) def student_3(self): return self.filter(birthday__month=today.month, birthday__day=today.day) class Student(models.Model): name = models.CharField() age = models.IntegerField() gender = models.CharField() birthday = models.DateField() height = models.DecimalField() weight = models.DecimalField() remark = models.TextField() objects = SpecialStudentManager() # model 这里就改了这一行 class Meta: verbose_name = '学生' 虽然改动了 Model 但是没有改动数据库表，所以不需要migrations。 现在我们尝试调用一下上面定义的几个查询：Student.objects.student_1()这样就可以了，而且如果用@property装饰上面的方法的话，会更好一些。当然了，我们还可以扩展一下用法： class SpecialStudentManager(models.Manager): @property def student_count(self): # 所有学生人数 return self.all().count() @property def student_name_list(self): # 所有学生人名列表 return list(self.all().values_list('name', flat=True)) 不过可以看出来，正如上面所说“一切用自定义 Manager 实现的功能，都可以不用自定义 Manager 实现”，但是自定义 Manager 的做法也确实让我们的代码更清晰明了，后面改起来也更舒服（不用为了一个常用的筛选去改多处代码），可读性也更高了。 ","date":"2020-07-14","objectID":"/posts/django-objects/:0:0","tags":["Django","Python","ORM"],"title":"Django 中的 objects","uri":"/posts/django-objects/"},{"categories":null,"content":"0X00 按惯例得有一个标题 众所周知save是 Django 中最常用的保存数据的方法。但是一般来说大家经常会把“常用“理解成“万能“，然后能用的时候就全用这一种方式。不过编程 中是没有所谓的“一招鲜吃遍天“的，Django 之所以提供了那么多中保存数据的方法也侧面证实了这一点。 首先来看一下我遇到的这个问题： from .models import Student from .utils import calculate_score queryset = Student.objects.all() for student in queryset: student.score = calculate_score(student) # 调用一个工具函数计算该学生的成绩 student.save() 这段代码乍一看没有什么问题，因为计算的值是通过calculate_score这个函数进行的，所以不能使用queryset.update(xxx)的方法。然后咱们看一下 Django 文档是如何描述 queryset 的。 QuerySets are lazy – the act of creating a QuerySet doesn’t involve any database activity. You can stack filters together all day long, and Django won’t actually run the query until the QuerySet is evaluated. QuerySets are lazy nternally, a QuerySet can be constructed, filtered, sliced, and generally passed around without actually hitting the database. No database activity actually occurs until you do something to evaluate the queryset. When QuerySets are evaluated QuerySets are lazy 内容总结来说就是“Django 中的 QuerySet 只有在用的时候才会真的去数据库里查，而不是生成 QuerySet 的时候“。后面的 When QuerySet are evaluated 则标明了什么叫做“真正在使用“，给出了下面几个条件，当你做这些事情的时候就是“真正在使用“了。这些条件包括：迭代、 切片、列表等（我英文水平小学三年级，解读地不对的地方还希望大家指出）。 所以显然我们对这个 queryset 来了个 for 循环就满足了上述的“迭代“，所以这时候数 Django 就会真正的从数据库中将数据真正的 取出来 。 现在问题来了。我们思考一个问题，如果我一秒钟能计算10 个学生的成绩，然后整个Student表有 3W 学生，得出“处理所有学生信息需要消耗 50 分钟的时间“这样的结论（每秒 10 条和一共 3W 是乱写的，真实数据通常比这个大得多）。 如果在执行这个循环的时候，某位同学修改了自己的的信息，比如手机号，会发生什么？ 有两种可能：第一种可能是这位同学修改自己手机号的时候计算分数的循环已经把他的分数计算完了，那么他的手机号修改也生效了（这种最好）。但是如果他改手机号的时候循环还没到他呢？假设他把手机号从原来的 123 改成了 456，那么他改完手机号的一瞬间数据库里存进去的确实是 456，没有问题。但是 queryset 里是他改手机号以前取出来的 123 ，这时候循环到他了，计算完之后来了一个student.save()，如此一来他刚刚改好的手机号码就又回到了 123。 所以说这种写法并不会 100% 出现问题。整个循环耗时越久，出现问题的可能性越大；系统中数据变更越频繁，出现问题的可能性越大。当然了，bug 就是 bug，不能因为 bug 没触发就无所畏惧了，还是得解决的。通常来说有两种解决方法，下面是第一种 for student in queryset: new_score = calculate_score(student) Student.objects.filter(id=student.id).update(score=new_score) 这种方式仍然比较 young 比较 simple 比较 naive，不过又不是不能用 。 但是这种用法显然是不好的，而且 update 本来也不是让我们这么用的。所以我们还是得回到save上，Django 其实已经提供了一个参数给 save 了，可以用下面这种方法 for student in queryset: student.score = calculate_score(student) student.save(update_fields=['score']) 也就是在 save 的时候带上具体需要更新哪个字段，其他的就不更新了。而且通过传递的参数也可以看出，指定的是多个字段，如果有需要修改多个字段的话，就只修改这一个就好了。 不过其实这里还是有一个潜在问题的，那就是说：恰好我们在更新 score 的时候，其他地方也在更新这个。不过这个更多的时候就是我们程序逻辑的问题了，因为在几乎同一时间对一个字段进行修改，然后修改的双方又互相不知道的话，总是会出问题的。 ","date":"2020-07-11","objectID":"/posts/django-save-exception/:0:0","tags":["Django","ORM"],"title":"记一次 Django save 导致的数据异常","uri":"/posts/django-save-exception/"},{"categories":null,"content":"0X00 Django 中的权限结构、定义 我们知道在创建了一个 Django 项目之后，默认就有两个公开可用的 model：User 和 Group，这两个 model 的一项功能就是用来做权限管理的。系统中会有很多项权限，单个 user 可以配置拥有哪些权限，也可以将权限配置给 group。然后校验单个权限的时候其实就是将 user 本身的权限，和 user 所在的所有组的权限做一个并集，看本次操作的权限是否在这个并集里。在，那就校验通过；不在，那就只有 HTTP 403 了。 HTTP 401 和 HTTP 403 的区别：401 的描述是 Forbidden，而 401 是 Unauthorized，前者是没有权限，而后者干脆没通过认证。举个例子，你想查看公司财务的详细报表，财务经理一看你就是个一线小程序员，就给你一个 401，告诉你这不是你可以看的东西。如果你想看别的公司财务的详细报表，别人公司财务经理一看你根本不是他们公司的人，就直接给你了个 401 了。（俗话说十个比喻九个不准，我这个比喻当然也并不非常准确，不过对于分不清 401 和 403 的同学而言应该也问题不大🤣） Django 自己对每一个 model 都创建了 create，update，delete 的权限，我们可以直接拿来用，也可以自己添加新权限。Django 自己是针对各个 model 做的权限，所以最简单的权限建立是在 model 层进行的。就比如下面这种，如果我想要为 Student 这个 model 建立相关的权限，就可以通过修改 Meta 类里的 permissions 来实现。 class Student(models.Model): user = models.OneToOneField('django.contrib.auth.models.User') name = models.CharField() # 纯展示，就不详细定义了 birthday = models.DatetimeField() phone = models.CharField() class Meta: verbose_name = '学生' verbose_name_plural = verbose_name permissions = ( # ('权限名', '权限描述'), ('check_classmate_score', '查看同班同学的成绩'), ('send_class_notify', '发送班级通知'), ) 不过这里也看到了，每次对权限进行 CUD 的时候都是在改 model 的，所以每次改动完 model 记得都要进行一次migrate操作才行。不过不用担心性能问题，这个 migrate 只对 Permission 表进行 CUD 操作，而并非改表，所以非常快就搞定了。注意的一点是，不管你把这些 permissions 写在哪个 model 下，最终他们创建好的数据都还是在 Permission 表里，也就是数据库（我这里用的是 MySQL）里的auth_permission表了，这个表结构和数据是下面这样的。 +-----------------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-----------------+--------------+------+-----+---------+----------------+ | id | int(11) | NO | PRI | \u003cnull\u003e | auto_increment | | name | varchar(255) | NO | | \u003cnull\u003e | | | content_type_id | int(11) | NO | MUL | \u003cnull\u003e | | | codename | varchar(100) | NO | | \u003cnull\u003e | | +-----------------+--------------+------+-----+---------+----------------+ +-----+------------------------------------+-----------------+-------------------------------+ | id | name | content_type_id | codename | +-----+------------------------------------+-----------------+-------------------------------+ | 1 | Can add log entry | 1 | add_logentry | | 2 | Can change log entry | 1 | change_logentry | | 3 | Can delete log entry | 1 | delete_logentry | | 4 | Can add group | 2 | add_group | | 5 | Can change group | 2 | change_group | | 6 | Can delete group | 2 | delete_group | | 7 | Can add permission | 3 | add_permission | | 8 | Can change permission | 3 | change_permission | | 9 | Can delete permission | 3 | delete_permission | | 10 | Can add user | 4 | add_user | | 11 | Can change user | 4 | change_user | | 12 | Can delete user | 4 | delete_user | +-----+------------------------------------+-----------------+-------------------------------+ 0X01 在 Django 中校验与分配权限 权限在表里之后“唯一”生效的地方就是 django-admin 了，就是说如果登录 django-admin 的用户没有add_user的权限，那你创建用户的时候就会被拒绝。但是我们平时更多的时候并不是在 django-admin 里，而是自己实现的前台页面，那该怎么自己校验权限呢？Django 中给 user 实例了两个方法：user.has_perm和user.has_perms。显然，前者校验单个权限，后者校验多个权限。咱们先来看一下是如何校验的： In [1]: from django.contrib.auth.models import User In [2]: shawn = User.objects.create(username='shawn') In [3]: shawn.has_perm('auth.add_user') # 单个权限校验直接传权限的 code_name Out[3]: False In [4]: shawn.has_perms(['auth.add_user', 'auth.del_user']) # 多个全线同时检验时将多个 code_name 放在列表中 Out[4]: False In [5]: admin = User.objects.create(username='admin') In [6]: admin.is_superuser = True # 设置 admin 用户为超级管理员 In [7]: admin.save() In [8]: admin.has_perm('auth.add_user') Out[8]: True In [9]: admin.has_perms(['auth.add_user', 'auth.del_user']) Out[9]: True 我们从 Django 源码中可以看到，如果正在校验的用户是“活跃的”而且是“超级管理员”，那就直接不校验了，通过。否则就去校验一波。同时校验多个权限的时候用了all去逐个校验，遇到一个没有的权限也就 False 了。具体的细节这里就不多说了，感兴趣的话可以看一下源码，在django.contrib.auth.models.User中。 def has_perm(self, perm, obj=None): \"\"\" Return True if the user has the specified permission. Query all available auth backends, but return immediately if any backend returns True. Thus, a user who has permission from a single auth backend is assumed to have permission in general. If an object is provided, check permissions for that object. \"\"\" # Active superu","date":"2020-07-05","objectID":"/posts/django-permission/:0:0","tags":["Django","DRF","Permission"],"title":"如何在 Django 与 DRF 中优雅地校验权限","uri":"/posts/django-permission/"},{"categories":null,"content":"0X00 网站怎么登陆 回忆一下你用过的网站们，一般都是怎么登陆的？ “就输入用户名密码登陆呗，要么就扫码登录，要么手机验证码登录，还能有啥”。确实没啥，咱们平时用到的登录方式也就都是这样的，而且这些其实从原理上来说都是 证明你是你 的手段，用户名密码是通过“密码只有你自己知道”为前提的，扫码和验证码是以“手机一定在你自己手上，并且只有你自己能解锁”为前提的。所以其实用户名密码并不是登录的唯一方法，理论上能证明你是你的一切方法都可以用来做授权认证，所以我们可以看到除了密码，出现了扫码、短信邮箱验证码、指纹、人脸、声纹、虹膜巴拉巴拉的。 上面这些都是稳定靠谱的登录方式，我们来想另一个问题，你们有一个网站选用了用户名密码的方式登录，你和用户开开心心登录。后来你上线了另一个网站，你觉得用户该怎么登陆这个新网站呢？“新网站就再注册一次呀！”好的，用户又注册了一次。随着你们公司业务逐渐壮大，上线了 100 个网站，那用户要注册一百次吗？万一自己常用的用户名被占用了呢，岂不是 100 个网站可能会用 100 套不同的用户名密码？那用户可能就要骂娘了呦。 聪明的你可能会想到，既然现在这么流行微服务，我们把登录做一个微服务，所有系统的登陆都用这一套不就好了？ 0X01 这就是 SSO 那有一个好消息和一个坏消息，坏消息是，你以为的这个天才想法就是 SSO（Single Sign On）单点登录，早就被人想到而且实现了很多遍了；好消息是，轮子很多，完全不用自己造，挑一个拿来用就好。 单点登录（英语：Single sign-on，缩写为 SSO），又译为单一签入，一种对于许多相互关连，但是又是各自独立的软件系统，提供访问控制的属性。当拥有这项属性时，当用户登录时，就可以获取所有系统的访问权限，不用对每个单一系统都逐一登录。这项功能通常是以轻型目录访问协议（LDAP）来实现，在服务器上会将用户信息存储到LDAP数据库中。相同的，单一退出（single sign-off）就是指，只需要单一的退出动作，就可以结束对于多个系统的访问权限。 –维基百科 SSO 简单来说就是当你有一堆系统需要登录的时候，通过一个登录服务将所有系统统筹起来的方案。在不用 SSO 的时候，登录一个系统是这样的： 打开网站 A A 网站发现你没有 token 或者 cookie，要求你登录 你输入用户名密码传递给 A 网站 A 网站通过核对发现用户名密码正确，给你下发一个 token 或者 cookie 你带着 token 或者 cookie 就可以正常访问了 如果你要登录一家公司的 3 个系统，那就得重复上面流程三次，如果密码输入错了或者记混了用户名密码和网站的联系可能还会登录好多次。SSO 的存在不能解决你登陆一个系统的问题，但是能解决你登录多个系统的问题。 通常来说 SSO 的工作流程是这样的 打开网站 A A 网站发现你没有 token 或者 cookie，要求你登录，给你一个 302 跳转到 SSO 站点 你把用户名密码发给 SSO SSO 认证通过后发给你 token，并且再通过 302 给你跳回刚才的网站 后面你就可以带着 token 正常访问了 乍一看好像一步都没有少，反而程序上的实现还更复杂了，但是 SSO 的优势就在于再登录新系统的时候。如果这时候你要登录这家公司的 B 系统，在没有 SSO 的情况下就得重复上面的步骤，而且你还得重新输入用户名密码。但是在有 SSO 的情况下是这样的： 你打开网站 B B 网站发现你没有 token，要求你登录，给你一个 302 跳转到 SSO 站点 SSO 站点是同一个，所以你会带着 token 访问 SSO，这时候 SSO 就知道你登陆过了，就会给你一个用于网站 B 的 token，并且 302 跳转过去 你带着 token 正常访问 B 网站 从条数上看没少什么，但是你会发现真正你人工在操作的就只是“打开 B 网站”而已，后面的都是程序在跳转。这样不仅成功实现了一套用户名面走天下的方案，还成功地给用户减小了登录多个系统的麻烦。 0X02 CAS-SSO 如何工作 那么 CAS 又是什么呢？ 集中式认证服务（英语：Central Authentication Service，缩写CAS）是一种针对万维网的单点登录协议。它的目的是允许一个用户访问多个应用程序，而只需向认证服务器提供一次凭证（如用户名和密码）。这样用户不仅不需在登陆web应用程序时重复认证，而且这些应用程序也无法获得密码等敏感信息。“CAS”也指实现了该协议的软件包。 –维基百科 也就是说，CAS 是 SSO 的一种实现方式。它的工作流程是这样的： 用户访问站点 A 网站 A 将请求重定向 CAS 服务器 用户在 CAS 处输入用户名密码，提交 CAS 认证通过，给用户发一个 token，并重定向回站点 A 用户将 CAS 给的 token 带到站点 A 站点 A 将 token 拿去 CAS 服务器核查（因为 站点 A 现在不知道用户是谁，自己也识别不了 token） CAS 服务器核查通过，并将用户 A 的信息返回给站点 A（这时候站点 A 知道用户是谁了） 登录成功，可以正常使用 token 访问站点了 ","date":"2020-07-04","objectID":"/posts/sso-yu-cas/:0:0","tags":["SSO","CAS","Auth"],"title":"SSO 与 CAS","uri":"/posts/sso-yu-cas/"},{"categories":null,"content":"0X00 运行一个 Django 程序 运行一个 Django 程序可太简单了，从创建项目到运行起来总共也不超过 5 行代码。项目运行起来了就可以打开我们的 vim 或者 IDE 之类的一顿 coding 了。作为最最最开始写 Django 的同学来说到这里也就了解的差不多了，因为大家都是自己写好代码本地测试一下就提 Pul Request 到上游仓库了，然后什么单元测试、数据库迁移、测试环境版本发布甚至可能包含 docker 镜像更新就全都交给 CI 来做了。自己就这么开开心心的写了一段时间的代码，一切都在朝着好的方向发展。突然有一天部门主管或者老大告诉你有一个新项目要你来开个头，先搭好脚手架然后发布上去，后面再来人一起做功能迭代。 然后你开开心心地django-admin startproject xxxx、开开心心地django-admin startapp xxxxx，一顿 coding 之后懵逼了，没有部署过测试环境，没有部署过生产环境，只知道 CI 给做了，却完全不知道做了什么。然后你跑去看 CI 脚本，去问其他同事同学，得到了一堆 Nginx 和 uWSGI 之类的答复。你也照着做了，但是完全不知道为什么，因为你觉得python manage.py runserver明明就可以启动项目了，为什么还需要搞什么 Nginx 和 uWSGI 呢？ 0X01 部署到生产环境 按我现在手上的项目来说，部署阶段用到了：docker、uWSGI、supervisor、Nginx、fabric、gitlab-ci 这 5 种技术。其中 docker 是为了方便大家的生产环境、测试环境和开发环境高度一致的；uWSGI 和 Nginx 后面单独说；supervisor 是用来保活的，有时候进程挂了需要有人来重启它；fabric 是用来将常用的命令组整合的；gitlab-ci 是用来方便运行单元测试、测试环境发布和生产环境发布的。 下面就重点来解答一下这个新手群体中很容易出现的疑问： 为什么明明python manage.py runserver就能运行的 Django 项目非要用 Nginx 和 uWSGI 运行呢？ 0X02 为什么需要 uWSGI 具体为什么要用 uWSGI 其实偶遇好多个原因，下面会逐个提到。 首先最常见的一种说法是：“python manage.py runserver是单线程的，一个请求不结束其他的请求就得阻塞，所以性能差”，这种说法彻头彻尾是错误的。而且验证起来也及其容易，随便找一个你的 Django 项目然后找个 api 用 ipdb 打个断点，通过 runserver 把它跑起来。然后先调用打了断点的 api，发现它卡在断点处了，然后再调另一个 api，实际上是可以得到 response 的，所以并不是所谓单线程导致的。与其说是因为单线程导致性能低下倒不如说是因为“单进程”，众所周知 Python 中有一个 GIL 全局解释锁，这东西的存在导致真正意义上的单进程多线程 Python 程序并不得行，所以 uWSGI 可以通过启动多个进程的方式来规避这个问题，也就是下面这个配置文件中processes = 8的部分。 [uwsgi] chdir = /code pp = /code module = ratel.wsgi master = true processes = 8 vacuum = true http = 0.0.0.0:7701 stats = /tmp/xxxxx_stats.socket env = LANG=en_US.utf8 env = DJANGO_SETTINGS_MODULE=xxxxx.settings harakiri = 60 http-timeout = 60 socket-timeout = 60 max-requests = 8192 listen = 8192 no-orphans 这里说到多进程，有个题外话可以说一下：“你觉得 Django 中使用全局变量有什么需要注意的吗？”。通常情况来说编程中使用全局变量是一个很常见的问题，但是在多进程的情况下就需要多多注意了。比如按上面的配置，一个项目用了 8 个进程，其中一个进程接到了一个 request 并且把CURRENT_COUNT从 80 改到了 100，但是其他进程在访问自己进程里的CURRENT_COUNT的时候还是 80，这就是因为多个进程中全局变量不够“全局”导致的问题。那么怎么解呢？其实也简单。我们还是用CURRENT_COUNT，但是不用它做变量名了，而是 MySQL 的配置表中的一条数据，或者 Redis 中的 key 就能解决这个问题了。 性能问题当然是个大问题，不用 runserver 而是改用 uWSGI 的一大原因就是性能问题，我们用 ab 来测试一下 runserver 和 uWSGI 两种方式启动项目的性能差距，ab -n 1000 -c 100 -p data -T application/json http://127.0.0.1:7070/remit_verify/fetch_order/意思为总共 1000 次请求，并发 100，访问后面的登陆接口。首先看一下使用 runserver 方式运行的 Django 成绩： Concurrency Level: 100 Time taken for tests: 53.702 seconds Complete requests: 1000 Failed requests: 0 Non-2xx responses: 1000 Total transferred: 269000 bytes Total body sent: 167000 HTML transferred: 63000 bytes Requests per second: 18.62 [#/sec] (mean) Time per request: 5370.198 [ms] (mean) Time per request: 53.702 [ms] (mean, across all concurrent requests) Transfer rate: 4.89 [Kbytes/sec] received 3.04 kb/s sent 7.93 kb/s total Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.6 0 3 Processing: 18 1574 6329.8 113 53693 Waiting: 6 1567 6330.6 107 53691 Total: 18 1575 6330.2 114 53695 Percentage of the requests served within a certain time (ms) 50% 114 66% 133 75% 147 80% 177 90% 1543 95% 7727 98% 27565 99% 53662 100% 53695 (longest request) 然后再来看一下使用 uWSGI 的成绩： Concurrency Level: 100 Time taken for tests: 2.223 seconds Complete requests: 1000 Failed requests: 0 Non-2xx responses: 1000 Total transferred: 231000 bytes Total body sent: 167000 HTML transferred: 63000 bytes Requests per second: 449.93 [#/sec] (mean) Time per request: 222.258 [ms] (mean) Time per request: 2.223 [ms] (mean, across all concurrent requests) Transfer rate: 101.50 [Kbytes/sec] received 73.38 kb/s sent 174.87 kb/s total Connection Times (ms) min mean[+/-sd] median max Connect: 0 0 0.8 0 4 Processing: 10 213 30.9 222 298 Waiting: 6 213 31.0 222 297 Total: 10 214 30.5 222 300 Percentage of the requests served within a certain time (ms) 50% 222 66% 225 75% 226 80% 227 90% 229 95% 231 98% 250 99% 274 100% 300 (longest request) 其中使用 runserver 方式运行压力测试总共耗时 53.7 秒，uWSGI 耗时 2.2 秒。使用 uWSGI 后吞吐率是 24 倍，用户平均等待时间是 1/24，服务器处理时长也是 1/24 的样子。综合来说在我自己配置情况下得到了答曰 24 倍的性能提升，我这儿还是只有双核双进程的情况，如果我给 docker 配置允许使用所有 12 个逻辑处理器，情况会更显著。 0X03 为什么需要 Nginx 至于为什么要用 Nginx 就更简单了，有这么几个原因：静态资源、","date":"2020-06-30","objectID":"/posts/why-django-need-uwsgi/:0:0","tags":["Django","uWSGI"],"title":"为什么 Django 需要uWSGI","uri":"/posts/why-django-need-uwsgi/"},{"categories":null,"content":"0X00 前言 一见程序员，立刻想到 web 开发，立刻想到后台管理系统，立刻想到数据展示，立刻想到数据筛选筛选，立刻想到数据统计，立刻想到导出 Excel 表格。产品经理的想象惟在这一层能够如此跃进。 –鲁迅：我不是，我没有，别瞎说 虽然上面这种说法有点夸张了，不过确实很多很多很多人在工作中遇到过不止一次的需要在一个 web 系统里添加一个”数据导出”的功能，而且通常都是导出成 csv 这种文件。自然我也遇到了很多很多很多次，也写过那种最蠢的手拼逗号的 csv 导出，还看过别人效果更好代码量更少的版本。也就在此总结一下具体这个 csv 导出该怎么搞才好。 最蠢的方案可能就是我最早实习的时候写出来的那种手拼逗号的方案了，为了大家刚吃的早午晚饭着想，就不给大家看了，省得吐出来浪费粮食。真正用的比较多的是这么两种：一种是传统的拼接二维数组的方式来模拟表格，然后通过 Python 的 csv 库直接导出；另一种是使用 djcsv 来进行导出。下面来简单看一下嘞。 0X01 原始 csv 导出 其实简单的导出原始的方法就可以解决问题了，下面只列出重点代码。首先导入 csv 模块；然后打开一个文件用来保存这次导出的内容；接下来按行写入表头；然后循环整个 queryset 构建一行行的数据从而写入；最终导出就成功结束了。 import csv with open('export.csv', 'w') as f: # 打开待用的文件 csv_writer = csv.writer(f) # 生成一个 csvwriter table_title = ['姓名', '性别', '生日'] # 准备表头 csv_writer.write_row(table_title) queryset = Student.objects.all().only('name', 'gender', 'birthday') for student in queryset: line = [ student.name, student.gender, student.birthday.strftime('%Y-%m-%d'), ] csv_writer.write_row(line) 这样导出来的文件就是我们需要的内容了。我们来看一眼成果（不是诸葛大力，是我们导出来的 csv 文件🤣）： 发现问题了没？我们的性别出现了一丢丢问题，本来是用 M/F 来当做男女来存的（这种情况其实非常多的，比如你的类型可能用了 integer 然后用一个 map 去映射到不同的中文名上去），现在却把数据库中真实的内容导出来了。 | 姓名 | 性别 | 生日 | | -————— | | Kevin Armstrong | F | 1988-01-02 | | Patricia Robinson | F | 1988-01-02 | | Michael Duffy | F | 1988-01-02 | | Kathryn Hodge | M | 1988-01-02 | | Justin Carlson | F | 1988-01-02 | | Larry Jones | F | 1988-01-02 | | Peter Palmer | F | 1988-01-02 | | William Smith | F | 1988-01-02 | | Karen Garcia | M | 1988-01-02 | | Eric Williams | F | 1988-01-02 | | Eduardo Bell | F | 1988-01-02 | | Cynthia Lee | M | 1988-01-02 | | Brandy Hoffman | F | 1988-01-02 | | Emily Jones | F | 1988-01-02 | | Kelly Perry | M | 1988-01-02 | | Jamie Nixon | F | 1988-01-02 | | Jeffrey Vega | F | 1988-01-02 | | John Chen | M | 1988-01-02 | | Laura Stevens | M | 1988-01-02 | | Linda Robinson | M | 1988-01-02 | 如果你说这种问题不大，那现在我们要求加一列 ”是否成年“ ，然后这个是否成年又没有存，只有用生日来计算，那咋搞？可能只有在line = [xxxxx]的地方再加一行'成年' if (datetime.datetime.now() - student.birthday).days / 365 \u003e= 18 else '未成年'才行了。当然这只是理想情况，正常情况下一张业务表可能会有 100 多个字段，导出的时候可能要从这 100 多个字段中选择 80 多个导出来然后还要导出他们外键关联的其他表的数据。这是由上面这种写法就会越来越长，而且尤其是当”不能从数据库中直接取“的数据越来越多的时候就会麻烦了。下面介绍的这种使用 djcsv 导出的方法就很好用了。 0X02 使用 djcsv 导出 这种方法需要安装一个三方库 djcsv ，顾名思义它就是用来方便 Django 导出 csv 文件的。这坨代码的具体解释就直接写在下面注释里了。 from djcsv import write_csv def export_csv(): queryset = Student.objects.all() queryset_values = queryset.values( 'name', 'gender', 'birthday', 'teacher__name', # 跨表也是可以的 'teacher__gender', ) # 导出字段表头 field_header_map = { 'name': '姓名', 'gender': '性别', 'birthday': '生日', 'teacher__name': '老师姓名', 'teacher__gender': '老师性别', } # 格式化数据 gender_dict = dict(Student.GENDER_CHOICES) \"\"\" Student.GENDER_CHOICES = ( ('M', '男'), ('F', '女), ) \"\"\" field_serializer_map = { # 折页机就是为什么要叫 serializer 的方法了，因为确实有一个翻译在这儿 'gender': (lambda x: gender_dict.get(x, '其他')), 'teacher_gender': (lambda x: gender_dict.get(x, '其他')), } with open('export.csv', 'w') as csv_file: write_csv( queryset_values, csv_file, field_header_map=field_header_map, field_serializer_map=field_serializer_map ) ","date":"2020-06-28","objectID":"/posts/django-export-csv/:0:0","tags":["Django","CSV"],"title":"Django 相对优雅地导出 CSV","uri":"/posts/django-export-csv/"},{"categories":null,"content":"0X00 前言 啊，这个破系统怎么这么慢。 -你写的程序的用户 是的我写过一篇类似的博客，但是一篇肯定说不完，毕竟影响性能的东西太多了：数据量巨大、机器配置差、查询SQL效率低、额外的多余的查询、低质量的代码balabala的。今天这篇文章主要是从Django查询和ORM层面来分析一下API变慢的原因。 其中可能性比较多，我这里先挑几个我经常遇到的情况来说：不管自己的需求直接查询所有字段、完全不在意索引、疯狂使用in、循环创建/更新数据、不善于使用缓存。这几个问题其实都会对我们的API响应速度造成比较大的影响，下面我们来一个个介绍一下。 0X01 SELECT * 首先我们一定知道，从数据库里查询的数据越多那反应也就越慢。我这里拿一个只有500条数据的表来展示一下，这里可以看到两次的耗时虽然都很短（那可不嘛，就500条数据），但是比例上还是差了三倍还多，如果随着数据量变大和字段变多这个差距就会更加明显。但是我们可以看到在循环里只需要name这个字段，多余的是用不到的。 实际上这两个查询执行的是不同的两个SQL（当然了这是废话），那具体区别呢？可以看到一个是SELECT * 一个是单独取了两个字段。 SELECT * FROM `main_staffprofile`; -- 其实并不是一个*，而是把所有字段都列在这儿了，这里方便展示就不都贴出来了 SELECT `main_staffprofile`.`id`, `main_staffprofile`.`name` FROM `main_staffprofile`; 所以说，当我们确定知道自己下面要用到什么字段的时候，就给queryset加上only('xxx', 'xxx', 'xxx')的参数，从数据库里取的时候尽可能少的取数据，这样就能加快查询了。尤其是在查得的数据很大和字段很多的时候，比如在表里取10000条数据但是只用到30列中的3列的时候就非常应该加上only。 0X02 Index/index_together 好吧，说起优化速度就绕不开索引，这是必然的。我们都知道在数据库里给字段加索引可以大幅提升查询效率，具体在Django里建立普通索引也是很简单的，直接在字段定义里加一个参数index=True就好了。但是可能大家会忽视联合索引的建立，这里简单介绍一下： class Meta: verbose_name = u'随便一个什么表' verbose_name_plural = verbose_name index_together = [ ['gender', 'age'], ['gender', 'address'], ['city', 'gender', 'name'], ] Django中的每个Model类我们都会创建一个Meta子类，子类里有一个属性叫做index_together顾名思义这就是定义联合索引的地方了。index_together是一个二维数组，第一维度下面的每个item都是一个联合索引，第二个维度下面的每个item就是一个字段，也就是构成联合索引的字段（注意联合索引是讲究顺序的喔）。具体索引和联合索引应该怎么建才会让数据查询更快可以看我上一片博客或者搜索其他资料。 注意：索引/联合索引定义好之后需要进行数据库迁移 0X03 IN balabala 我们在Django的查询中无处不见这种User.objects.filter(user_type__in=('1', '2', '3', '4,', '5'))的用法，这种用法在业务上确实有需求，所以我们是避免不掉的。不过还是可以通过一些其他的方法来尝试规避，比如从业务上看看这些选项组合到一起的时候是不是意味着一种特殊的数据，比如User.objects.filter(city__in=('成都', '绵阳', '自贡'........))可以发现是在查询四川省，那么可以考虑换成User.objects.filter(province='四川'')这种。或者User.objects.filter(level__in=('A', 'B', 'C', 'D', 'E', 'F'))是不是意味着User.objects.exclude(level='S')。 这种情况因为业务上的严格需求，可能并没有太好的调节方法（也可能是我没想到，如果大家有方法的话欢迎反馈给我，谢谢指教🙏），不过遇到了的话还是可以尝试上面两种方案，玩意能行的话还是可以调节一下性能的。 0X04 疯狂关联 条条大路通罗马。 疯狂关联也比较容易出现，正所谓条条大路通罗马，但是总会有一条更近一点的路。比如这种User.objects.filter(father__father__name='shawn'')完全就可以用User.objects.filter(grandfather__name='shawn'')这种来替代。这个其实大家都会比较清楚，对技术没什么考验，但是比较考验对程序的理解程度尤其是数据库的理解程度。有时候我们接手一个新的项目时候可能并没有对这个项目的数据库有充分的了解，所以可以尝试在写这种多层的关联查询之前取调研一下是不是有更近的通向罗马的路。 0X05 bulk_create 这种问题应该是最常遇到的了。我们从前端接收到了这么一波数据要写到数据库里（数据哪儿来的并不重要，也可能是csv来的或者其他什么鬼地方） [{'name': 'shawn_0'}, {'name': 'shawn_1'}, {'name': 'shawn_2'}, ............................... {'name': 'shawn_998'}, {'name': 'shawn_999'}] 新手同学很有可能写出如下代码： for user in user_list: User.objects.create(**user) 但是我们来看看速度，耗时4秒，再看一下批量创建的时间，耗时0.13秒，性能提升了很多倍。 批量创建虽好，但是有一点就需要特别注意：如果你要创建的数据比较复杂并且量非常大，请不要一次性全部创建完成，否则可能会对数据库造成较大的压力，可以尝试将比如10000条数据分成10个1000或者100个100来创建。 0X06 cache/cached_property 缓存可能是提升性能最明显的方案了，同样数据量的数据从MySQL里取出来和从Redis里取出来，耗时根本不在一个数量级，所以我们应该善用缓存。 比如我们有一个巨大的关键词配置放在数据库里存起来了，大概有10W条数据，平时又很少更新，但是经常会查，那就非常适合使用缓存。我们可以这样 def get_keyword_config(): result = cache.get(KEYWORD_CONFIG_KEY) if result: return result else: queryset = Keyword.objects.all() result = handle_keywords(queryset) cache.set(KEYWORD_CONFIG_KEY, result) return result 然后在更新关键词库的时候再同时刷新一次缓存就好了。另一种使用缓存的方式是Django中带的cached_property。我们知道Django中有一个property可以把function作为属性来用，直接通过点就能获取值，这个顾名思义就是加了缓存的property。也就是说每次调用student_count的时候都要计算一遍，但是调用cached_student_count的时候只有第一次会计算，后面就是缓存的值了。 class Teacher(models.Model): xxxxxx @property def student_count(self): return self.student_set.all().count() @cached_property def cached_student_count(self): return self.student_set.all().count() 下面是官方文档关于cached_property的解释，简单明了： 注意里面提到的生命周期，小心用到已经失效的数据 将一个类方法转换为特征属性，一次性计算该特征属性的值，然后将其缓存为实例生命周期内的普通属性。 类似于 property() 但增加了缓存功能。 对于在其他情况下实际不可变的高计算资源消耗的实例特征属性来说该函数非常有用。 class DataSet: def __init__(self, sequence_of_numbers): self._data = sequence_of_numbers @cached_property def stdev(self): return statistics.stdev(self._data) @cached_property def variance(self): return statistics.variance(self._data) ","date":"2020-06-06","objectID":"/posts/django-api-fater-2/:0:0","tags":["Python","Django","Index"],"title":"如何让 Django API 再快一点 (2)","uri":"/posts/django-api-fater-2/"},{"categories":null,"content":"0X00 前言 \u0026 Pythonic Python管filter/map/reduce这些叫高阶函数，听起来有点高级有点难搞的意思，实际上是贼简单的东西。下面通过几个简单的例子来帮助大家了解一下filter/map/reduce这三个高阶函数的简单用法。 事先声明，这三个函数都是扩展性质的东西，从来不用这三个函数也可以正常的编写程序，没有什么功能是没了这三个函数就写不出来的。只不过是这三个函数的出现能让之前很丑陋的代码变得精简易读了而已。 这三个函数非常适合搭配lambda来使用，编写非常Pythonic的代码，具体什么是Pythonic其实很难定义，其实就是把Python编程一个形容词了，比如你看到一个人“穿了运动鞋牛仔裤帽衫双肩包黑框眼镜电子表”就会说他“太程序员了”，大概就是这么个意思。总结来说呢就是 非常具有Python特色的Python代码 。比如下面这段代码明显就不Pythonic for index in range(len(name_list)): print(name_list[index]) 而这种代码就是Pythonic的写法 for name in name_list: print(name) 尤其是结合了lambda之后，就能写出更Pythonic的代码了，例如 def is_boy(student): if student.gender == 'M': return True else: return False 就可以直接用lambda改写成这个样子 is_boy = lambda studnet: student.gender == 'M' 0X01 filter filter顾名思义，一定是一个筛选器。当我们有一个列表想要找出这个列表中满足某些条件的数据，在不是用filter的情况下很有可能会写出这样的代码 number_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] def is_odd(input_number): if input_number == 0: raise Exception('不考虑0的问题，我们现在是研究filter') return input_number % 2 odd_list = [] for number in number_list: if is_odd(number): # 把奇数怼到新列表里 odd_list.append(number) print(odd_list) 但是借助filter就可以将新列表的生成变得很简单，本质上只要了一行代码。 odd_list = filter(is_odd, number_list) print(list(odd_list)) filter接受两个参数，第一个参数是个只接受一个参数的function，第二个参数是个列表。工作原理就是将列表中的对象一个个塞到第一个参数的function中取得返回值，将返回值为True的保存到新列表中，最终返回。 Python2中filter返回的直接是列表，而Python3中返回的则是可迭代对象。 0X02 map map可以用来批量处理数据，批量传参。map接受不定长度的参数，其中第一个参数固定为一个“接受n个参数”的function，然后后面紧跟真就是n个可迭代的参数。 #!/usr/bin/env python3 # coding=utf-8 # 接受n个参数 def join_name(first_name, middle_name, last_name): \"\"\"拼接三个字符串\"\"\" return '{}_{}_{}'.format(first_name, middle_name, last_name) if __name__ == '__main__': first_name_list = ['zhang', 'li', 'wang', 'zhao'] middle_name_list = ['a', 'b', 'c', 'd'] last_name_list = ['san', 'si', 'wu', 'liu'] # 方法1，使用map传递n+1个参数，第一个是function对象，后面的是参数列表 for result in map(join_name, first_name_list, middle_name_list, last_name_list): print(result) # 方法2，不使用map，相当于下面这种 for index in range(len(first_name_list)): result = join_name(first_name_list[index], middle_name_list[index], last_name_list[index]) print(result) 输出的内容都是这个样子的： 使用了map之后不仅在for那里不用非常蠢得用range(len(first_name_list))了，也不用拿到下标之后到处跑着用下标取值了。而且有一个很大的优点是：map自动使用多个参数列表中最短的那个，也就是说不会出现IndexError: list index out of range的数组越界问题了。 0X03 reduce reduce方法从Python 3开始就不是全局命名空间里的function了（说人话就是挪到官方包里去了，需要导入，不能直接用了），所以我们需要from functools import reduce才能用到。reduce相对更简单一些，固定接受两个参数，第一个参数是一个“固定接受两个参数的”function，第二个参数是一个可迭代对象。具体用法可以看示例，非常通俗易懂。 #!/usr/bin/env python3 # coding=utf-8 from functools import reduce def join_chars(char_a, char_b): \"\"\"拼接两个字符串, reduce的第一个参数只能是一个接受两个参数的function\"\"\" return char_a + char_b if __name__ == '__main__': char_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'] print(reduce(join_chars, char_list)) # 使用reduce # 其实这个reduce就类似于下面这种写法 res = join_chars(char_list[0], char_list[1]) res = join_chars(res, char_list[2]) res = join_chars(res, char_list[3]) res = join_chars(res, char_list[4]) res = join_chars(res, char_list[5]) res = join_chars(res, char_list[6]) # 或者类似这种 res = join_chars( join_chars( join_chars( join_chars( char_list[0], char_list[1] ), char_list[2] ), char_list[3] ), char_list[4] ) 0X04 结合lambda 结合lambda之后可以将上面的is_odd改写成 #!/usr/bin/env python3 # coding=utf-8 number_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] odd_list = filter(lambda number: number % 2, number_list) print(list(odd_list)) 可以将上面map中的代码改写成这样： #!/usr/bin/env python3 # coding=utf-8 if __name__ == '__main__': first_name_list = ['zhang', 'li', 'wang', 'zhao'] middle_name_list = ['a', 'b', 'c', 'd'] last_name_list = ['san', 'si', 'wu', 'liu'] # 方法1，使用map传递n+1个参数，第一个是function对象，后面的是参数列表 for result in map( lambda a, b, c: '{}_{}_{}'.format(a, b, c), first_name_list, middle_name_list, last_name_list ): print(result) 可以将上面reduce中的代码改成这样： #!/usr/bin/env python3 # coding=utf-8 from functools import reduce if __name__ == '__main__': char_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n'] print(reduce(lambda a, b: a + b, char_list)) ","date":"2020-05-25","objectID":"/posts/python-filter-map-reduce/:0:0","tags":["Python"],"title":"Python 中的 filter 与 map/reduce 方法","uri":"/posts/python-filter-map-reduce/"},{"categories":null,"content":"0X00 GTD 尽管去做 （英语：Getting Things Done / GTD ），是一种行为管理的方法，也是戴维·艾伦写的一本书的书名。 GTD的主要原则在于一个人需要通过记录的方式把头脑中的各种任务移出来。通过这样的方式，头脑可以不用塞满各种需要完成的事情，而集中精力在正在完成的事情。 ​ –维基百科 GTD说白了就是把自己要做的事项从脑子里拿出来，写到纸上或者记到什么鬼地方，我最喜欢的就是\"滴答清单\"了。 最早的时候我用的是Google日历，在学校里的时候这样用了一两年还不错。后来大四出来实习，那段时间工作哪儿哪儿都不顺利，也就没心情安排自己的生活和学习了，所以实习的那一年基本就是下班了就趴在床上玩手机，什么都没干成。不只是没学习，而是没玩游戏没看电影，实打实的什么都没干成。后面又重拾了GTD，把自己安排的明明白白的，对我来说效果还是立竿见影的。我安排自己不只是学习，也包括社交和娱乐，约好什么时间和谁在哪儿吃饭或者干嘛的；自己给自己规定什么时间玩游戏，给自己规定什么时间看电影。 这样一套流程下来，有两个明显的好处：自己的时间利用率极大的提升了，不会出现坐在桌子前面考虑接下来要干什么的事情；另一个就是方便自己回顾生活，毕竟不写日记的话自己也搞不清楚哪天哪天干了什么事儿了。这里给大家看一下我滴答清单里的任务分类： 这里主要是学习类、个人日常生活类、工作和年度规划。除了每年年底会对新一年进行规划以外，还会在每个月对下个月进行规划。不过正所谓\"取法其上，仅得其中；取法其中，仅得其下\"，所以也不要因为自己没有完成月内所有计划或者年度计划而自我否定，只要在没有突发事件的情况下都按部就班完成了自己的任务安排，那损失的那一点点跟全局比起来都是小问题了。 安排时间的时候有个问题需要注意：给自己一定的缓冲时间，比如整整一周不要尝试将周一至周五的晚上都安排上学习，然后周六周日又安排三四部电影和游戏时间。因为很大可能你周五晚上下班是要跟朋友约出去吃吃喝喝的，很大可能周末会被叫出去玩或者自己睡过了之类的。而且即使没有这些外在因素，也不应该给自己安排的满满当当的。我自己的建议是这样：周一到周四的晚上安排三次学习，空出来一天用来缓冲；周五晚上不安排，即使没有约也可以自己给自己放个假；周六周日给自己安排一天半的时间，而且然后周六晚上不安排学习，可以的话给周日晚上安排一波学习。 仅从我自己的角度来说，这类GTD方案让我非常受用，不仅时间变多了，也更有节奏了。自己知道周一到周四晚上不该玩游戏，周末又可以玩一整天。时间拍起来之后的生活里我玩到了很多以前觉得没时间玩的优秀游戏，比如 战神 塞尔达 GTA5 和一众优秀游戏 ，也看了很多以前觉得没时间看的豆瓣Top250电影。 0X01 卸载 我不知道其他人，反正我自己入手一个新的电子产品尤其是：手机、平板、电脑的时候，就会疯狂的配置，然后疯狂安装软件，一度觉得不多装几个软件新电脑、手机就白买了似的。这里给出的方案恰恰与之相反：卸载 。 针对电脑来说，这个其实没所谓，主要还是在手机和平板上。假设你想控制自己少玩手机少玩平板，其实很简单呐，平时玩什么就卸载什么就得了。我本来就对手机游戏没有兴趣，但是以前手机上有的时候偶尔还是会玩一玩，现在直接卸载掉就不玩了，也完全不想玩。 最应该卸载的其实是手机里的那些\"社区\"，如果你有一两个自己特别喜欢的社区，不管是知乎还是豆瓣，那都应该留着。不过如果你手机里装了一大堆社区类的app，那完全可以考虑卸载一波，这样就省的让你拿起手机一顿看之后单纯的因为无聊而点开一个app。当然了，卸载是一个非常极端的做法，还有不那么极端的做饭：修改通知\u0026推送。 0X02 通知\u0026推送 现在拿起来你的手机，看一看有多少未读的推送和通知呢？再想想你有多少次是正在开心的学习、玩游戏、看电影的时候听到了手机\"叮咚\"一声推送，然后拿起来看了看，关掉了无关的推送，然后又点开了微博知乎吧啦吧啦？ 所以说如果做不到卸载，那就关掉他的推送。其实你仔细想想，你很需要微博给你推送那些乱七八糟的流量明星出轨恋爱多人运动的消息吗？你很需要知乎给你推送的人在内蒙刚下航母吗？你很需要bilibili给你推送的黑人抬棺交响乐版吗？其实并不，反而是他们给你推送之后你可能会因为好奇点开，就算没点开也有可能会点开其他的app然后中断自己正在做的事儿。 所以现在我手机上除了微信、QQ这类工具以外，大多数程序的推送我都关掉了，因为我并不是很关心他们的推送，反而这些推送会严重干扰到我。当然如果你实在不想关掉推送，那也可以把推送的声音和\"通知亮屏\"这种关掉，防止你专心工作学习玩游戏的时候被手机中断。 0X03 设备独立 设备独立就很简单了，试想你正在用手机看一篇技术文章或者一篇其他的长文，看了三分钟就收到了朋友在微信群里at你给你看沙雕视频的通知，你坚持住没点；五分钟后又收到同事发给你的\"王者峡谷邀请函\"，你又坚持住了；十几分钟后又收到了UC推来的\"震惊🤯七旬老人竟和孙子在家中做这种事情！\"，好吧，这次你可能没坚持住，点进去了。这样算下来你一共看了半个小时文章，中间被打断了好几次。 如果你用Kindle看书，或者干脆拿纸质书，那肯定就不会出现这种问题了。如果你用walkman听歌，那就不会出现手动切歌的时候顺便刷上半个小时微博的问题。如果你用switch/PS4/XBox玩游戏就不会遇到游戏中被一个突如其来的系统更新打断或者被视频聊天打断的问题。 所以说在可以的情况下，尽可能让自己的设备独立，每个设备做每个设备最擅长的事情，这样也最不容易被打断。（每个工具只做一件事，并将其做到极致。 其实这也是经典的UNIX思想） 0X04 锁定 我之前的一加手机有\"禅定模式\"，现在的小米手机有\"专注模式\"，这个功能很棒。手机打开禅定模式前会让你选择时间，比如30分钟、60分钟、90分钟这种。选好之后就开始进入禅定模式了，禅定模式期间手机只能看时间、接电话和拨打紧急电话，其他一切的通知、推送、短信、at、全都没了。这种情况下你的工作学习效率会高很多，起码不会被手机打扰到。 在iPad上也有类似的，叫\"勿扰模式\"，macOS和Windows上也有，连Linux上都有。虽然功能没有手机那么夸张，但是也会帮你拦截绝大多数通知，然后让你自己沉浸到当前的工作和学习当中去。强烈推荐在电脑上使用勿扰模式的时候将当前窗口全屏，这样就可以将干扰放到最低。 0X05 立flag 立flag是个好办法，把自己的计划疯狂得说出来。比如你打算一年减肥20斤，那就告诉全世界，虽然你谁都不告诉自己悄悄减失败了也不丢脸，但是也会给自己埋下一个说辞\"不成功也没人知道的，问题奀\"。所以应该大胆的告诉周围人你的计划（当然了，私密的问题就别说了，别老说争取下次争取涨个 xx% 的工资这种）。当所有人都知道你要减肥20斤的时候，下次你再跟他们吃饭还吃那么多，你的朋友就会时不时提醒你甚至\"嘲讽\"你，给你一些动力。而且你自己也会给自己心理暗示：“NB都吹出去了，不能一点动静都没有吧，还是得努努力才行”。 我自己也是在朋友圈里立过flag的，说什么\"完成不了就请朋友圈全圈吃火锅\"这种听起来就不靠谱的话。大家也都知道是在开玩笑，但是遇到你做了和计划相反的事儿还是会提醒你的。虽然我最后没有彻底完成，也没有请全圈吃火锅。估计真请的话怕是两个月工资都不够🤣 0X06 最后 最后简单总结一下，这些技巧也好方法也罢都是我自己总结到的，对我自己是很有用的，但是不确定对大家是不是有用。如果有兴趣的话也可以试一下，毕竟大家都是沙雕网友，受用的方法应该也不会差太多吧哈哈哈哈哈🤪 ","date":"2020-05-14","objectID":"/posts/self-control-and-target/:0:0","tags":["Life"],"title":"自控\u0026目标","uri":"/posts/self-control-and-target/"},{"categories":null,"content":"0X00 前言 啊，这个破系统怎么这么慢。 –你写的程序的用户 是的，我用Django写的程序经常会出现性能问题，有时候是逻辑问题、有时候是数据库问题、有时候又是机器问题。我就现在这儿总结一波我自己的经验好了（这里都是基于我自己的经验来的，可能会相对比较简单，没有太骚太复杂太高级的东西）。这儿默认大家都是用的Django + Django REST framework了，因为我自己是用的这套技术栈，而且这套技术栈也算是Django生态下前后端分离的最常见的了。 0X01 问题出在哪儿呢 众所周知\"想要解决问题，首先就要找到问题在哪儿\"。那怎么判断问题在哪儿呢？ 首先登到服务器上看htop，有面有一个Load average就是综合负载。一般来说，如果你的服务器是n核心的，那负载在n-2以下就算是正常的，快到n了也不是不能用，但是就要考虑升级了。这里给出来了三个负载值，从左到右依次是1分钟、5分钟、10分钟的负载情况。（为什么说是一般情况呢，如果你就只有一两个核心，那这个算法肯定不生效；如果你有128核，那负载到126了就意味着马上就炸🤣。所以说一半双核心不超过1.5、4核不超过3，8核不超过6这种） 如果确定了就是机器性能的问题，那就好办了，升级服务器就好（当然不是不够久升级，还是要觉得当前的数据量啊并发啊已经挺高了再考虑。要不然一慢就升级服务器，那岂不是太奢侈了，而且对自己的代码质量也没有一点好处） 我们假设不需要升级服务器配置，那就从程序和数据库两个方面来说。一般是先打开MySQL的慢查询日志，然后根据慢查询日志来逐渐优化表结构，优化查询，优化程序逻辑。 0X02 代码质量低or逻辑问题 代码质量低是个问题，一般来说呈现在这几个地方：多余的循环次数、查了完全没卵用的数据、进行额外的操作。我们都知道计算机里几种处理速度的差距是巨大的CPU缓存\u003e\u003e内存\u003e\u003e硬盘\u003e\u003e网络，这四个之间的性能差异两两之间往往可以差出至少一个数量级（其实随着网络发展，现在网络速度已经可以赶上机械硬盘了）。而其中最慢的就是I/O了，所以我们应该尽一切可能避免I/O，而且有一点要注意的是\"读写数据库\"当然也算I/O。下面列举两种常见的问题 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:0:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"多余的I/O for student_data in studnet_list: token = get_token_from_another_system_with_http_api() response = requests.post(url, student_data, headers={'token': token}) 比如说这部分代码，我们都知道一个token不应该是一次性的，那把这个多余的取token的方法放在循环外面就好了。其实如果get_token_from_another_system_with_http_api()不是从其他的web服务上取token而是get_token_from_local_cache()的话，虽然也还是执行了多余的操作，但是就好得多了。 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:1:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"查了完全没卵用的东西 queryset = Student.objects.filter(age__gte=20, gender='F') for student in queryset: send_mail(studnet.email, '一个标题', 'hello,world') 这部分代码看起来问题不大，但是假设我们有10W的学生，并且Student表有大几十个字段，那到for student in queryset的时候，就会卡住一会儿（如果机器不太行的话可能会卡很久）。其实我们知道，默认这样的查询是SELECT * FROM student WHERE xxxx来的，把所有数据都取出来了。如果我们稍加改动 queryset = Student.objects.filter(age__gte=20, gender='F').only('email') for student in queryset: send_mail(studnet.email, '一个标题', 'hello,world') 仔细看，其实就只是在filter()后面加了only('email')，这就相当于是SELECT email FROM student WHERE xxxxx了，效率明显高了好多。或者直接改成 email_list = Student.objects.filter(age__gte=20, gender='F').values_list('email', flat=True) for email in email_list: send_mail(email, '一个标题', 'hello,world') 这样返回来的queryset里的元素就是email了。 0X03 数据库瓶颈（MySQL） **数据库一定要多占内存，数据库一定要多占内存，数据库一定要多占内存。**一般来说，在Linux系统下是用多少内存分配多少，但是我们MySQL通常都是独立部署的，有且只有一个MySQL，所以就直接一次性给MySQL分配够内存，这是最好的方法。记得内存就是买来用的，买内存回来结果一年到头都是30%的占用，那岂不是亏了吗哈哈哈哈哈🤓 数据库机器不要开swap，数据库机器不要开swap，数据库机器不要开swap 。数据库的机器内存不够了，就加内存或者优化查询，万万不可使用交换分区。只要你的数据库机器一开swap分区，再结合上面的原则，就意味着瞬间爆炸 。因为内存一直都是几乎占满的情况，你一打开交换分区，Linux就会疯狂开始用内存和硬盘进行交换，本来你MySQL里有很多东西放内存里就是图个快的，结果又给在swap的机制下放回磁盘了，再折腾一圈下来甚至比直接在磁盘里还要慢。 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:2:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"索引问题 因为把优化查询改成了优化ORM，所以也就理所当然放到上一个段落里了，那这里就只剩下索引为题了。索引简单来说就是：“针对某些字段，牺牲内存和写入速度换取查询速度”。所以说如果有些字段你很少改，甚至写进去就不会再变了，然后又要疯狂的以它为条件查询，那就给他加个索引。不过加索引之前有几个注意的点： 这个字段一定是读取频率远高于写入频率的； 这个字段的\"唯一性\"要高，比如学生表的身份证号这种，每条数据都有不同的身份证号； 这个字段要在SQL中的WHERE子句后面，而不是SELECT后面。也就是说：应该是条件，而非需要得到的数据； 具体的可以看我的[另一篇博客]](https://blog.just666.com/2019/09/15/database-index/)。 0X04 机器瓶颈（Linux） 机器瓶颈，这里给几种简单的排查、结解决方法，主要还是得靠运维同事了。 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:3:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"htop htop就是我们上面说到的看CPU/内存/进程和负载的工具，便于你找到疯狂消耗CPU或者内存的程序。 这个htop排序那里是可以用鼠标点的喔 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:4:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"swap 如果你机器内存爆炸💥了，还不能第一时间加内存上去，那就只有先用交换分区缓一下。swapon和swapoff两个命令可以帮到你，具体的可以搜索一下，是可以在不停机的情况下加入新的交换分区和关闭交换分区。 一个奇技淫巧：可以使用dd命令搞一个块文件，然后格式化成swap格式，最后挂成交换分区喔。 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:5:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"df/du df可以看到当前挂载的磁盘，哪些快要满了。du可以方便得看目录的大小，还有一个ncdu是du的进阶版，是一个类似图形化的界面，用起来更舒服。能确认到哪个目录占用的空间多，然后指向性得清理一些数据。有时候磁盘满了都不知道是什么东西占了空间，这时候du和ncdu就很好用了。 ","date":"2020-05-13","objectID":"/posts/django-api-faster/:6:0","tags":["Django","Python"],"title":"如何让 Django API 再快一点","uri":"/posts/django-api-faster/"},{"categories":null,"content":"0X00 为什么要记读书笔记 本来我也是一个从来不记读书笔记的人，总觉得张无忌师傅的那个\"都忘了吗？忘了就去吧\"的神奇教诲很有道理，我也是一直都是看书看多少算多少，记住的就记住了，忘了也就忘了。即使这次开始记笔记也不是因为我要把看过的内容都记住，这完全不可能。 那到底是什么原因让我开始记读书笔记了呢？“走神”。没错，就是走神。我看书的时候，尤其是自己没有那么大兴趣的书，我经常机械的读着书然后思路就飞到不知道哪里去了，可能是其他相关的知识、可能是海拉鲁大陆、可能是九界湖、也可能幻想着自己去悬崖边上的麦田里抓那些快要掉到悬崖下的小孩儿了。 所以为了让自己不再继续走神下去，我决定做点什么。最开始的时候是尝试把自己看到的一切内容都真正的\"读\"出来，读出声来，但是后来发现这种方式不太适合我，每次一本正经得把书里的内容一个字一个字念出来都觉得很奇怪，像是小学生在上语文课一样。放弃读出声来的操作之后就想到了记笔记，目前看起来这种方案还是不错的。 0X01 我平时怎么记 我这里给出的方案只适用于跟我类似的朋友，如果你是认认真真读书然后想要整理大纲呀整理知识点呀之类的，那我的方案应该是帮不到你（不过也可以看看，万一呢）。我介绍的方案是我自己用过的，也就两种，很简单的。 首先是实体笔记，也就是像在学校里那样，左右各一本，一边看一边记；目前我是用iPad来统筹了看书和记笔记，也就放下了纸质笔记本和用了好几年的kindle（现在他可以去做他的专职工作：泡面 了）。我自己用电脑的时候不管显示器多大我都很少分屏，但是到了iPad上开个分屏一边是Kindle app一边是godnotes，效率还是挺高的（这里庆幸一下之前纠结过后还是买了iPad Air，要是当时选择了iPad mini的话分屏记笔记怕不是记笔记得累死个人）。这张图就是我平时记笔记的时候（不要在意好不好看。什么？你说看不懂？我的笔记是给自己看的，你看不看得懂问题不大🤣） 当你准备好所有设备了，记得要做几件事儿： 拿出手机，打开\"禅定模式\"或者\"冥想模式\"或者什么鬼的，反正就是那种类似直接锁定你手机，让你什么都干不了的模式； 如果是用iPad看书，打开你的\"勿扰模式\"； 去一个安静的地方，桌子上的东西尽可能的少，最好只有书、笔记本和一支笔； 让你的小爱同学或者Siri或者谁在一个小时后叫你； 我平时记笔记怎么说呢。。。叫\"意识流\"吧。凡事都可以类比成软件开发，我最了解自己的需求，所以我知道怎么做才对。正因为我最清楚自己的需求\"通过摘抄书上的重要内容和记录自己的想法来强制自己不走神\"，所以我从来不在乎自己的笔记是不是知识点完整，是不是排版方便二次阅读，是不是便于后期查找。 所以说我记笔记的原则很简单： 遇到书上写的\"非常有道理，非常值得摘抄\"的内容，抄下来，适当再加符号加重一下； 遇到书上提出的问题，比如\"仔细想想你身边的专家是如何解决工作中他人的打断的\"，我就会把我想到的答案写上去。因为一旦我不需要输出内容的时候，就有可能偷懒不去思考了； 遇到逻辑关系复杂的内容，画个图整理； 看到不认同的观点，将书上的观点和我自己的观点都写上去，并且标个问号； 大概就是这样，整体来说非常简单，并不是什么\"正经\"的读书笔记记录法，不过这种方式对我来说非常奏效。 0X03 资源和工具 先从资源说起吧，目前我的书主要分两种：京东和淘宝买的纸质书、亚马逊买的电子书。以前用Kindle的时候只买与专业技术无关或者关系甚小的书，因为Kindle确实太小了而且前后翻页也很不方便。现在用iPad Air替换Kindle之后明显买电子书的意愿比原来更大了，现在出了特别硬核的极其特殊的书，我都买电子版了。（经历了搬家和大刀阔斧整理房间之后，才发现纸质书真的太重太占地方了） 工具方面我在iPad上几乎只用这两个：Kindle app和Goodnotes。其中Kindle是因为电子书市场的书比较多，虽然现在好多电子书平台，不过他们市场里的书都比较少，主要还是以畅销书为主。Goodnotes是用来看PDF和记笔记用的，Notability也买了，不过不太用的惯。另外的工具其实也有，比如用来画思维导图的MindNode，在电脑上画好直接传到iPad上粘贴到Goodnotes，体验很棒。目前用着不舒服的一点就是Kindle app好像并不能把图片拖出来，在iPad上左右分屏的时候可以轻松将Safari里的图长按拖动到Goodnotes里，其他好多程序也都支持，但是Kindle是不可以的。不知道是不是版权的问题有限制，搞得我每次都是切过去截图然后把截图剪出来粘贴上去，幸亏这种需求不算频繁。 硬件工具除了iPad以外就是Apple Pencil和一张膜了。Apple Pencil跟iPad一起记笔记真的还挺舒服的，但是最好给iPad贴一张\"类纸膜\"，要不然iPad的屏幕太光滑了，写字总是不太舒服。 0X02 真的有用吗 有没有用还真不好说，反正我自己的综合体验来说有这么几点 看书的效率明显下降，因为要专心，还要做笔记。估计是之前的一半左右； 即使是自己看过了不太记得的地方，回头乱翻翻自己乱记的笔记也能回忆起来不少东西； 以前看了实体书，厚厚一本就会有成就感，后面看了电子书就没有那么大成就感了。现在开始记笔记之后，看看自己的笔记（即使是电子笔记）成就感又回来了 如果各位也有跟我类似的困扰，那也可以试试我的方法。不管用不收钱，管用也不收钱🤩 ","date":"2020-05-05","objectID":"/posts/reading-note/:0:0","tags":["Life"],"title":"读书笔记?","uri":"/posts/reading-note/"},{"categories":null,"content":"0X00 前言 Linux诡异的权限是怎么回事呢？Linux相信大家都很熟悉， 但是诡异的权限是怎么回事呢？下面就让小编带大家一起了解吧。 Linux诡异的权限，其实就是诡异的权限了。那么Linux为什么会诡异的权限，相信大家都很好奇是怎么回事。大家可能会感到很惊讶，Linux怎么会诡异的权限呢？但事实就是这样，小编也感到非常惊讶。 那么这就是关于Linux诡异的权限的事情了，大家有没有觉得很神奇呢？ 看了今天的内容，大家有什么想法呢？欢迎在评论区告诉小编一起讨论哦。 说正事说正事儿。说起Linux权限大家肯定：“这我知道啊，不就是rwx吗，r是读、w是写、x是执行。就这？“当然不只是这个，不过我们还是要从最基础的开始说起来。 0X01 基础权限部分 首先最基础的权限就是 rwx 这种，三组权限针对:所 属用户、所属用户的组、其他用户，每组3位(对应 二进制位)。正因为对应二进制位所以rwx就是三个 二进制位均为1的7;r-x就是对应的101也就是5;r- 就是100也就是4 最基础的rwx权限就不多说了，说一个不是所有人都 知道的，看下面这张截图:这个叫做linux.pdf的文 件，这个文件的权限是777，但是当我们试图删除它的时候，发现完全不能行，那是为什么呢？我们很自然的认为对一个文件有rwx的权限就是有所有权限了，其实这么理解问题不大。但是考虑一个问题，删除一个 目录里的文件，实际上是不是在对这个目录进行w操 作呢? 返回来再看这个目录的权限就明白了。是的，基于Linux中\"万物皆文件\"的思路，可以知道目录其实也是文件，所以删除目录里的文件就是在修改这个目录，进而得到结论：删除文件是需要拥有对文件所在目录的w权限才行的。 0X02 ACL 现在再来看另一个问题:我们看这个叫macOS.txt的文件，又是一个777权限的文件。按照上面提到的内容，我们就算不能删了它起码也能给它写成空文件是吧，因为毕竟有w权限。但是你真的有这个文件的w权 限吗?当你尝试给这个文件写入内容的时候直接就报错了，完全没有权限。是的，也许机智的你注意到了，问题就出现在 -rwxrwxrwx+ 中最后的+那里。 实际上是因为Linux上还有一个叫做ACL的机制。系统用户肯定不能单纯通过用户和组来完成的，正是通过 ACL的这个机制可以在传统的rwx权限的基础上进行扩 展。可以使用ACL在rwx之外给单独一个/多个用户/组 指定权限。比如下面这种用法:setfacl -m u:shawn:--- macOS.txt拆分开看这个命令，第一个参数 -m指 的是(modify)，后面的u:shawn:---就是说 用户: shawn:三无权限 。再使用getfacl看一下文件具体权限，可以看到，所属人和组都是root，用户、组和其 他人的权限都是777，但是只有一个user:shawn:---，这个就意味着只有这个用户是没有权限的。 0X03 隐藏属性 我们来看一下这个DELETE_ME的文件，我们仍然还是有777的权限，也没有通过ACL限制单个用户的权限，而且当前目录我也有w权限。那我们来尝试删除或者重写一下内容好了，发现还是还是还是还是没有权限。。。。那这回又是为什么呢？ 是的，又是一个奇怪的东⻄:attr我们可以使用lsattr filename来查看当前文件的隐藏属性，有很多，这里可以看到的是i和e。其中e是系统底层的自带的， 有兴趣的话可以自己查阅一下资料。刚刚文件删不掉 改不了是因为这个i。这个i的功能就是: immutable 不可改变的 。所以我们不能删除，也不能修改内容。 使用chattr -I filename就可以将这个标记删除了，然 后这个文件也就可以改动了。如果需要增加这个标记 的话是chattr +I filename也就是说用加减号来控制隐 藏的标记。也可以使用等号chattr =I filename的方式来直接重写所有的标记。 关于这个attr的所有参数内容都在这里 0X04 奇怪的权限知识增加了 到此为止这些隐藏在最基础的rwx权限之外的奇怪的权限（并不全，其实还有其他的）就说完了，希望大家能有所收获～ 备注1:有些Linux发行版本没有默认附带这个，我展 示是用的Fedora所以要通过dnf装一下这个功能。 备注2:除了rwx的基础权限部分以外，ACL和attr两 部分内容是仅适用于Linux发行版本的，macOS并不 适用，如果需要学习或者测试的话要搞一台Linux才 行。 备注3:ACL和attr的相关资料: https://man.linuxde.net/setfacl https://man.linuxde.net/lsattr https://man.linuxde.net/chattr https://linux.die.net/man/1/chattr ","date":"2020-04-23","objectID":"/posts/linux-permission-simple/:0:0","tags":["Linux","Permission"],"title":"Linux中诡异的权限（奇怪的权限增加了）","uri":"/posts/linux-permission-simple/"},{"categories":null,"content":"0X00 视频在这里 下面是配置文件 这篇博客要配合发在bilibili的视频来看，这个文件是在~/.zshrc的。大家有问题直接在视频下面留言或者直接给我私信好了～ # system env export ZSH=\"/Users/shawn/.oh-my-zsh\" export LANGUAGE=en_US export LANG=en_US.UTF-8 # ZSH_THEME=\"agnoster\" ZSH_THEME=\"powerlevel10k/powerlevel10k\" EDITOR=/usr/bin/vim PATH=$PATH:$HOME/Library/Python/3.7/bin PATH=$PATH:$HOME/Library/Python/2.7/bin HIST_STAMPS=\"yyyy-mm-dd\" HISTFILESIZE=100000 HISTFILE=~/.zsh_history # zsh plugin plugins=( z git docker fabric extract thefuck fzf-zsh git-open colored-man-pages zsh-autosuggestions zsh-syntax-highlighting ) # alias for simple command alias py2='/Users/shawn/Library/Python/2.7/bin/ipython2' alias py='/Users/shawn/Library/Python/3.7/bin/ipython3' alias cat='/usr/local/bin/bat' alias down='aria2c -x16 -j4' alias me=\"cd $HOME/Workstadion/ \u0026\u0026 ls\" # alias to source command alias _cat='/bin/cat' # ctrl + n autosuggest bindkey '^n' autosuggest-accept source $ZSH/oh-my-zsh.sh # docker attach() { docker exec -it `docker ps | grep $* | awk -F ' ' '{print $1}'` bash } attach_django() { docker exec -it `docker ps | grep $* | awk -F ' ' '{print $1}'` python manage.py shell } git_set_proxy() { git config --global http.proxy 'socks5://127.0.0.1:1080' git config --global https.proxy 'socks5://127.0.0.1:1080' } git_unset_proxy() { git config --global --unset http.proxy git config --global --unset https.proxy } json() { # echo `xclip -o` | jq # Linux echo `pbpaste` | jq # macOS } # To customize prompt, run `p10k configure` or edit ~/.p10k.zsh. [[ ! -f ~/.p10k.zsh ]] || source ~/.p10k.zsh ","date":"2020-04-18","objectID":"/posts/better-terminal/:0:0","tags":["Linux","macOS","Terminal","Shell"],"title":"如何让自己的 Linux/macOS 终端更好用","uri":"/posts/better-terminal/"},{"categories":null,"content":"0X00 前言 2020年要来了，听起来这是个多科幻的年号啊。《银翼杀手》里说2019年底人类就有强人工智能了，可以穿梭宇宙了，甚至可以星际殖民了。然而现在就是2019年最后一天了，我的小爱同学还是像智障一样经常听不懂我说话。 换个角度想一想，90后是不是会觉得60后已经算是老年人了；那么相同时间差算下来20后也会觉得我们90后是老人，然而第一个20后还有一个多小时就出生了呢。 0X01 2019年度分析 2019年是我第一次做整年的规划，大概进度是这样的。正所谓“取法其上，仅得其中；取法其中，仅得其下”，所以给自己的目标定高一些甚至超出自己的能力也没什么不好的，这样可以让自己完成目标的动力最大化的嘛（其实也算是给自己没完成目标找一个借口，哈哈哈）。 阅读20本书【完成】 专业进阶（学习了解新技术）【5/7】 了解五个新领域【4/5】 撰写30篇博客（包含20篇技术类）【27/30】 观看50部电影（包含25部豆瓣top250）【47/50】 减重至75KG（共减10KG）【5/10】 因为我是一个比较喜欢对自己生活做计划的人，之前我没有计划的时候就自己趴在床上玩手机甚至都能玩一下午。所以今年年初的时候就给自己定了一系列的目标，试着打卡式过这个2019，结果发现效果还是挺好的。 其中30篇博客我觉得是可以推荐给同样写博客的同学的，其实一年下来写30篇博客而且又不都是技术类的并没有很难。这样的话不仅可以让自己的博客更饱满还更可以逼迫自己学习，大家一定都会有过那种自己觉得看了几篇博客文档又写了几行代码验证就觉得自己回了的时候吧。其实这种时候如果计划把这部分整理成博客的话，就可以大幅度提升自己的学习效果，毕竟自己也不可能乱写一通就发表了。想必也都知道那句“教别人的同时也非常能提升自己”，其实这里的道理是一样的。我每次写博客都是想尽可能把事情说清楚说明白，如果过程中发现自己说不明白那就只有继续研究这个东西，直到能解释给别人为止。所以说程序员自己写博客传授自己的知识和经验几乎就是百利而无一害的。 观看25部豆瓣top250的电影也非常棒的，再结合前面20本书里包含的那本《认识电影》ISBN: 9787506287081书，感觉近一年来对电影越来越了解了，更多的会去看电影里导演想要表达的东西而不是简简单单的故事了。这本书也非常推荐给喜欢看电影的同学，里面从多个角度介绍了电影的基础知识，对理解电影有非常棒的帮助。 了解新领域就更有意思了。现在网络这么发达（听起来好象是2005年的台词啊），几乎是想要了解的任何一个领域都会有对应领域的大佬做科普，甚至还会有国际一流院校推出的MOOC可以学。想要了解一个新领域的成本越来越低了，没有什么是搜索引擎解决不了的（什么 你说百度？打扰了），尤其是对我这种好奇宝宝（宝宝？）来说简直太棒了。 0X02 2019值得纪念 今年有什么特殊的事情发生吗？让我仔细想想……好多都比较私人，说几个不那么私人的吧。 年初的时候考了红十字会的急救员证书，真希望永远也用不到这个东西； 换了一台4核8线程32G内存的电脑（这也算很值得纪念了哈哈哈哈）； 跟去年比起来，今年真正属于自己的时间多了不少，也就做了不少饭，厨艺精进哈哈； 我成UP主了，当时买电脑发现网上Thinkpad T480的评测很少，获取不到重要信息，就自己录了一个，现在使用体验和拆机视频一共播放量都有六万了； 因为《脱口秀大会》的缘故，去看了本地的一场线下脱口秀，虽然当时没抱太大期望但是结果却意外的好，大家有兴趣的可以试试线下脱口秀； 卖掉PS4换了一台Switch（不是PS4不好，是最近想玩的PS4游戏都玩过了，正好玩Switch等明年的PS5） 0X03 2019看到的好电影 这里给大家推荐一些今年看过的电影，都是我觉得非常棒的，豆瓣top250里的就不算了哈。 《利刃出鞘》：这是一部悬疑电影，虽然号称007大战美国队长，但实际上并没有动作戏。我觉得悬疑篇最重要的两点：一个是主角和观众的信息对等，不要出现主角知道的一些事情没有告诉观众的情况，比如我们还在想这个杀人凶手是怎么逃离人群的时候，结果其实人家会隐身？？第二个是让观众一直循环在”我是个天才-我是个智障-我是个天才-我是个智障“之间。 《双子杀手》：这个电影说实话剧情上不算多好，不过由于是李安指导的120fps电影，机会可是不多啊。就赶紧买了杜比影院的票去体验2K 120FPS的杜比影院技术了。真的太棒太棒了，120FPS使得动作极为流畅，维尔史密斯的每个动作都清晰且流畅；杜比影院的超高对比度使得一场暗处动作戏相当好看。一场电影看下来真的是意犹未尽的感觉，甚至有着不输给复仇者联盟的视觉震撼。 《流浪地球》：这个电影其实是没什么问题的，非常棒。虽然不是顶级制作但是也没有一些人嘴里的那么不堪。电影总体来说即使去掉所谓情怀和国产的tag也是一部非常不错的电影呢。 《复仇者联盟4》：这个电影基本就是一个字“爽”。就像大佬说的这叫 movie 而不是 cinema ，不过这也不是什么贬义啦。这也是我为数不多二刷的电影，确实很好看。 《徒手攀岩》：这部电影出乎意外的好看。我以为会很无聊，结果全程双手握紧直出冷汗。喜欢看纪录片的同学强烈推荐这部记录片。 《中国机长》：跟上面的徒手攀岩一样，我以为会很无聊，结果全镇双手紧握直出冷汗。（还真就一样，完全一样哈哈哈哈哈）都说看了这部电影就不敢坐飞机了，我反而觉得有这样的机长坐镇我更敢坐飞机了呢。 0X04 2019玩到的好游戏 要说今年玩到过的好玩的游戏，如果直让我选一个，那就是《战神》。虽然这个游戏是去年出的，但是也太棒了吧！！！ 故事非常棒，整个算下来大概就是小战神和奎爷打算把小战神母亲的骨灰扬了。（？？？哈哈哈） 认真认真，这里说一个我最喜欢的点。游戏的越肩视角一镜到底是真的厉害，完全不会打断游戏流程。假设玩家永远不挂的话，那就可以保持整个镜头从头到尾不动的。为了满足这个点就设计出了我在游戏里最喜欢最喜欢的一个小细节：传送。一般游戏传送就两种，选择目的地后切换界面读条，再切换载入；要么就是上车上马然后一路自动跑过去要等很久；但是战神里的处理简直太棒了！游戏里有固定的传送点，是一堆散落在地上的石头，奎爷靠近的时候石头就自己围起来变成门，中间是白色的像是传送门一样；然后奎爷就进入到白色的空间里，里面是一个类似迷宫的小地图，在里面跑哇跑哇跑哇，突然前面又刷新出了一个白色的门，出去就到了目的地。其实类似迷宫的那个小地图八成是一直在内存里的（或者奎爷离传送门近的时候开始加载的），所以我们进入小地图的过程是无缝的；从小地图里跑来跑去自己以为是跑向传送门，其实是游戏正在加载目的地的资源，等加载好了就在你面前放一个传送门，直接出去无缝衔接。这个设计真的是让我惊叹不已。 0X05 总结 总的来说2019年过得还是挺好的，希望2020年过得更好吧~~~ 大家新年快乐!!! ","date":"2019-12-31","objectID":"/posts/2019-summary/:0:0","tags":["Summary"],"title":"2019 年度总结","uri":"/posts/2019-summary/"},{"categories":null,"content":"0X00 使用docker部署RabbitMQ 自从用起docker之后，每次在自己本地开发环境部署新服务就首选用docker了。虽然理论上docker跟裸机部署比起来多多少少有一些缺点，但是跟3分钟部署几乎一些开发环境服务的优势比起来简直都是毛毛雨了。 首先要拉个镜像下来，通常拉镜像都是选择最新的或者特定某个版本，但是RabbitMQ有一点比较奇怪，如果逆向拉带有web管理页面的就不能用latest，而应该选择management。然后确定好镜像之后再了解一下端口情况，RabbitMQ带有web管理页面的话会用到两个端口：提供MQ服务的5672和提供web服务的15672。 下面是我的配置文件，把内容保存为docker-compose.yml然后docker-compose up -d就好了（如果不在yml文件所在目录下执行或者文件名不叫docker-compose.yml的话要用docker-compose -f xxx/xxx/xxx/xxx.yml指定配置文件的位置。 version: '3' services: rabbitmq: # image: rabbitmq:latest # 如果要用不带web界面的可以选这个 image: rabbitmq:management # 带有web界面的镜像 container_name: rabbitmq # 取一个容器名 ports: # 开放两个端口，当然没有web界面的话就不用开放15672了 - \"5672:5672\" - \"15672:15672\" environment: # 这里设置登录名和密码 RABBITMQ_DEFAULT_USER: shawn RABBITMQ_DEFAULT_PASS: **************** # 高科技加密（骗你的，我自己打的星号 这样以来服务就启动起来了，可以访问http://127.0.0.1:15672看到RabbitMQ的web登录页面了。 docker-compose并不是docker的一部分，而是一个用Python编写的docker编排工具。如果电脑上的话可以使用pip install docker-compose来安装它 0X01 使用Python调用RabbitMQ RabbitMQ的官方文档上有一个非常简单明了的介绍如何使用Python接入RabbitMQ。有两坨代码，一坨是sender另一坨是receiver，首先是sender： #!/usr/bin/env python import pika # 建立和RabbitMQ的连接 credentials = pika.PlainCredentials('shawn', '********') # 两个参数：用户名和密码 connection = pika.BlockingConnection( pika.ConnectionParameters('localhost', 5672, '/', credentials) # 四个参数：机器、端口、虚拟主机（新手先不管它）、认证信息 ) channel = connection.channel() # 选择使用一个队列 channel.queue_declare(queue='hello') # 发送一个消息 channel.basic_publish(exchange='', routing_key='hello', body='Hello World!') print(\" [x] Sent 'Hello World!'\") # 断开连接 connection.close() 然后是差不太多的receiver的这一坨 #!/usr/bin/env python import pika # 完全相同的建立连接 credentials = pika.PlainCredentials('shawn', '********') connection = pika.BlockingConnection( pika.ConnectionParameters('localhost', 5672, '/', credentials) ) channel = connection.channel() # 完全相同的选择使用一个队列 channel.queue_declare(queue='hello') # 创建一个回调方法 def callback(ch, method, properties, body): print(\" [x] Received %r\" % body) # 从'hello'队列来的消息交给`callback`方法处理 channel.basic_consume(queue='hello', on_message_callback=callback, auto_ack=True) # 开始等待消息 print(' [*] Waiting for messages. To exit press CTRL+C') channel.start_consuming() 现在就可以用python sender.py把消息塞到RabbitMQ中，再用python receiver.py拿到RabbitMQ队列中的消息了。 0X02 使用Django调用RabbitMQ（Celery） Django在生产环境中经常需要Celery的支持，在Django中使用Celery主要是为了两大特性：定时任务和异步任务。如果够骚的话定时任务可以通过Linux的crontab来替代，但是异步任务目前还不太好离开Celery。通常部署Celery的时候后端都是Redis，这次可以尝试一下使用RabbitMQ（Celery默认就是支持使用RabbitMQ这类MQ的）。在正常配置了Redis作为后端的情况下切换到RabbitMQ其实是不麻烦的：唯一要做的就是将本来的BROKER_URL改成BROKER_URL='amqp://shawn:********@localhost:5672//'就可以了。 相比Redis，RabbitMQ自带web界面，可以方便的查看后台任务；而且作为broker来说性能更强劲。可能这也是Celery官方建议使用RabbitMQ的原因吧。 Django3其实已经开始支持异步了，但等到大规模高质量应用可能还需要一段时间 ","date":"2019-11-21","objectID":"/posts/django-rabbitmq/:0:0","tags":["Django","Python","RabbitMQ"],"title":"在 Django 中使用 RabbitMQ","uri":"/posts/django-rabbitmq/"},{"categories":null,"content":"0X00 url的源头 使用django-admin startproject test_project创建一个新的Django项目之后在settings.py中可以找到一个配置项ROOT_URLCONF，默认情况下值为项目目录下的urls，也就是test_project.urls。 默认情况下这个urls.py的内容大致是这样的 \"\"\"learn_django URL Configuration The `urlpatterns` list routes URLs to views. For more information please see: https://docs.djangoproject.com/en/2.2/topics/http/urls/ Examples: Function views 1. Add an import: from my_app import views 2. Add a URL to urlpatterns: path('', views.home, name='home') Class-based views 1. Add an import: from other_app.views import Home 2. Add a URL to urlpatterns: path('', Home.as_view(), name='home') Including another URLconf 1. Import the include() function: from django.urls import include, path 2. Add a URL to urlpatterns: path('blog/', include('blog.urls')) \"\"\" from django.contrib import admin from django.urls import path urlpatterns = [ path('admin/', admin.site.urls), ] 这里就是根目录了，新项目使用python manage.py runserver启动之后访问http://127.0.0.1:8000/就是访问到这个url的根路由了，默认情况下有一个admin/可选，也就是Django自己的后台管理页面。Django所有的url都是从这个文件发散出去的 ，urlpatterns里除了将url路由至view就是其他的子url配置。换句话说，通常情况下Django中所有url最终都应该被路由到View上才对。 0X01 路由到子url和view 上面提到通常情况下Django中所有url最终都应该被路由到View上，那就来看一下究竟该怎么做。现在有一个项目，项目中有一个app叫student是用来管理一些学生信息的，app中有一个views.py，具体内容就暂时不列出了，在此处关系不大；还有一个urls.py，内容是这样的 from django.urls import path from .views import StudentView, ExamView urlpatterns = [ path('^$', StudentView.as_view()), # 直接将这个子url的根目录路由到一个view path('^exam/$', ExamView.as_view()), # 直接将url路由至一个view ] 现在要来修改根的urls.py了 from django.contrib import admin from django.urls import path, include urlpatterns = [ path('admin/', admin.site.urls), path('student/', include('test_project.student.urls')), # 其实就加了这一行，这里的include就是包含另一个urls的配置文件 ] 现在就是这样的，如果访问http://127.0.0.1:8000/admin还是之前的DjangoAdmin默认管理页面没问题，如果访问http://127.0.0.1:8000/student/的话就是到StudentView而http://127.0.0.1:8000/student/exam/就是ExamView了。 0X02 参数与正则 url中常见的出现一些参数，一般来说参数分成两种：第一种是在path中作为路由的一部分，另一种是在后面以GET的查询参数方式出现例如/student?name=shawn\u0026age__gt=16这种。第二种方式比较简单，在传入到view后，从view的request.GET就能取到了。但是也经常会遇到第一种，例如这样一个path/article/2019/11/20/why-linux，可以猜测它指的是2019年11月20日的一篇名为’why-linux’的文章。那么这种该怎么取呢？其实很简单，在view里request.path.split('/')然后取下标就行了（当然这很蠢且很不靠谱，但是还不失为一种方案哈哈哈哈）。 这种时候比较靠谱的方式是使用url中的参数，有一个子url配置如下 urlpatterns = [ path('\u003cint:year\u003e/\u003cint:month\u003e/\u003cint:day\u003e/\u003cstr:name\u003e/', StudentView.as_view()), ] 其中\u003cint:year\u003e就是指的一个参数，前三个是整型参数，最后一个是字符串参数。我们知道view中GET方法的定义是def get(self, request, *args, **kwargs)，那么其实后面的这个**kwargs里就是这里传进来的参数了，可以通过year, month, day, name = kwargs['year'], kwargs['month'], kwargs['day'], kwargs['name']这种类似的方式来取到对应的值 还是上面这个例子，咱们知道年号一定是正整数，月份一定是112之间，日期一定是131之间（先不考虑闰年和大小月的问题，只是方便探讨url）。那上面这个例子中的url如果我传一个/article/0/666/233/test/过去其实是没有意义的，所以需要一些简单的校验。那么众所周知，正则表达式非常适合做这种事情。下面来修改一下刚刚的这个url配置好了，修改后的配置Django就可以根据正则来匹配了。 from django.urls import re_path urlpatterns = [ path(r'^[1-9]\\d*/0[1-9]|1[0-2]/xxxxx', StudentView.as_view()), # 完整正则好长，就不都贴在这儿了 ] Django在匹配的时候是按照urlpatterns这个列表的下标顺序来的，所一说如果先符合了上面的规则，即时再符合下面的规则也不会继续判断下去了。 0X03 DRF中的router.register 一般使用Django的同时也会使用Django REST framework 了，所以也简单介绍一下在DRF中特有的一种路由方式好了。因为DRF中大量使用ViewSet而非标准的Django View，所以可以使用DRF封装的下面这种方式来建立路由 from rest_framework import routers from . import views router = routers.DefaultRouter() # 实例化一个router router.register(r'student/', views.xxxxxxxxViewSet) # 注册viewset router.register(r'teacher/', views.xxxxxxxxViewSet) urlpatterns = router.urls # 最后还是要生成urlpatterns 虽说是叫做router不过翻译成路由器总是有点怪怪的，哈哈哈 ","date":"2019-11-20","objectID":"/posts/django-url/:0:0","tags":["Django","Python"],"title":"Django 中的 url","uri":"/posts/django-url/"},{"categories":null,"content":"0X00 Python3的super Python中对象的概念都快被大家淡忘了，因为一切都是对象（话虽然这么说，但是怎么可能淡忘对象呢）。看下面一段Python2的代码，Python2中麻烦的就是这个super()的用法。 class Human: def __init__(self): self.name = 'human' print 'hello, i im', self.name class Student(Human): def __init__(self): super(Student, self).__init__() self.name = 'student' print 'hello, i im', self.name a = Student() 在初学Python的时候，如果是Python2很大可能会在super(Student, self).__init__()这段迷惑好一阵子，不过好在Python2马上就要凉透了，在Python3中可以将代码改写成如下方式 class Human: # 不用强行继承自object了 def __init__(self): self.name = 'human' print('hello, i im', self.name) class Student(Human): def __init__(self): super().__init__() # super的用法也更明了 self.name = 'student' print('hello, i im', self.name) a = Student() 其中super的用法由super(Student, self).__init__()改成了super().__init__()，看起来清晰多了，在使用Python3后不建议以任何理由使用老式Python中的super调用。 0X01 str 写一个自己的类通常都需要实现一个__str__方法，这个方法用于粗略的展示对象，可以看下面这个例子。 class Student: def __init__(self, name, age, gender): self.name = name self.age = age self.gender = gender # 下面这一堆xxx表示其他很多属性 # self.xxxx = xxxx # self.xxxx = xxxx # self.xxxx = xxxx # self.xxxx = xxxx # self.xxxx = xxxx # self.xxxx = xxxx # self.xxxx = xxxx def __str__(self): return 'name:{} age:{} gender:{}'.format(self.name, self.age, self.gender) a = Student('shawn', '24', 'm') b = Student('lucy', '24', 'f') c = Student('bill', '24', 'm') print(a) print(b) print(c) 可以尝试先把__str__的定义注释掉执行一下，看到的输出应该是类似这样的： \u003c__main__.Student object at 0x7f875f881d10\u003e \u003c__main__.Student object at 0x7f875f881d90\u003e \u003c__main__.Student object at 0x7f875f881e10\u003e 如果再取消__str__的注释，看到的输出就是这样的了： name:shawn age:24 gender:m name:lucy age:24 gender:f name:bill age:24 gender:m 可以看到输出变成肉眼可识别的了。 通过 str(object) 以及内置函数 format() 和 print() 调用以生成一个对象的“非正式”或格式良好的字符串表示。返回值必须为一个 字符串 对象。 此方法与 object.repr() 的不同点在于 str() 并不预期返回一个有效的 Python 表达式：可以使用更方便或更准确的描述信息。 内置类型 object 所定义的默认实现会调用 object.repr()。 官方文档 0X02 repr __str__其实很多人都是知道的，毕竟这也算是Python中最基础的部分之一了，不过这里的__repr__貌似就有些同学不太清楚了。__repr__的功能和__str__是类似的，不过__str__输出的结果是方便肉眼识别的，而__repr__输出的结果是”可以通过输出反向还原对象“的，换句话说就是带有对象的详尽信息。 a = {'a': 1, 'b': 2, 'c': 3} print(repr(a)) b = {'a': a, 'b': a, 'c': a} print(repr(a)) 执行上面这坨代码就理解这个方法的基本情况了，注：repr(a)算是a.__repr__()的语法糖了，效果相同。 由 repr() 内置函数调用以输出一个对象的“官方”字符串表示。如果可能，这应类似一个有效的 Python 表达式，能被用来重建具有相同取值的对象（只要有适当的环境）。如果这不可能，则应返回形式如 \u003c…some useful description…\u003e 的字符串。返回值必须是一个字符串对象。如果一个类定义了 repr() 但未定义 str()，则在需要该类的实例的“非正式”字符串表示时也会使用 repr()。 此方法通常被用于调试，因此确保其表示的内容包含丰富信息且无歧义是很重要的。 官方文档 ","date":"2019-11-19","objectID":"/posts/python-oop-2/:0:0","tags":["Python"],"title":"Python之面向对象 2","uri":"/posts/python-oop-2/"},{"categories":null,"content":"0X00 使用docker部署的优势 在使用docker部署之前，一般都是直接将MySQL和Redis这类服务直接安装在机器上的。以至于好多新手才开始安装使用的时候经常会出问题，出了问题解决不了就重装系统然后再重装软件，而且如果想同时用MySQL5和MySQL8就非常麻烦了。话说回来，在生产环境服务器上其实还是很多直装的服务的，不过其实使用docker部署一套相同的环境是非常有利于自己本地开发的。 我个人看来使用docker部署的优势有这几点比较明显的： 很容易做到开发环境、测试环境和生产环境的“环境与版本”大统一； 很容易在开发环境本地同时部署多套不同版本的同一服务；（比如你负责8个项目，这8个项目要用8个不同版本的MySQL） 能很快部署一套开发环境；（事实证明在一台网络环境好的机器上，能在5分钟内部署一套数据库） 整理好配置文件后可以很容易备份整套配置； 安全问题，在物理机上直接装了MySQL，如果部署不够仔细的话有数据库被攻破后危及服务器的风险，而Docker部署的由于容器所在就不会有这种问题； 0X01 使用docker部署MySQL 首先把最新的镜像拉下来docker pull mysql:latest，然后可以使用docker run --name your_first_mysql -e MYSQL_ROOT_PASSWORD=your_password -d mysql:latest这个命令来启动一个MySQL了。 现在来尝试连接一下MySQL吧（其实并不能，现在连端口都没映射出来，不信可以连一下试试）。要想真正连到刚刚的MySQL里的话需要这样操作 # 创建MySQL容器 docker run --name your_first_mysql -e MYSQL_ROOT_PASSWORD=your_password -d mysql:latest # 进入到MySQL容器里 docker exec -it your_first_mysql bash # 在容器里 root@b42296a45a92:/# mysql -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 9 Server version: 8.0.18 MySQL Community Server - GPL Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) 可以发现这次终于连上了，不过这样直接启动还是有诸多问题存在的：容器被删除后数据也就没了、其他程序很难连接到这个MySQL上、只设置了密码还有好多配置没动呢。。。先不管这些个问题，只需要知道现在这个服务已经成功启动了就好，具体的配置会在docker-compose编排中介绍。 0X02 使用docker部署Redis 首先把最新的镜像拉下来docker pull redis:latest，然后就可以用docker run --name your_first_redis redis -d这个命令来启动一个Redis了。 现在来试试连接到Redis吧（不知道你发现了没有，我完全就是在重复上面的过程。就算刚才没发现，现在也该发现了。既然你都发现了那我也就不重复了，直接来看docker-compose的编排吧） 0X03 使用docker-compose编排 作为开发人员而非运维，docker-compose的使用次数是非常高的。通常来说使用Docker都是在Unix like环境中（也就是指的Linux、MacOS或者BSD），那我们安装和使用它就很方便了：pip install docker-compose --user就可以了。显然这又是一个Python编写的工具，装好就可以开始编写docker-compose.yml文件了下面看一下我完整的这个实例文件 version: \"3\" services: mysql: # 服务的名字 image: mysql:latest # 使用最新的MySQL镜像 container_name: dev_mysql_latest # 指定一个容器名 restart: always # 自动重启（开机后也自动启动） network_mode: bridge # 指定网络为桥接 volumes: # 挂载的目录 - ./data/mysql:/var/lib/mysql # 将容器中的/var/lib/mysql挂载到当前目录下的data/mysql（MySQL的数据文件，防止容器删除后数据丢失） - ./config/mysql:/etc/mysql/conf.d # 挂载好配置文件的目录（没有特殊配置的时候可以不用写这行） ports: - \"3306:3306\" # 将容器的3306端口映射到本地的3306 environment: - MYSQL_ROOT_PASSWORD=test_passwd # MySQL的root密码「必填」 # 以下皆为选填 - MYSQL_DATABASE=test_db # 如指定，则在容器生成时创建该数据库 - MYSQL_USER=test_user # 新用户名 - MYSQL_PASSWORD=test_passwd # 新用户的密码 - MYSQL_ALLOW_EMPTY_PASSWORD=no # 不允许使用空密码 - MYSQL_RANDOM_ROOT_PASSWORD=no # 不适用随机密码（为yes时会随机生成一个密码并输出到stdout上，通常是你看到的窗口） - MYSQL_ONETIME_PASSWORD=onetime_passwd # 一次性密码（使用时，第一次登录会强制要求修改密码） redis: image: redis:latest container_name: dev_redis_latest network_mode: bridge restart: always ports: - \"6379:6379\" command: redis-server --requirepass \"mypassword\" # 指定容器运行的命令，命令处设置密码 接下来在这个docker-compose.yml所在的目录下执行docker-compose up -d，这时候两个服务就都在后台悄悄启动好并可以连接使用了。如果不想同时开启所有的服务可以在后面接服务名，例如：docker-compose up -d redis。 0X04 其他关于docker与docker-compose docker exec -it dev_mysql_latest bash可以接入到dev_mysql_latest容器的bash中进行简单的操作； docker-compose up -d中的-d是在后台的意思，可以不加这个参数从而把输出都打在终端上方便调试； docker-compose stop和docker-compose rm分别是停止容器和销毁容器，容器必须先停止再销毁； 如果想直接销毁容器可以使用docker-compose down，从输出可以看到就是先执行了stop再执行了rm。 ","date":"2019-11-14","objectID":"/posts/docker-mysql-redis/:0:0","tags":["Docker","MySQL","Redis"],"title":"使用 Docker 部署 MySQL 和 Redis","uri":"/posts/docker-mysql-redis/"},{"categories":null,"content":"0X00 线程池和进程池 多线程和多进程在平时编程中是挺常见的操作，不过控制进程和线程的数量是一件比较麻烦的事情。尤其是线程，之前在搜索到的关于线程池的内容多数都是“造轮子”，实际上Python已经给我们造好了这个轮子。文档在这里，甚至还是中文的https://docs.python.org/zh-cn/3.7/library/concurrent.futures.html#module-concurrent.futures 我这里简单的整理了一下，做个小样例展示出来方便查阅。这里就假装大家对Python有一定的了解，而且也对操作系统中的线程和进程有一些了解了。（哦对了，还需要了解一下GIL才行） 0X01 使用线程池 #!/usr/bin/env python import time from concurrent.futures import ThreadPoolExecutor # 定义一个用来测试的方法 def test_func(num): time.sleep(1) res = num * num print(res) return res if __name__ == '__main__': # 构造一个可以容纳两个线程的线程池 thread_pool = ThreadPoolExecutor(max_workers=2) with thread_pool as pool: for i in range(5): pool.submit(test_func, i) # 将任务提交到线程池里 这段代码执行下来耗时3s大概，因为有两个线程在执行，所以第一次执行了两个任务，第二次两个，第三次一个。可以通过调整range()数量和max_workers来观察输出结果。 0X02 使用进程池 #!/usr/bin/env python import time from concurrent.futures import ProcessPoolExecutor def test_func(num): time.sleep(1) res = num * num print(res) return res if __name__ == '__main__': process_pool = ProcessPoolExecutor(max_workers=2) with process_pool as pool: for i in range(5): pool.submit(test_func, i) 可以看到跟上面线程池的方案比起来就只是把ThreadPoolExecutor换成了ProcessPoolExecutor而已。 0X03 通用的部分 其中ProcessPoolExecutor和ThreadPoolExector均接收参数max_workers，不过由于线程和进程的本质区别，所以还是要适当设置这两个值。默认情况下max_workers的值设置为自己的逻辑处理器个数，如果你的CPU是4核8线程的那就自动设置成8。在 Windows 上，max_workers 必须小于等于 61，否则将引发 ValueError。 pool.submit()中的参数是submit(func, *args)，所以把需要传递给func的参数逐个写在后面就好了。 pool.submit()后会返回一个Future对象，这个对象可以查看任务的执行情况：cancel()可以取消任务（如果任务还没开始的话）；cancelled()查看任务是否被取消了；running()任务是否在进行；done()任务是否执行完了。使用result()可以获得任务的结果，还未完成的任务会等待结果，可以使用timeout参数指定等待多少秒。 与Python无关的部分 使用多线程要注意不要开过多的线程，因为在线程中切换也需要资源，线程过多可能反而会影响效率； 进程不宜过多，防止系统负载过大； 使用多进程时要万分小心不要失控，因为进程数量一旦失控可能会导致系统宕机（相关内容可以搜索了解一下fork炸弹）。 ","date":"2019-11-12","objectID":"/posts/python-thread-process-pool/:0:0","tags":["Python"],"title":"Python中的线程、进程池","uri":"/posts/python-thread-process-pool/"},{"categories":null,"content":"0X00 前言 实话讲，Django的信号(signal)机制其实用到的时候并不多，但是某些特定场景下一个信号能解决非常大的问题，所以信号这个东西还是值得了解一下的。那么为什么这里只说一些初级内容呢，主要是因为通过调查发现信号的高级知识用（我）的（也）很（不）少（会）。 目前我工作中用到的信号机制也比较少，所以可能有些事情说不到点上还请见谅。那我们开始吧~ 0X01 什么是信号 “信号机制”光从名字上来就大概能懂了。应该就是：某人发出某信号，某人接受到之后做一些事情。所以看起来非常类似我们熟悉的“订阅-发布”模式，实际上也也确实很类似。整个信号机制分成这么几个部分：发布者、信号、接收者和一个函数。 我们传统战场上的一个行为来类比会比较好解释 发送者：类似于战场上打信号弹的人 信号：信号弹（信号弹会分成几种比如红色、蓝色、绿色的） 接受者：各个不同阵地都有人观察着战场的信号弹 一个函数：接受者看到信号弹后会对应作出战术动作 比如我们有三个人：小明、小强和李铁蛋，每人又有三种信号弹：红色、蓝色和绿色，又有三个不同阵地：路口、广场和理发店。其中每个人的信号弹不同，小明的红色信号弹打出去是一个”明“字，小强的是”强“，李铁蛋的是个”蛋“。 那么这个时候场上的小明发射了蓝色的信号弹（发送者发送了特定信号），三个阵地的人都看到了，但是之前首长说只有广场的阵地要响应小明的蓝色信号弹（提前固定好接受者要接收哪部分信号），广场的阵地接受到信号之后按照之前的计划前去攻打碉堡（接受者收到指定信号后执行一个函数）。 大概的流程是这个样子的，中间可能有些不准确不过大体是对的，下面我们来看一下Django自身内置的一些信号。 0X02 Django内置的信号 Django中的信号分两类：内置和自定义的。我们先来列出现有的部分信号：（更完整的Django内置信号可以看官方文档） # model部分 from django.db.models.signals import pre_init # 数据模型构造前触发 from django.db.models.signals import post_init # 数据模型构造厚触发 from django.db.models.signals import pre_save # 数据对象保存前触发 instance.save() from django.db.models.signals import post_save # 数据对象保存厚触发 from django.db.models.signals import pre_delete # 数据对象删除前触发 from django.db.models.signals import post_delete # 数据对象删除厚触发 # migrate部分 from django.db.models.signals import pre_migrate # migrate前触发 from django.db.models.signals import post_migrate # migrate后触发 # request部分 from django.core.signals import request_finished # 请求结束后触发 from django.core.signals import request_started # 请求开始前触发 from django.core.signals import got_request_exception # 请求异常后触发 0X03 使用Django信号 使用一个Django信号首先要导入（或者编写）signal，接下来再设置好接受者，紧接着写好要执行的函数，然后注册它，最后调用就好了。 首先个人建议给signal单独放一个文件，如果你signal用的很多很多那就可以给每个app安排一个signal.py，或者如果你用的不过的话整个项目用一个signal.py就好了。首先我来在我的项目的School/下搞一个signal.py并写上下面的内容: from django.dispatch import receiver from django.db.models.signals import pre_save from School.models import Student @receiver(pre_save, sender=Student) def print_hello_when_student_pre_save(sender, instance, **kwargs): print('hello,world') 装饰器参数中：第一个参数是信号类型，第二个参数是发送者；自定义的函数中instance就是数据对象了，但是由于是pre_save参数所以此时的instance还没有被真正写入到数据库中，所以如果打印instance.id的话其实是为空的 然后我们来注册这个，看一下现在School/apps.py的内容是这样的： from django.apps import AppConfig class SchoolConfig(AppConfig): name = 'School' 我们稍加改动就能完成注册： from django.apps import AppConfig class SchoolConfig(AppConfig): name = 'School' def ready(self): # 这里的ready在Django启动的时候会自动被执行，然后我们的注册就完成了（并没有其实） from .signal import print_hello_when_student_pre_save 但是现在其实还不能用，我们知道在settings.py中我们只需要在INSTALLED_APPS中添加一个app名就可以了，但是我们这次需要把它改成apps.py下的app类名。我之前写的是School现在就要改成School.apps.SchoolConfig才行。（现在才是真的好了） In [1]: from School.models import Student In [2]: Student.objects.create(name='shawn') hello,world Out[2]: \u003cStudent: Student object (1120)\u003e 我们可以看到在我create一个数据对象的时候确确实输出了我们指定的hello,world。 关于pre_save/post_save的提示当使用obj = models.Student.objects.first(); obj.name = 'hello'; obj.save()的方法更新数据时会触发该信号，但是如果是用models.Student.objects.filter().update(name='hello')是不能触发的；在pre_save/post_save的响应函数里切忌再执行该model的.save()否则会进入死循环。 0X04 自定义信号 要使用自定义信号的时候并不是很多，不过还是可以说一下。我们拿上面信号弹的例子看一下 首先在目录下搞一个custom_signals.py文件，里面写好下面的内容 import django.dispatch # 声明一个信号：还接收两个参数（好像比信号弹强一些哈哈哈） blue_signal = django.dispatch.Signal(providing_args=['attack_time', 'attack_zone']) def callback(sender, attack_time, attack_zone, **kwargs): if sender == '小明': # 只相应小明的信号 print('attack it from {} {}'.format(attack_time, attack_zone)) else: pass blue_signal.connect(callback) 然后我们启动shell来试试python manage.py shell，在shell里模拟一下发送信号。这个过程在正常程序中是写在view中的，此处只是为了展示方便 In [1]: from School.custom_signal import blue_signal # 首先导入我们的信号 In [2]: blue_signal.send(sender='李铁蛋', attack_time='now', attack_zone='left') # 发送信号 Out[2]: # 并没有输出，因为接受者并不相应铁蛋的信号 [(\u003cfunction School.custom_signal.callback(sender, attack_time, attack_zone, **kwargs)\u003e, None)] In [3]: blue_signal.send(sender='小明', attack_time='now', attack_zone='left') attack it from now left # 小明的信号得到了相应，并且对应的参数也成功传进去了 Out[3]: [(\u003cfunction School.custom_signal.callback(sender, attack_time, attack_zone, **kwargs)\u003e, None)] 0X05 结尾 Django的信号机制","date":"2019-11-06","objectID":"/posts/django-signal/:0:0","tags":["Django","Python","Signal"],"title":"Django信号初级","uri":"/posts/django-signal/"},{"categories":null,"content":"0X00 IO总是比运算慢 众所周知计算机的IO都要比计算慢很多很多，即时是目前民用的高级SSD：三星970PRO，它的读写速度都要比内存慢上几个数量级，更不要说CPU了。所以软件的IO通常都是瓶颈，很多时候都是CPU等内存，内存等磁盘，磁盘等网络。 那么如何才能提升自己web服务的响应速度呢？通常来说简单的操作有如下两种：换硬盘或者改SQL。 0X01 换硬盘 “这难道不是废话吗？”对呀，这就是废话。当瓶颈出现在数据库的查询上了，那么把正在用的机械硬盘换成固态硬盘当然会提升效率，稍微想想就呢能明白的事情。事实上也是这样的，之前我把同样量级的数据从我们的测试环境搞到我本地，测试环境是企业级HDD，而我本地是三星970EVOPlus的SSD，会发现查询同一个内容就快了好多好多。 那其实这个换硬盘并不是好办法，毕竟不能指望全都用上SSD。而且即使用上SSD了，在查询更复杂或者数据量更多的情况下还是会出现瓶颈。那首先想到的方式就是优化SQL了。 0X02 慢查询 大家都知道优化SQL，那么优化哪条呢？一个系统里可能有几千条SQL，总不能一个个看吧。而且现在还有很多很多项目用上了ORM，根本不在系统里写原生SQL了。 这时候MySQL的慢查询功能就帮得上忙了。顾名思义“慢查询”就是很慢的查询，我们通过简单的配置能够让MySQL记录下很慢的查询语句，通过整理分析再回去系统中找到产生这些慢查询的位置逐个优化就可以了。 0X03 开启 通常来说开启MySQL慢查询日志记录的方式有两种：直接改配置文件和修改全局变量。 改配置文件： [mysqld] slow_query_log = ON # 开始记录慢查询日志 slow_query_log_file = /usr/local/mysql/data/slow.log # 慢查询日志的位置 long_query_time = 1 # 对“慢”的定义，这里是耗时超过1s 蛤？这很暴力吗？ ··· 修改配置文件的好处在于不论怎么重启数据库服务，这项配置都是存在的；缺点在于想让其生效需要重启一次数据库才行。 改全局变量： ```sql mysql\u003e set global slow_query_log='ON'; // 打开慢查询记录 mysql\u003e set global slow_query_log_file='/home/shawn/slow_query.log'; // 慢查询日志的位置（我乱写的，不建议写在自己的$HOME下） mysql\u003e set global long_query_time=1; // 还是1s 修改全局变量的优势和缺点正好与上面相反，能立即生效但是重启会失效。 0X04 使用 我们来检查一下这个是不是配置好了： ~/Workstadion/blog  master ?1  mycli -h 127.0.0.1 -uroot Password: mysql 5.7.17 mycli 1.19.0 Chat: https://gitter.im/dbcli/mycli Mail: https://groups.google.com/forum/#!forum/mycli-users Home: http://mycli.net Thanks to the contributor - Ryan Smith mysql root@127.0.0.1:(none)\u003e show variables like 'slow_query%'; +---------------------+--------------------------------------+ | Variable_name | Value | +---------------------+--------------------------------------+ | slow_query_log | ON | | slow_query_log_file | /home/shawn/slow.log | +---------------------+--------------------------------------+ 2 rows in set Time: 0.042s mysql root@127.0.0.1:(none)\u003e show variables like 'long_query_time'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 1.000000 | +-----------------+-----------+ 1 row in set Time: 0.035s 现在看起来我们的慢查询日志就搞好了，开始试一试真正意义上的慢查询呢SELECT SLEEP(5)。其实这是个没有意义的查询（不太好拿测试数据出来给大家展示），不过没关系了，因为它会很扎实得等5s，满足我们对“慢查询”的认知了。 然后去看看我们的日志呢，可以看到如下输出，就没有问题了。 root@1971e980abad:/var/lib/mysql# tail -f 1971e980abad-slow.log mysqld, Version: 5.7.17 (MySQL Community Server (GPL)). started with: Tcp port: 3306 Unix socket: /var/run/mysqld/mysqld.sock Time Id Command Argument # Time: 2019-11-03T12:47:41.827832Z # User@Host: root[root] @ [172.17.0.1] Id: 14 # Query_time: 12.000459 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0 SET timestamp=1572785261; select sleep(12); 这时候就可以将慢查询日志的时间设置成一个合理的值，然后就可以静待生产环境或者测试环境打一大堆log出来喽。我们可以看到每个超出我们定义的时长的SQL和其耗时，当日志累计起来我们就可以从耗时最长的或者出现最频繁的开始优化SQL了。 当然，有很多第三方的慢查询日志分析工具可以帮助我们，不过我这种初级MySQL用户遇到的也还不太需要。每次的慢查询日志自己搞下来逐行看一看总结一下规律也就能找到问题所在啦。如果后面日志越来越多越来越复杂的时候再考虑用第三方工具吧~ 这里连接MySQL用的不是mysql的官方命令行工具，而是一个Python编写的叫做mycli的工具。这个工具实现了mysql命令的几乎所有功能，并且支持语法高亮与自动补全，使用体验非常棒。可以直接用pip install mycli --user安装 ","date":"2019-11-03","objectID":"/posts/mysql-slowquer-simple/:0:0","tags":["MySQL"],"title":"MySQL 慢查询初步","uri":"/posts/mysql-slowquer-simple/"},{"categories":null,"content":"0X00 前言 这篇博客的目标读者：正在使用Linux桌面，打算长期继续使用下去的同学（这也就意味着熟悉Linux下的基础操作，理解Linux下的常见概念）。 这里有一个我之前写的“在Linux桌面下存活”可以参考一下。 0X01 颜值就是战斗力 Linux也不都是黑色背景白色字的命令行。 首先要换的就是一套主题和图标，不论是KDE、Gnome还是Xfce都可以在对应的网站找到大量的主题，简单换过图标和主题之后再配合一张好看的壁纸，整个观感立马就好了不少。 KDE: https://store.kde.org/ Gnome: https://www.gnome-look.org/ Xfce: https://www.xfce-look.org/ 然后要换的就是字体了，等宽字体我推荐这几个Hack、Source Code Pro、Cascadia Code都很好看，适合在终端、IDE和编辑器里使用。 最后就是zsh主题，如果使用的是zsh的话推荐使用powerlevel10k这个主题，是由powerlevel9k发展而来的，但是速度比powerlevel9k快好多。 0X02 善用alias和bash函数 相信大家在初学Linux的时候都学过alias指令，没学过的话我简单介绍一句：“alias是给命令取别名的工具，例如执行alias new_ls=\"ls -l\"过后再使用new_ls命令就和使用ls -l一样了”。 首先我们知道将alias命令直接写在shell里，在关闭重开shell之后就失效了（起码现在知道了）。所以我们要把alias写在.bashrc或者.zshrc中（依你使用的shell而定）。现在我的.zshrc中就有很多已经写好了的，下面给大家分项几个对大家都有用的 # alias for simple command alias f='open .' # 用图形文件管理器打开当前目录 alias h='open ~' alias py='ipython3' alias py2='ipython2' alias du='/usr/bin/ncdu' # ncdu是一个命令行里可视化查看磁盘（目录）占用率的工具 alias cat='/usr/bin/bat' # bat是一个带语法高亮、行号显示且能够上下滚动的cat加强版 alias down='aria2c -x16 -j4' # 使用aria2进行多线程下载 alias me=\"cd $HOME/Workstadion/ \u0026\u0026 ls\" alias connect_android=\"scrcpy --bit-rate 256M\" # 使用scrcpy连接到接入电脑的Android手机 alias code='LANG=\"zh_CN.UTF-8\" LANGUAGE=\"zh_CN.UTF-8\" code' # 用VSCode打开 alias jwt=\"python3 $HOME/Workstadion/script/get_token.py jwt\" # 自己编写的方便获取开发环境jwt token的工具 alias token=\"python3 $HOME/Workstadion/script/get_token.py token\" alias open='LANG=\"zh_CN.UTF-8\" xdg-open \u003e /dev/null 2\u003e /dev/null' # 使用对应的工具打开文件 # alias to source command # 映射到原始工具 alias _du='/usr/bin/du' alias _cat='/usr/bin/cat' # alias some command for network and proxy # 使用代理做一些事情 alias use_proxy=\"ALL_PROXY=socks5://127.0.0.1:1080\" alias git_clone_with_proxy=\"ALL_PROXY=socks5://127.0.0.1:1080 git clone\" alias yay_with_proxy=\"ALL_PROXY=socks5://127.0.0.1:1080 yay\" alias的功能还是比较单一，毕竟从“别名”这个名字上就看出来了。不过好在还可以写函数，写函数这就是我们程序员比较擅长的了。这里简单介绍（真的超简单的那种）一下Bash的函数（当然zsh也是兼容的） foo() { # xxxxx } 这就是一个函数的基本结构了，再配合参数：$0 $1 $2 $3这种就足够搞一些特别简单的函数了。我们一段命令如果前面固定后面是变动的那么可以方便得用alias比如：docker-compose up -d web和docker-compose up -d test这种就可以将前半部分搞成alias再拼起来用比如alias dp-up-d=\"docker-compose up -d\"然后dp-up-d web就可以了。但是如果是grep -Rn \"hello\" ~/articles和grep -Rn \"world\" ~/article这种呢就比较麻烦，这时候就要用函数了。 grep_this() { grep -Rn \"$1\" . } 这个函数就可以通过grep_this hello来执行grep -Rn \"hello\" .这个命令。其中的参数：$0和$1指的就是整个命令用空格分隔之后的下标，例如$1指的就是整条命令用空格分隔后，下标为1的值，这里的实例函数也就是指的\"hello\"了。 这里给大家复制几条我自己的配置，可能是大家都用的到的 # v2er translate # v2ex一老哥搞的词典api，直接用`v2 hello`就可以查到hello的中文（支持中英互转） v2() { declare q=\"$*\" curl --user-agent curl \"https://v2en.co/${q// /%20}\" } # docker # 使用`attach mysql`可以接入到含有mysql的行的docker容器的bash中（当有且仅有一个grep结果时才有效，有兴趣的老哥可以自行修改） attach() { docker exec -it `docker ps | grep $* | awk -F ' ' '{print $1}'` bash } attach_django() { docker exec -it `docker ps | grep $* | awk -F ' ' '{print $1}'` python manage.py shell } grep_this() { # `grep_this test`从当前目录递归找下去，将所有含有test的文本文件的行都输出出来 grep -Rn \"$1\" . } find_this() { # `find_this test`递归当前目录找到路径含test的 find . -name \\*$1\\* } 0X03 本地服务尽量Docker化 传统方式我们都是将本地开发所依赖的服务例如：MySQL和Redis这种直接部署一份。但是其实这种部署方式在开发环境是不太好的，强烈建议使用Docker部署。如果你同时要用3个MySQL版本，3个Redis版本，这只还要用5个Mongo版本，那怕不是要疯了对吧。 首先我们搞一个docker再搞一个docker-compose然后开始尝试部署环境（docker和docker-compose的安装这里就不说了）。 version: \"3\" services: mysql: image: mysql:latest container_name: dev_mysql network_mode: bridge restart: always volumes: - ./data/mysql:/var/lib/mysql ports: - \"3306:3306\" environment: - MYSQL_ROOT_PASSWORD=你的密码 command: mysqld --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci redis: image: redis:latest container_name: dev_redis network_mode: bridge restart: always ports: - \"6379:6379\" mongo: image: mongo:latest container_name: dev_mongo network_mode: bridge restart: always ports: - \"27017:27017\" volumes: - ./data/mongo:/var/lib/mongo 我们将内容保存为docker-compose.yml，并放到一个空目录下开始一波复杂的操作：首先执行docker-compose up -d，然后等一会儿，等拉完这三个镜像，接下来。。。。。。就好了（是的，这就好了）。这样你就在本地部署了MySQL+Redis+Mongo并且都是最新的版本，如果需要不同的版本可以这个样子 version: \"3\" services: mysql: image: mysql:latest container_name: dev_mysql network_mode: bridge res","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:0:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"z 我们在Linux下最常用的可能就是cd了，z配合zsh的原生功能可以让我们不再需要cd。比如我们经常去~/Workstadion/too_young/simple这个目录，那么以后就可以直接z simple跳过去了。其实是这样，你可以在终端直接用z来查看当前各个目录的权重，z命令就是从权重最高开始找，直到找到对应的目录并跳转过去。 另外我说的zsh原生功能是指：默认就是cd。比如我们在zsh中直接输入/就会跳到根目录，直接..就是上级，直接~/Workstadion就是对应的目录，省下了cd的过程 ","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:1:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"fabric 如果你用fabric的话，在fabfile.py存在的目录下输入fab 再tab就可以补全fabric中的命令 ","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:2:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"extract 我们知道解压zip要用unzip，解压rar要用unrar，解压tar.gz要用tar -zxvf……知道是知道了，但是每次用起来还是有点麻烦，所以有了这个插件。不管你是啥tar.gz/tar.xz/tar.bz2/zip/rar，统统一个x全部解压。 x qwer.zip x asdf.rar x zxcv.tar.gz x qwer.tar.xz ","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:3:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"thefuck 顾名思义，当你想吼fxxk的时候，这个插件就有用了。thefuck是一个python程序，要用pip install thefuck --user来安装。装好之后再配合上这个插件就可以实现神奇的效果。比如当你在命令行里输入了一长串命令git commmt -m \"feat: 新增了什么鬼的需求点\"，显然前面的commit拼错了，如果是以前的话就只有返回去改。这时候就很想大吼fxxk了，但是请淡定。双击一下你的ESC键，会发现你的命令已经被自动修正并写在命令行里了。 是的，这个thefuck就是自动修复写错的命令的工具。 ","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:4:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"zsh-autosuggestions和zsh-syntax-highlighting 这两个都是需要手动安装的，可以去GitHub上搜一下，很简单。分别是实时提示命令的正确性和预先提示类似的历史命令的 ","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:5:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"一句话介绍其他的： git 是一堆现成的git的alias fzf-zsh 需要先安装fzf。在命令行里按下Ctrl+R，用新的逆向搜索替换原来不好用的老式搜索 git-open 用浏览器打开当前git仓库：支持GitHub和GitLab colored-man-pages 彩色高亮的man 0X05 好像没啥了 一时半会也想不到有什么要补充的了，等什么时候想到新的了再搞吧。祝大家生活工作愉快~ ","date":"2019-11-02","objectID":"/posts/better-linux-desktop/:6:0","tags":["Linux"],"title":"在 Linux 桌面下活得舒服","uri":"/posts/better-linux-desktop/"},{"categories":null,"content":"0X00 前言 这篇博客的目标读者：有Linux使用基础，打算使用Linux作为桌面系统的人（这也就意味着熟悉Linux下的基础操作，理解Linux下的常见概念）。 注：这篇博客不讨论不对比Linux/Windows/MacOS等不同的操作系统。 0X01 首先选择一个发行版本 众所周知Linux是一个内核，在这个内核基础上有很多发行版本例如：Archlinux、Manjaro、Ubuntu、Deepin、Debian、openSUSE、Fedora等等。那我们首先要选择一个来安装到自己电脑上。 基于我自己的使用体验比较推荐使用的是：Manjaro、Ubuntu、Deepin这三个，他们共同的优点是：适合初次使用Linux桌面的人。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:0:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"Manjaro 基于Archlinux，滚动更新。该发行版本最大的特点是软件版本紧跟最新，而且由于是滚动更新所以不用像多数系统一样隔一段时间来一次大版本更新。只要按照系统提示进行常规系统升级就可以将整个系统一直保持在最新的状态。 Manjaro能够使用Archlinux的AUR软件仓库，算是软件仓库最丰富的发行版本之一了，很多其他系统要操作很多步骤才能装好的软件在Manjaro下一个命令就能装好。 另一个优势是可以使用ArchWIKI，很多问题在WIKI里都有详细的解释。 而Manjaro的缺点也是有的，例如用户群体还是不够大。如果你电脑出现了点问题，那么极有可能你的同事和朋友都没有用过这个操作系统，只有一点点在网上找解决方案。如果是Ubuntu的话可能好多人都用过，可以给你一些提示或者建议。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:1:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"Ubuntu Ubuntu是基于Debian的发行版本，应该是目前用户数量最庞大的Linux桌面发行版本了。最大的优势也是上述的用户足够多，几乎你遇到的一切问题都会有人遇到过，网上一搜几本就有答案了。 缺点就是跟Manjaro比起来软件比较少，倒不是不能安装而是装起来可能更麻烦一点。还有就是据说Ubuntu的图形界面会无缘无故挂掉，不知道现在的情况是怎么样了。 注：Ubuntu的版本号为xx.04和xx.10两个，例如\"14.04/14.10/15.04/15.10/16.04/16.10\"这种，代表的意思是“发布的年份.发布的月份”。Ubuntu每年4月和6月会发布新的版本，然后偶数年发布的是LTS也就是长期支持版。所以建议使用最大的偶数年的四月份版本，目前来说的话就是18.04。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:2:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"Deepin Deepin是基于Ubuntu的发行版本，是国内的一个团队在维护的。界面比较漂亮，又由于是国内团队维护的系统所以集成了一些常用的Windows软件例如：微信、QQ这种。通常来说在Linux上运行Windows程序还是有点麻烦的，尤其是完美运行，所以Deepin做的这个工作是非常值得肯定的。尤其适合初次使用Linux桌面的人。 而且Deepin有很多优秀的高质量的自研软件，例如深度音乐、深度影院、深度终端等，不仅好用而且好看。 仍然是因为国产的原因，开发团队设计团队都是中国人，用户群体也很多中国人，所以有一个维护的很棒的中文论坛，方便大家交流问题。 注：Deepin即将发布V20版本，值得期待 Archlinux并非不好，只是安装起来比较麻烦，而且有N卡独立显卡的话驱动也是一个问题，不是很适合新手使用，如果不在乎这些的话Archlinux则是一个更棒的发行版本； openSUSE并非不好，只是国内的用户太少了，而且给openSUSE打包的程序也不多，不过据说openSUSE的KDE调教很棒。如果不太在乎这些的话openSUSE也是非常棒的发行版本； Fedora并非不好，只是更新太快了，半年就有一个大更新。而且作为红帽系列的发行版本，相对来说提供的软件包也比较少。如果能接收半年一个大版本并且一年就不再对旧版本提供支持的话，Fedora也不失为一个好发行版本（实际上我自己也用过大概一年）； LinuxMint等等也并非不好，有些我自己也没有体验过，再有些确实不太适合新手使用这里也就不再一一列出； 有一点要注意的是：尽可能不要使用极小众的发行版本在生产电脑上，因为一旦出问题可能很难解决。 0X02 然后选择一个桌面环境 桌面环境简称DE，在Windows上我们只有一个可选，在MacOS上同样只有一个可选，但是Linux上我们有非常多的选择，主流的就有：KDE、Gnome、dde、Xfce、i3等。那么我们应该怎么选呢？ 首先使用Linux桌面有一个准则（我自己认为的，不一定所有人都认同）：尽可能少的调整你的桌面，所以我们尽可能使用原生的桌面样子。因为Linux桌面环境并没有Windows和MacOS那么稳定，所以越多的修改就意味着越多的不稳定。而且由于Linux桌面允许极高程度的自定义，初次使用很容易掉到“美化桌面”的坑里。 综合来说我目前推荐的三个DE是：KDE、Gnome、Xfce、dde ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:3:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"KDE 自从KDE5之后，KDE就变得非常非常棒了，也正是我现在使用的桌面环境。整体设计风格比较统一，也比较漂亮。KDE的默认桌面初看起来有点像Windows（可能就是因为下面的“任务栏”吧） ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:4:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"Gnome Gnome与KDE一起位居Linux桌面环境市场的前两名。与KDE不同，Gnome的设计初看起来有点像MacOS（应该是因为顶部的“Topbar”） ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:5:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"Xfce Xfce的最大特点就是简洁高效。我记得之前使用Xfce的时候开机默认才使用不到500M的内存。总之Xfce就是那种简单的，不华丽的高效的桌面环境。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:6:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"dde dde是Deepin团队的作品，风格与以往不同，我也不太会说，可以去官网看一下具体的。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:7:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"如何选择呢？ 实际上操作系统和DE是完全剥离的，不过我们既然是新手就最好使用官方自带的或者直接支持的，防止出现意外对吧。其中Manjaro官方支持最多：同时提供了KDE、Gnome和Xfce的安装镜像，又由社区提供了dde的版本；Ubuntu官方使用Gnome，但有社区提供的kUbuntu使用KDE和xUbuntu使用Xfce；Deepin的话就使用dde，使用其他DE的话会使Deepin丧失优势（Deepin里很多官方提供的Windows程序在非dde桌面下支持并不好）。 现在操作系统和DE已经对应好了，可以选择一个自己喜欢的组合了。选好之后可以先搞一个虚拟机来体验一下，觉得不错的话就来装系统吧。 0X03 软件部分 都已经打算使用Linux作为桌面系统了，那应该很清楚Windows、MacOS、Linux的二进制文件是不互通的吧。所以有可能你在Windows、MacOS中使用的程序在Linux下并没有。当然，这也是目前Linux桌面的最大瓶颈（我自己这么认为）。 不过也不用过分担心，我们常用的工具比如：Chrome、Firefox、PyCharm（IDEA等Jetbrains家所有IDE）、Postman、VScode等还是原生支持的，甚至比其他系统有更好的体验。接下来主要介绍一些那些没有的软件的替代方案 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:8:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"输入法 输入法在Linux下是有一个搜狗的，不过我不喜欢，大家有兴趣可以搜索一下尝试尝试。如果不用搜狗的话我个人比较推荐两个：sunpinyin和Rime。其中sumpinyin几乎是所有Linux都带的，选好就能用；Rime是一个开源输入法，使用门槛稍高，但是我觉得更好用。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:9:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"梯子 我自己是使用的v2ray作为梯子，在Linux下很容易就能装好。如果是用ssr的可能会比较麻烦，虽然有ssr的解决方案但是我没用过不太好说。ss倒是简单，可以搜索安装一个shadowsocks-qt5的图形工具，就可以正常使用了。 Linux下一般梯子都是一个独立的服务，需要在浏览器上配置代理。不论是Chrome还是Firefox都推荐使用switchomega这个扩展来配置浏览器代理 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:10:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"影音播放 Linux下本地视频播放器两个大佬：mpv和VLC。其中VLC开箱即用，简单粗暴；mpv则是异常简单，想要使用高级功能？麻烦您自己编写配置文件。所以可以见得如果对播放器自定义要求很高的话建议使用mpv或者如果没有什么要求也能用mpv，否则vlc更合适。 当然这两个播放器都贼厉害，没有说谁好谁坏。 本地音频播放器我用的是deadbeef，简单、好看、好用。在线播放器的话，网易云音乐是提供官方Linux客户端的。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:11:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"Office 这里我只想说\"金山NB！\" * 100。金山公司搞的wps-office是提供官方Linux版本的，对Office支持度非常高，轻度使用的话能替换掉MS-Office了。如果对开源比较敏感的话，可以使用libreoffice，是开源的。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:12:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"终端模拟器 MacOS的用户应该知道iTerm2，Linux中优秀的终端模拟器层出不穷，不过我最推荐的还是terminator。不仅可以轻松的分多个tab，还可以在每个tab中左右上下分栏。再配合一个好看的zsh主题、hack字体、自己喜欢的配色，简直找不到任何缺点。 ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:13:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"virtualbox 虚拟机是不得已的方案，毕竟有时候要用的程序真的在Linux下没有替代方案，所以我选择用一个开源的虚拟机工具virtualbox来应对不时之需。 0X04 最后 我个人觉得Linux是一个非常有魅力的操作系统，虽然它的桌面还不够好，学习成本也比较高。但是就像Vim一样，当你越过鸿沟之后会发现另一个世界。到现在位置我使用Linux桌面已经两年多了，目前对自己的所用的系统非常满意。系统信息、桌面环境和主题们如下： ██████████████████ ████████ shawn@T480 ██████████████████ ████████ ---------- ██████████████████ ████████ OS: Manjaro Linux x86_64 ██████████████████ ████████ Host: 20L5A027HK ThinkPad T480 ████████ ████████ Kernel: 4.19.79-1-MANJARO ████████ ████████ ████████ Uptime: 12 hours, 23 mins ████████ ████████ ████████ Packages: 1374 (pacman) ████████ ████████ ████████ Shell: zsh 5.7.1 ████████ ████████ ████████ Resolution: 2560x1440 ████████ ████████ ████████ DE: Plasma ████████ ████████ ████████ WM: KWin ████████ ████████ ████████ WM Theme: Materia-Manjaro ████████ ████████ ████████ Theme: Breath-Dark [GTK2/3] ████████ ████████ ████████ Icons: Tela-dark [GTK2/3] Terminal: terminator CPU: Intel i7-8550U (8) @ 4.000GHz GPU: Intel UHD Graphics 620 GPU: NVIDIA GeForce MX150 Memory: 14795MiB / 32058MiB ","date":"2019-10-29","objectID":"/posts/living-on-linux-desktop/:14:0","tags":["Linux"],"title":"在 Linux 桌面下存活","uri":"/posts/living-on-linux-desktop/"},{"categories":null,"content":"0X00 就算只有一节我也要写标题 众所周知Dockerfile是构建Docker镜像的优良方式，而使用Dockerfile构建镜像最重要的就是为数不多的几个命令，而本次的主题COPY和ADD就是其中两个。我们知道这两个命令都是将文件搞到Docker镜像里用的，那究竟有没有区别，有什么区别呢？ 首先我们看一下当前这个目录： λ ~/Workstadion/learn_docker ls Dockerfile excited.tar 我们有着么一个Dockerfile，可以看到是基于fedora的一个镜像，并且将目录下的excited.tar放进了创建好的shawn目录中 FROM fedora RUN mkdir shawn WORKDIR shawn COPY excited.tar . 我们可以看到工作目录下已经存在了一个excited.tar了，也就意味着我们成功将这个文件搞进去了。 λ ~/Workstadion/learn_docker docker build . -t wtf ; docker rm learn_docker; docker run --name learn_docker -it wtf bash Sending build context to Docker daemon 23.04kB Step 1/4 : FROM fedora ---\u003e e9ed59d2baf7 Step 2/4 : RUN mkdir shawn ---\u003e Using cache ---\u003e 5014855b8533 Step 3/4 : WORKDIR shawn ---\u003e Using cache ---\u003e 82f9d06a4d3c Step 4/4 : COPY excited.tar . ---\u003e 1bc293d81b33 Successfully built 1bc293d81b33 Successfully tagged wtf:latest learn_docker [root@f6a4f34ed504 shawn]# ls -l total 20 -rw-r--r-- 1 root root 20480 Sep 25 14:15 excited.tar 如果同样的操作用ADD呢？看上去是类似的操作实际上并不是 FROM fedora RUN mkdir shawn WORKDIR shawn ADD excited.tar . 我们进到容器里可以看到打包文件被拆解了（压缩文件也会被解压） λ ~/Workstadion/learn_docker docker build . -t wtf ; docker rm learn_docker; docker run --name learn_docker -it wtf bash Sending build context to Docker daemon 23.04kB Step 1/4 : FROM fedora ---\u003e e9ed59d2baf7 Step 2/4 : RUN mkdir shawn ---\u003e Using cache ---\u003e 5014855b8533 Step 3/4 : WORKDIR shawn ---\u003e Using cache ---\u003e 82f9d06a4d3c Step 4/4 : ADD excited.tar . ---\u003e cc1d067d0dbe Successfully built cc1d067d0dbe Successfully tagged wtf:latest learn_docker [root@89276af7b0a2 shawn]# ls -l total 4 drwxr-xr-x 2 1000 1000 4096 Sep 25 14:15 excited [root@89276af7b0a2 shawn]# cd excited/ [root@89276af7b0a2 excited]# ls file_0 file_10 file_12 file_14 file_16 file_18 file_2 file_4 file_6 file_8 file_1 file_11 file_13 file_15 file_17 file_19 file_3 file_5 file_7 file_9 其实不止这样，ADD 命令还能下载文件：ADD https://too.young/too/simple.pdf /hello就能将文件下载下来并且命名为hello；如果是ADD https://too.young/too/simple.pdf /hello/（只是最后多了斜杠），docker会认为你想将文件下载到/hello的目录下，如果没有他会自己创建；还有我们不是说ADD能下载文件还能解压文件吗，但是这两个又不能同时生效，意味着如果你想用ADD https://too.young/too/simple.zip /sometimes/naive/将下载好的simple.zip文件解压到/sometimes/naive/目录里是不行的，最后结果只是将压缩包下载到了那个目录而已。 总结来说的话就是 COPY简简单单复制，ADD灵活多变。但是其实我们最好就直接用COPY命令，真的需要解压什么的操作多写一个管道符也没有多麻烦。而且其他人看管道符后面接命令参数要比分析ADD命令来的舒服得多。 算下来ADD能做的没有什么是COPY加管道做不了的。而且又加上ADD的结果变换多端，不管是自己写还是其他人看都很麻烦，所以说大家能不用ADD就不用了吧。 ","date":"2019-09-25","objectID":"/posts/dockerfile-copy-and-add/:0:0","tags":["Docker"],"title":"Dockerfile 中的 COPY 与 ADD 指令","uri":"/posts/dockerfile-copy-and-add/"},{"categories":null,"content":"0X00 Redis的分库 使用过MySQL或者类似的数据库都应该知道，一个数据库内部是可以分成多个库的。比如MySQL从上到下是MySQL service -\u003e database -\u003e table -\u003e field，但是一开始使用redis的时候好像是没有database这一层的呢？其实是存在这么一层的，redis默认是存在编号0到15这总共16个库的，每个库除了命名空间不同以外都是相同的。也就是说在编号为0的库里set name shawn之后跑到编号为1的库里get name是拿不到的。 那这么说来这个分库究竟有什么用呢？其实很少用的到，甚至就连redis的设计者自己都说搞这个是较蠢的操作。我们日常用到的唯一一处地方就是在测试服务器上，因为一台机器部署了太多服务，而且每个服务又都要用redis所以就每个服务分开使用0~15这些数据库。 因为开一个完整redis实例的资源消耗本身就很小，所以分库这个操作就更显的不太用的到了，毕竟我们完全可以在一台机器上开多个redis实例从而实现相同的效果，而且又方便管理。 不过也还是简单介绍一下好了：/etc/redis.conf文件中的databases参数是设置redis实例具有多少个数据库的，如果真的需要的话可以修改这个参数然后重启redis从而生效。 那么怎么切换这些库呢？我们使用redis-cli连上数据库之后可以看到类似这么一个命令提示： 127.0.0.1:6379\u003e 这就意味着我们在操作0库，使用select指令可以切换库，切换后在命令行上会提示出来 127.0.0.1:6379\u003e select 2 OK 127.0.0.1:6379[2]\u003e select 6 OK 127.0.0.1:6379[6]\u003e select 15 OK 127.0.0.1:6379[15]\u003e select 16 (error) ERR DB index is out of range 127.0.0.1:6379[15]\u003e 这里可以看到在默认情况下切到一个并不存在的库会报错。（真是一本正经的废话） 0X01 Redis的认证 redis的认证要比MySQL要简单的多，在MySQL中要配置用户名、密码甚至还要校验网段，不过redis中就只有一个简简单单的密码校验。而且是那种不需要用户名，仅有密码的校验。还有一点不同的就是，redis不支持设置多个密码。 在默认情况下redis刚刚装完是不需要认证直接使用的，设置密码有下面两种常用方式： ","date":"2019-09-19","objectID":"/posts/easy-redis/:0:0","tags":["Redis"],"title":"Redis入门使用：分库、认证与持久化","uri":"/posts/easy-redis/"},{"categories":null,"content":"修改配置文件 修改/etc/redis.conf配置文件，里面有一个requirepass参数，默认是加了注释的也就意味着并没有密码。如果要为其加上密码就将那一行改为requirepass this_is_password，然后使用类似systemctl restart redis的命令来重启一下你的redis服务，密码就生效了。 ","date":"2019-09-19","objectID":"/posts/easy-redis/:1:0","tags":["Redis"],"title":"Redis入门使用：分库、认证与持久化","uri":"/posts/easy-redis/"},{"categories":null,"content":"执行redis命令 另一种方法就是使用redis命令config set的方式来设置密码，这种方式好在不需要重启redis服务，但是默认情况下一旦重启密码也就跟着失效了。 127.0.0.1:6379\u003e config set requirepass this_is_password ","date":"2019-09-19","objectID":"/posts/easy-redis/:2:0","tags":["Redis"],"title":"Redis入门使用：分库、认证与持久化","uri":"/posts/easy-redis/"},{"categories":null,"content":"认证登录 那密码搞上了，怎么登录呢？登录也有两种方式，一种是直接把密码写在连接的命令里，另一种是进入到redis-cli的交互界面再输入密码 λ blog master ✗ redis-cli 127.0.0.1:6379\u003e keys * (error) NOAUTH Authentication required. 127.0.0.1:6379\u003e exit # 直接将密码写在连接的命令行里 λ blog master ✗ redis-cli -a this_is_password Warning: Using a password with '-a' or '-u' option on the command line interface may not be safe. 127.0.0.1:6379\u003e keys * 1) \"a\" 127.0.0.1:6379\u003e exit # 进入到redis-cli交互界面后再验证 λ blog master ✗ redis-cli 127.0.0.1:6379\u003e auth this_is_password OK 127.0.0.1:6379\u003e keys * 1) \"a\" 没有验证也能进入到redis-cli的交互界面，只是没有权限而已 其中keys *可以看到当前库里的所有key 0X02 Redis持久化 本来还像要自己写一些关于持久化的初级知识，结果发现这篇文章已经写的超级棒了，索性补充一小部分他这里没有提到的吧。 默认情况下redis装好就启用了RDB； 默认情况下重启redis的时候会写入一次RDB； 使用docker部署redis需要注意的点 ：如果使用docker部署redis，并且需要对数据进行持久化的话一定记得将redis数据文件挂载出来，否则会导致数据丢失。因为持久化的文件存在与docker容器内部，只是重启容器还好问题不大，但是如果docker stop/rm/run一波下来就会导致容器内部的数据被删的一干二净了（准确地说容器都不是之前的那个了，数据当然也就不在了）。 ","date":"2019-09-19","objectID":"/posts/easy-redis/:3:0","tags":["Redis"],"title":"Redis入门使用：分库、认证与持久化","uri":"/posts/easy-redis/"},{"categories":null,"content":"0X00 最常见的两种构建方式 构建Docker镜像的方式并不多，最常用的也就只有：编写Dockerfile和使用docker commit这两种。既然方式分为两种那么肯定是有区别的（废话），那我们来看看吧。 首先来介绍一下这两种构建方式，假设打算使用docker部署我们的服务，那么我们来使用两种方式来构建一下这个镜像吧。 0X01 docker commit 像我们这种新手平时用的比较多的应该就是docker commit xxxxx hub.xxx.xxx/xxx:xxx这种方式了，我们称之为docker commit。这种方式比较好操作，比较好理解，操作也比较容易。如果掌握了git的工作流程，那么使用docker commit方式来构建镜像简直是小菜一碟。 我们搞一个基础镜像比如fedora，那我们把它搞下来：docker pull fedora 运行并进入到容器里docker run --name our_container -it fedora /bin/bash，此时shell已经接入到容器里了 我们来安装吧 dnf install python3然后pip3 install Django 这样就装好了，可以退出docker里的shell了 docker commit -m \"安装了Python和Django\" our_container our_image:latest这样就将刚刚的容器打包成名为our_image的镜像了 现在使用docker images就可以看到刚刚打包的our_image了 后面如果还需要更新这个镜像那么就可以继续运行这个docker run --name our_container -it our_image:latest然后更新完了再docker commit -m \"balabalabala\" our_container our_image:latest打包一层新的上去就好了 0X02 Dockerfile 另一种常见构建镜像的方式是编写Dockerfile文件，通过Dockerfile构建一个新的镜像。 创建一个名为Dockerfile的文件，并写入以下内容 在当前目录执行docker build . -t our_image:latest就可以了 FROM fedora RUN dnf install python3 RUN pip3 install Django 0X03 对比两种方法 两者的对比从下面几个方面来进行 ","date":"2019-09-17","objectID":"/posts/build-docker-image/:0:0","tags":["Docker"],"title":"正确构建 Docker 镜像","uri":"/posts/build-docker-image/"},{"categories":null,"content":"docker hub 这两种方法其中docker commit方法通常需要使用私有docker hub，而Dockerfile则可以不使用。因为docker commit完成的镜像如果要分享给其他人的话最方便的途径就是走docker hub（自己构建完push到hub上，其他人再pull下来），而Dockerfile只要保证使用的是公共的基础镜像那么所有人就都可以直接构建出目标镜像。 之所以说是最方便，是因为还有一种docker save的方式可以打包镜像成xxx.tar，别人再docker load。这种方式虽然能用但是很麻烦，也不是常规操作 docker 镜像当然可以上传到公共的hub上，但是公司的商业内容怕是不允许呦 ","date":"2019-09-17","objectID":"/posts/build-docker-image/:1:0","tags":["Docker"],"title":"正确构建 Docker 镜像","uri":"/posts/build-docker-image/"},{"categories":null,"content":"使用难度 docker commit操作简单，每次更新镜像时接入到容器里一波操作最后docker commit xxxxx封装一下再使用docker push将镜像推到hub就可以了。基本所有人都可以很快搞清楚流程并且开始上手使用。 Dockerfile就显得难了不少，构建复杂一点的镜像往往Dockerfile就几十上百行了，再算上里面十多种Dockerfile语法，并不是所有人都能很快上手的。 ","date":"2019-09-17","objectID":"/posts/build-docker-image/:2:0","tags":["Docker"],"title":"正确构建 Docker 镜像","uri":"/posts/build-docker-image/"},{"categories":null,"content":"镜像层数 众所周知Docker镜像的每次commit都会在文件系统上新增一层 （反正现在你肯定知道了），而层数的增加意味着“镜像体积膨胀”。如果决定使用docker commit的方式维护镜像，那么不免后期会大量通过commit来更新镜像从而导致镜像层数过多，体积过大。 而Dockerfile中的每个指令也都会为镜像摞上一层，但是可以通过换行整合的方式使之缩小很多，比如将内容写成下面这种 RUN dnf install python3 gcc \\ \u0026\u0026 cat /data/our_hosts \u003e /etc/hosts \\ \u0026\u0026 cat /log/balabala.log | grep \"test\" \u003e /data/test_group.log \\ \u0026\u0026 pip install Django \\ \u0026\u0026 django-admin startproject testproject 将本来要写很多行的整理到一个命令后面，以此来缩减镜像层数 ","date":"2019-09-17","objectID":"/posts/build-docker-image/:3:0","tags":["Docker"],"title":"正确构建 Docker 镜像","uri":"/posts/build-docker-image/"},{"categories":null,"content":"镜像的可追溯性 Dockerfile比docker commit具有更好的可追溯性。所谓的可追溯性 就是说可以追溯这个镜像从基础镜像开始到最新的这个版本都做了什么。 如果使用docker commit的方式维护镜像，那么可以通过commit时候的备注和shell的历史记录来追溯，但是这两种方式都是不确定的，这样时间长久下去就会导致祖传镜像 的出现。所谓祖传镜像就是指那些“我也不知道这个镜像里都改过哪些配置，装过哪些软件和第三方包，反正还能用就凑合用把”的镜像。 如果是使用的Dockerfile那么就很容易了，构建这个镜像所经历的所有所有步骤都清清楚楚写在Dockerfile里了，可追溯性比docker commit高到不知道哪里去了。 0X04 精简镜像 精简镜像其实原则很简单，就是删除不用的东西，减少镜像层数 。首先正确使用Dockerfile就可以大幅度精简镜像了，其次就是在Dockerfile中记得清理一些无用的东西。 比如你要再容器中编译安装一个C库，那么最后是不是gcc就用不到了？用不到了就卸载掉。 每次apt install \u0026 dnf install \u0026 pip install是不是要有缓存文件？用完了就删掉 以此类推，将用不到的内容清理掉就行了 比较重要的一点就是选择一个合适的基础镜像，通常来说我们自己使用ubuntu搞一个镜像装上Python再搞上Django(Python的一个web框架)所构建的镜像不会比直接用Django的官方基础镜像更好。 0X05 需要注意 有几个需要注意的点： docker image是有层数限制的，目前是127层，超出的话再commit是会报错的 docker build实际上就是自动的docker run/xxx/commit/stop/rm工作流 docker build .中的.看起来是Dockerfile所在位置，实际并不是，而是“上下文环境”。具体的内容比较多，可以自行搜索了解 指定Dockerfile要用docker buld -f xxx/xxx/xxx/hello .，其中xxx/xxx/xxx/hello被当作Dockerfile ","date":"2019-09-17","objectID":"/posts/build-docker-image/:4:0","tags":["Docker"],"title":"正确构建 Docker 镜像","uri":"/posts/build-docker-image/"},{"categories":null,"content":"0X00 前言 不论多初级的后端程序员，只要认可自己是“后端程序员”那最起码也是听过“数据库索引”这个东西的，应该也直到这东西能让数据库变快。但是具体“数据库索引”是个什么东西，怎么用，为什么能让数据库变快可能就不一定清楚了。 这篇博客仅仅是简述了“数据库索引”的最基础内容，不涉及内部原理（其实我也还没搞懂）。阅读了本篇博客可以（也许）搞清楚怎么给一张表添加索引，索引加在哪儿，以及为什么不能给所有字段加索引。 0X01 一个业务场景 假设我们搞了一个电商平台“并夕夕”，现在有超多用户大概几百万，很多人都在我们系统里买了东西，大概下了1000W单。那么我们用来存储订单的表就有大概1000W条数据了，那么我们这个没有进行过任何优化的数据库就已经顶不住了，我们每次在后台查看订单列表的时候一个查询就要快20s，显然是一个接受不了的速度了。 那么我们来看一下这个列表都干了什么：“根据 订单状态（下单待支付/已支付/配送中/已收货/退货中/已完成），订单类型（直接购买/拼单购买/分享白拿）等状态筛选，并取出前20条展示出来”。后台SQL是这个样子的SELECT * FROM bxx_order WHERE order_status = xxx AND order_type = xxx ORDER BY -id LIMIT 20，我们可以看到真正会影响性能的主要就在WHERE子句里的查询条件。接下来我们给这两个字段分别加上索引，查询效率就会高非常非常多。 我在公司的一张400W的表中为“状态”字段添加索引后对比未添加索引的时候快了不止100倍。 下面我们来看一下索引是怎么使用的 0X02 什么是索引\u0026为什么用索引会加快查询 网上常见的解释是将“书的目录”与“数据库索引”关联理解，其实是没有问题的，但是第一次理解这个概念光直到这个也没什么用啊，还是不太能明白呢。我们来把一本书作为一张数据库表好了，其中有如下字段一级标题，二级标题，小节，正文。比如一级标题就是“1. 引论”，二级标题是“1.1 什么是操作系统”，小节是“1.1.1 作为扩展机器的操作系统”，正文就是“balabalabalabala”，那么一般的书籍都会对一二级标题建立索引（目录）。 我们想想这本500多页的《现代操作系统》如果没有索引，那我想看“多核处理器上的虚拟机”这一部分要怎么找呢，就只有从第一页开始翻书一直到找到为止。这就是数据库不添加索引时候的查询方式，逐条查看。 现在我们给标题添加了索引，也就多出了一部分名为“目录”的内容。但是这个目录只有7页，我可以在这7页目录里找到我需要的内容在500页中的位置然后直接翻到对应的那一页。数据库也是这样的，假设我们给订单状态添加了索引，数据库就会知道状态为“待支付”的订单在数据库的什么位置，状态为“已完成”的在什么位置。也就不用每次都一条条的查看这1000W条订单数据了。 0X03 如何添加索引 那说了这么多，怎么才能给数据库表添加上索引呢？如果是在建表阶段就考虑到了索引，本来的建表SQL是这样的 CREATE TABLE bxx_order( id INT NOT NULL, order_status VARCHAR(11) NOT NULL, order_type VARCHAR(11) NOT NULL, PRIMARY KEY (id) ); 那么只需要在最后加上索引指定就行了 CREATE TABLE bxx_order( id INT NOT NULL, order_status VARCHAR(11) NOT NULL, order_type VARCHAR(11) NOT NULL, PRIMARY KEY (id), -- 上面没变，只是加了最后的逗号 INDEX (order_status) ); 那如果表已经存在了呢？也只需要ALERT TABLE bxx_order ADD INDEX order_status就可以将已经存在的表添加上索引。 注意在生产环境上添加索引时可能会锁表从而导致业务收到影响。所以请务必调查清楚自己使用的数据库版本和当前的业务，结合数据库版本特性和业务情况酌情在生产环境操作 注意在生产环境上添加索引时可能会锁表从而导致业务收到影响。所以请务必调查清楚自己使用的数据库版本和当前的业务，结合数据库版本特性和业务情况酌情在生产环境操作 注意在生产环境上添加索引时可能会锁表从而导致业务收到影响。所以请务必调查清楚自己使用的数据库版本和当前的业务，结合数据库版本特性和业务情况酌情在生产环境操作 0X04 索引加在那儿 好了，我们已经知道了索引是什么也知道索引怎么加了，那在哪些字段加索引才会有很好的效果呢？给WHERE后面用的字段加索引，给ORDER BY后面的字段加索引，给“选择的值”加索引。 SELECT create_time, pay_time, username, amount FROM bxx_order WHERE order_status = \"done\" AND order_type = \"free\" ORDER BY -pay_time LIMIT 20 第一部分是“给WHERE后面的字段加索引”，也就是上面这个实例中的order_status和order_type字段；第二部分是\"给ORDER BY后面的字段加索引\"，也就是上面实例中的pay_time字段。因为我们加了索引是方便查询和排序的，你看我们的书上不会给正文加索引，更不会给每一页的字数加索引，因为这些索引加了之后对我们翻看目录并没有任何帮助。 重点是第三点给“选择的值”加索引 。什么是“选择的值”呢？就是说某个字段的值就只有这些，这个字段的值值可能是这几种十几种里的一种。 我们看这样一个SQLSELECT * FROM bxx_order WHERE order_status = \"done\"。因为我们的order_status一共就这么几种，假设这些状态的订单都差不多数量的话，应该是每种状态的订单都是167W左右。那么我们给这个字段加了索引后，数据库查询时候就可以瞬间将查询范围从1000W缩减为167W，效率一下就是原来的6倍，如果状态更多那效果会更明显。（当然不是越多越好，副作用后面会提到） 0X05 联合索引 联合索引可以理解为将索引后的数据再索引一次。比如这么一个SQLSELECT * FROM bxx_order WHERE order_status = \"done\" AND ORDER_TYPE = \"free\"，如果建立了order_status和order_type的联合索引，那么就会在对order_status索引完成后的167W条数据的g基础上再来一次对order_type的索引，速度会进一步变快。不过要注意的是联合索引要是AND条件下才会触发，如果用了SELECT * FROM bxx_order WHERE order_status=\"done\" OR order_type=\"free\"这种就不会触发了。 建立联合索引也比较简单，建表的时候将上述的INDEX (order_status)改成INDEX (order_status, order_type)就好了，改表也是同理。 0X06 索引当然有副作用 那是不是给所以字段都加上索引数据库就飞快了？当然不是，如果是的话那索引这东西就应该不需要手动添加而是直接自动实现了是吧。那索引的副作用是什么呢？下面这几种情况都是建立索引的副作用，或是无用功: 想想把书写完了，生成了一个目录，读者读起来开开心心是吧。突然你说要往书的第三章和第四章中间插入一些新内容，本来是只需要修改内容的，现在还要对应修改目录部分。数据库也是一样，本来一次的写入变成了两次写入，性能自然下降了； 如果建立了过多的索引，那么索引占用的内存就会很大，对机器造成过多的负载。想想如果我们给书的所有内容都建立了索引，甚至正文页建立了索引，那索引也就没什么太大的意义了； 如果我们把索引添加在了经常只出现在SELECT后面的字段上，那不仅每次写入数据的时候都要更新索引数据，而且这个索引对我们的系统性能还毫无提升甚至因为这个索引我们系统还更慢了。 0X07 总结 总结起来说的话，索引的建立应该遵循其设计初衷与原理，否则不仅对查询毫无帮助反而会加大消耗的内存并且减慢写入的速度。 ","date":"2019-09-15","objectID":"/posts/database-index-simple/:0:0","tags":["Database","Index"],"title":"关系型数据库索引初步使用","uri":"/posts/database-index-simple/"},{"categories":null,"content":"0X00 给一个方法计时 现在我们有一个需求，需要给程序中的一部分方法计时，以监控他们执行完具体用了多久。那么在没有装饰器的情况下我们会写出类似这样的代码： import datetime def foo(): pass # some code start = datetime.datetime.now() foo() end = datetime.datetime.now() print((end - start).seconds) 0X01 引入装饰器 如果只是临时给一个方法使用也不是不行，但是如果我们需要监控大量的方法呢？众所周知Python中function也是可以作为参数传递的，那来看一下下面这种写法呢 import datetime def timer(func): def run_func(): # 方法内部将之前计时的功能封装起来 start = datetime.datetime.now() result = func() # 得到参数方法的返回值 end = datetime.datetime.now() print((end - start).seconds) return result # 将真正的结果返回给调用者 return run_func # 调用内部方法，进行计时 def foo(): pass # some code def bar(): pass # some code # 将方法作为参数发送给 timer() timer(foo)() timer(bar)() 因为上面timer(foo)返回的结果是一个func，所以后面需要再加一对括号来调用这个方法 这种方法只写了一次计时的逻辑，但是可以给任意一个方法使用，其实这时候def timer(func)已经是装饰器了，下面调用的方法timer(foo)/timer(bar)也是正确的装饰器使用方法。那是不是觉得和常见的装饰器使用不太一样呢？其实常见的@timer用法是Python中提供的一种语法糖。 0X02 @语法糖 其实上面的代码就可以直接使用@语法糖了，具体用法是这样： @timer def foo(): pass # some code foo() 语法糖实际只是便于我们编码的一种设计，按理说一切被称为语法糖的东西都不是必须的。比如说这里的语法糖完全可以用上面的方法来应用装饰器，但是为什么还是设计了这个语法糖呢？我们对比下面两种方法的调用，我们假设get_data_from_server/db是一个耗时较久的方法： def get_data_from_server(): pass @timer def get_data_from_db(): pass timer(get_data_from_server))() get_data_from_db() 这两种方法看起来明显是后面get_data_from_db的用法更易读。所以这处语法糖的出现能大幅度的提升代码的可读性，更能提升维护性质：当代码中调用了1W次非语法糖形式的装饰器计时时，取消计时就要修改1W行代码；如果用了@语法糖，那就只需要将方法定义处的@timer注释掉就可以了。 0X03 传参 “那要传参咋搞哇？” “这么搞” #!/usr/bin/env python # coding=utf-8 import time import datetime def timer(func): def run_func(secs): # 这儿接受参数 start = datetime.datetime.now() result = func(secs) # 这儿把参数搞进去 end = datetime.datetime.now() print((end - start).microseconds) return result return run_func @timer def foo(secs): time.sleep(secs) @timer def bar(secs): time.sleep(secs) if __name__ == '__main__': foo(3) # 和不使用@timer时候的timer(foo)(3)是一样的 bar(5) # 和不使用@timer时候的timer(bar)(5)是一样的 “那我要是有好几个参数呢，怎么搞？” “就还是这么搞啊” def timer(func): def run_func(*args): # 这儿接受参数，几个都行 start = datetime.datetime.now() result = func(*args) # 这儿把参数搞进去，几个都行 end = datetime.datetime.now() print((end - start).microseconds) return result return run_func @timer def foo(start_secs, end_secs): time.sleep(random.randint(start_secs, end_secs)) foo(3, 5) ","date":"2019-08-22","objectID":"/posts/python-decorator-simple/:0:0","tags":["Python"],"title":"Python 装饰器","uri":"/posts/python-decorator-simple/"},{"categories":null,"content":"0X00 什么是可迭代对象 我们平时用到的list/set/tuples是最常见的可迭代对象，简单判断就是说当可以for item in this_obj的时候this_obj就是可迭代对象。所以不只是list/set/tuples，打开的文件或是Django中的queryset也都是可迭代对象。 In [1]: user_list = iter(['shawn', 'danny', 'jenny', 'liming']) In [2]: next(user_list) Out[2]: 'shawn' In [3]: next(user_list) Out[3]: 'danny' In [4]: next(user_list) Out[4]: 'jenny' In [5]: next(user_list) Out[5]: 'liming' 使用iter()可以将列表变为一个迭代器，然后使用next方法访问下一个元素。实际上iter(this_obj)方法和next(this_obj)方法分别调用的是this_obj.__iter__()和this_obj.__next__()，所以我们如果想要自己实现迭代器的话就需要实现这两个方法。 0X01 迭代器 那我们来实现实现一个计算斐波纳契数列的迭代器吧。 class Fib: def __init__(self): self.pre = 0 self.cur = 1 def __iter__(self): \"\"\"迭代器的iter方法返回自身\"\"\" return self def __next__(self): \"\"\"每次计算下一个值\"\"\" res = self.cur self.cur = self.cur + self.pre self.pre = res return res 运行起来： \u003e\u003e\u003e fib = Fib() \u003e\u003e\u003e for i in fib: print(i) 1 1 2 3 5 8 13 21 34 55 89 144 233 这里没有中断的判定，如果需要中断的话就抛出一个StopIteration异常就好了，就像这样： class Fib: ....... def __next__(self): res = self.cur self.cur = self.cur + self.pre self.pre = res if res \u003e 100: # 数列最后值小于100 raise StopIteration return res 0X02 生成器 生成器是一类特殊的迭代器 ，将我们平时写的这种[i for i in range(100)]列表生成式的方括号换成小括号，就是将结果从列表变成生成器了(i for i in range(100))。 我们可以从这里看出来 In [1]: a = [i for i in range(100)] In [2]: b = (i for i in range(100)) In [3]: type(a) Out[3]: list In [4]: type(b) Out[4]: generator 0X03 区别在哪儿呢 既然迭代器用起for .. in ..来跟列表之类的差不多，那为什么还要搞这个东西出来呢。显然不是用来玩票的，用处是大大的有。下面来举个例子，有一个10GB大小的日志文件，但是我们机器只有8G的内存，还打算逐行分析日志内容，那肯定就不能直接把文件全部都读入内存了，显然内存是不够用的。这种类似的时候就可以使用迭代器的方法打开这个文件，其实也就是我们常用的这种for line open('nginx.log')的方式。这种方式可以发现不管多大的文件，只要不是单行超长的文件，读取起来都是差不多的。 因为迭代器只是每次要用下一行的时候才去读取下一行，而不是把整个文件都读到内存里再在需要的时候去取。简单地来说就是列表元组之类的结构是保存数据，而迭代器是保存算法 。 ","date":"2019-08-20","objectID":"/posts/python-iterator-and-generators/:0:0","tags":["Python"],"title":"Python 中的可迭代对象、迭代器与生成器","uri":"/posts/python-iterator-and-generators/"},{"categories":null,"content":"0X00 前言 没有什么前言，只有一个数据库模型，下面的代码使用这个模型拿来测试。 from django.db import models class Major(models.Model): name = models.CharField( '名字', max_length=100, blank=True, null=True, ) class Student(models.Model): name = models.CharField( '名字', max_length=100, blank=True, null=True, ) select_major = models.ForeignKey( Major, verbose_name='专业', on_delete=models.SET_NULL, blank=True, null=True, related_name='main_student', ) ext_major = models.ManyToManyField( Major, verbose_name='附加专业', related_name='ext_student', ) 我们先假设Major表存在10条数据，而Student表存在1万条数据。 0X01 使用select_related 如果我们要得到所有的学生和他们所学专业的名字，那么我们可以轻松写出下面的代码 for student in Student.objects.all(): print(student.name, '--\u003e', student.selected_major.name) 这样就能得到所有学生姓名和他们所学的专业名了，但是重点在于这次查询其实是一个很低效的查询，因为在Student.objects.all()的时候查询了一次数据库，而且每次访问student.selected_major.name的时候都会再查询一次数据库，基于上述条件这两行代码将会查询10001次数据库，是一个比较夸张的数字了。那么如何用select_related来优化这次查询呢？ for student in Student.objects.all().select_related('major'): print(student.name, '--\u003e', student.selected_major.name) 其实就是在all()之后添加了select_related('major')，这次就只需要对数据库进行一次查询。在我本地的类似环境下测试结果是不使用select_related消耗的时间是优化后的400%左右。 本质上是select_related进行了数据库级的JOIN操作，具体的大家可以通过查看print(Model.objects.filter().query)或者django-extensions等方法查看具体的SQL 这里可能会有一种声音“查询从10001次到1次差了几乎10000倍时间却只省下了70%多？”这个问题其实比较好理解，JOIN操作本来就会使一次SQL查询变的很慢，毕竟要跨越多张表。 select_related是用于优化“多对一”结构中从“多”表出发查“一”表，也就是这里的多个学生对一个专业，从学生出发查询得到“专业”。 这里我贴出使用前和使用后的两坨SQL，大家可以对比一下。（这些SQL都是Django-extensions分析得出的） 优化前 SELECT \"School_student\".\"id\", \"School_student\".\"name\", \"School_student\".\"select_major_id\" FROM \"School_student\" LIMIT 3 SELECT \"School_major\".\"id\", \"School_major\".\"name\" FROM \"School_major\" WHERE \"School_major\".\"id\" = 4 SELECT \"School_major\".\"id\", \"School_major\".\"name\" FROM \"School_major\" WHERE \"School_major\".\"id\" = 5 一直重复，直到循环结束 优化后 SELECT \"School_student\".\"id\", \"School_student\".\"name\", \"School_student\".\"select_major_id\", \"School_major\".\"id\", \"School_major\".\"name\" FROM \"School_student\" LEFT OUTER JOIN \"School_major\" ON (\"School_student\".\"select_major_id\" = \"School_major\".\"id\") 只剩下了一条使用了JOIN的SQL，显然会比上面快很多 0X02 使用prefetch_related prefetch_related是从“一对多”结构中“一”表出发查“多”表，也就是说从专业表出发查询得到学生信息。比如我们想看这所有专业中每个专业下的学生 for major in Major.objects.filter(): print(major.main_student.all()) 这种查询是最容易写出来的，不过需要注意的一点是，这里第一行循环前有一个filter()会查询一次数据库，后面每一次main_student.all()都会再查询数据库。我们只有10个专业，查11次数据库还行问题不大，但是随着数据增多这里查询数据库的次数会呈线性增长。那么如何解决这个问题呢？ for major in Major.objects.filter().prefetch_related('main_student'): print(major.main_student.all()) 就只是向上面使用select_related一样添加一个prefetch_related在filter()后面就可以了。修改了之后的代码只会查询两次数据库：第一次把所有的专业查出来了，也就是Major.objects.filter()的作用；第二次是使用一个SELECT * FROM student WHERE select_major_id IN (x,x,x,x,x,x)形式的SQL查询到了对应的Student。然后使用Python将其组装整合而非数据库 ，这样就能大幅度减少查询数据库的次数了。 这里我贴出使用前和使用后的两坨SQL，大家可以对比一下。（这些SQL都是Django-extensions分析得出的） 优化前 SELECT \"School_major\".\"id\", \"School_major\".\"name\" FROM \"School_major\" SELECT \"School_student\".\"id\", \"School_student\".\"name\", \"School_student\".\"select_major_id\" FROM \"School_student\" WHERE \"School_student\".\"select_major_id\" = 4 LIMIT 21 SELECT \"School_student\".\"id\", \"School_student\".\"name\", \"School_student\".\"select_major_id\" FROM \"School_student\" WHERE \"School_student\".\"select_major_id\" = 5 LIMIT 21 ......一直重复直到循环完所有的 优化后 SELECT \"School_major\".\"id\", \"School_major\".\"name\" FROM \"School_major\" SELECT \"School_student\".\"id\", \"School_student\".\"name\", \"School_student\".\"select_major_id\" FROM \"School_student\" WHERE \"School_student\".\"select_major_id\" IN (4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103) 0X03 总结 还有总结？大家都有，我也有好了 通常来说恰当的使用select_related和prefetch_related可以大幅度提升自己ORM查询的速度，很多时候大家只是写了能用的查询，现在可以尝试着使用select_related/prefetch_related写出好用的查询啦 ","date":"2019-05-12","objectID":"/posts/django-select_related-prefetch_related/:0:0","tags":["Python","Django","ORM"],"title":"使用 Django 中的 select_related 和 prefetch_related 优化查询","uri":"/posts/django-select_related-prefetch_related/"},{"categories":null,"content":"0X00 没有什么用的开头 众所周知https是安全加密的协议，那么https究竟是如何保证数据传输的安全性的呢？这里来简单介绍一下https的安全机制。 0X01 简介 HTTPS全称是 Hypertext Transfer Protocol Secure 也就是传统的HTTP加上了S Secure ，也可以叫 HTTP over TLS ，总之就是加了密的HTTP嘛。 最常见的加密就是通信双方共有一个密钥，使用这个密钥对原文进行加密和解密，简单粗暴。但是万一密钥泄露了，那么任何人都可以用这个密钥解密你们通信的所有内容，甚至通信双方都是无感知的。（想象一下你和小姐姐的聊天记录被一个扣脚大汉一行行的看完还嘿嘿傻笑是不是贼恐怖） 这种加密方式有一个严肃的问题就在于通信双方得有一个约定密钥的机会，还要保证传输密钥的信道是绝对安全的。（想象一下你想跟小姐姐聊天的话需要先私下悄悄碰面，然后再一个没有人的地方悄咪咪的交换密钥，然后再用这个密钥进行加密通信）这还不是重点，重点就是你俩明明已经有了一个可以保证绝对安全的通信方法（在没有人的地方悄咪咪地通信）了，那为啥还非要交换密码然后加密通信呢，是不是觉得贼蠢。 HTTPS就能解决这种问题，下面我们来开始吧。我们假设一个场景就是客户端（你）和一台web服务器（小姐姐）通信的场景，在这个场景下描述HTTPS。 0X02 HTTPS的前提条件 HTTPS通信有两个重要前提条件，没有这些条件的话HTTPS是没有实际作用的 使用非对称加密算法对内容进行加密，目前我们使用的RSA算法就是非对称加密算法 存在一个绝对安全的第三方机构，我们称之为CA（Certificate Authority），我们认为它不会被攻击，他发布的信息都是可靠的 非对称加密：与对称加密的一个密钥用来加密解密不同，非对称加密有一对密钥，分为“公钥”和“私钥”。使用密钥对中的公钥对原文进行加密之后再次使用公钥解密是行不通的，必须使用对应的私钥才能对密文进行解密； CA：CA是一个被认为绝对安全且诚实的第三方机构，服务器将自己密钥对中的公钥交由CA管理。客户端访问服务器时候可以从CA处取得服务器的公钥； 0X03 开始吧 无论客户端还是服务器，所有人的公钥都是可以对外公开的，私钥都是绝对绝对不能公开的，要保证私钥绝对安全，一旦私钥泄露即可造成HTTPS加密失效 首先你打算以安全的方式联系小姐姐，从你这里开始整个流程。 首先你知道小姐姐的住址，然后通过小姐姐的住处前往CA去取得小姐姐的公钥； 你把想给小姐姐说的话用小姐姐的密钥加密好，连同自己的公钥一起发送给小姐姐； 小姐姐收到了你的密文，使用自己的私钥解密得到了你发送的原文； 小姐姐写好了给你的回复，使用你提供的公钥对原文加密，并传输给了你； 你收到小姐姐的密文后使用私钥将内容解密，得知了小姐姐的心意； 重复上面的操作。 0X04 如果被截取 我们假设在上面任意地方数据被黑客截取了，那他能获取你和小姐姐的消息内容吗？ ","date":"2019-04-09","objectID":"/posts/why-https-is-security/:0:0","tags":["Security","HTTP","HTTPS"],"title":"为什么https是安全的（简单介绍）","uri":"/posts/why-https-is-security/"},{"categories":null,"content":"截取了你和CA的通信 黑客截取了你和CA的通信，那么他能得到的信息就是：“你要和哪个小姐姐通信，小姐姐的公钥是什么”。虽然你也泄露了一点点信息（你要和谁通信），但是这些并非敏感数据，黑客并不能对这个做什么。小姐姐的公钥就更不重要了，任何人都能获得； ","date":"2019-04-09","objectID":"/posts/why-https-is-security/:1:0","tags":["Security","HTTP","HTTPS"],"title":"为什么https是安全的（简单介绍）","uri":"/posts/why-https-is-security/"},{"categories":null,"content":"截取了你第一次发给小姐姐的内容 如果黑客截取了你发送给小姐姐的内容，那内容中只有：“你发给小姐姐的密文，你自己的公钥”。密文由于他没有小姐姐的私钥，所以无法解密；你自己的公钥也是所有人都可以获取的，也没有用； ","date":"2019-04-09","objectID":"/posts/why-https-is-security/:2:0","tags":["Security","HTTP","HTTPS"],"title":"为什么https是安全的（简单介绍）","uri":"/posts/why-https-is-security/"},{"categories":null,"content":"截取了小姐姐回复给你的内容 假设黑客截取了小姐姐回复给你的内容，那内容中只有：“小姐姐给你的密文”。由于黑客并没有你的私钥，所以他并不能知道小姐姐给你发送了什么内容。 0X05 总结 总结下来，黑客即使截取了所有的数据，那也只有下面这些： 客户端访问的服务器地址 客户端request携带的加密数据 服务端response携带的加密数据 客户端服务端双方的公钥 其中加密数据由于没有对应的私钥所以无法解密，而双方的公钥本来就是公开的所以并没有什么用。 ","date":"2019-04-09","objectID":"/posts/why-https-is-security/:3:0","tags":["Security","HTTP","HTTPS"],"title":"为什么https是安全的（简单介绍）","uri":"/posts/why-https-is-security/"},{"categories":null,"content":"编辑器之神可不是浪得虚名 0X00 遇到了一些问题 如果你看到了这篇文章那么我可以认为你至少已经掌握了vim基本用法。在我个人使用vim的过程中遇到了一些问题，这次就挑“寄存器”这部分出来说一下吧。 我这里贴一段代码过来，这段代码出自我在学校时写的一个小项目open_disframe if slave_method == 'handshake': process_handshake_request(slave_id) slave_socket.send(\"handshake success\".encode('utf-8')) continue elif slave_method == 'heartbeat': process_heartbeat_request(slave_id) slave_socket.send(\"heartbeat success\".encode('utf-8')) continue elif slave_method == 'submit_message': process_submit_message_request(message['message_list']) slave_socket.send(\"submit success\".encode('utf-8')) continue elif slave_method == 'get_message': send_message_list = process_get_message_request(slave_id, 10) slave_socket.send(send_message_list.encode('utf-8')) continue elif slave_method == 'exit': process_exit_request(slave_id) slave_socket.send(\"exit success\".encode('utf-8')) continue else: slave_socket.send((\"not found request: \" + slave_method).encode('utf-8')) 现在我们在编辑编辑这段代码的时候需要将其中的部分内容替换掉，比如将第一个if中的slave_socket.send.....和第二个if中的process_heartbeatxxx和第三个if中的continue替换成break。虽然这么做可能有点诡异，不过这里实在没找到完美的适用场景，公司的代码也不能贴出来，又懒得编造一个完美适用场景，就先这样看吧。 通常我们与到这种情况都会是下面的操作流 手动替换第一处的break 复制break 找到下一处需要替换的地方选中 p粘贴过去 找到下一处需要替换的地方选中 p粘贴过去 发现“剪贴板”被上面那次被替换掉的数据覆盖了，以至于将上次删掉的内容贴过来了，而不是自己想要的break 沮丧中。。一个个复制粘贴替换过来 这个问题困扰了我还是挺久的，一度认为vim居然这么蠢，后来才发现是我自己太蠢了，vim中有一个“寄存器”机制是我们均大多数人要么没用过要么听都没听过的，而这个“寄存器”就是用来解决这种问题的。 0X01 什么是vim中的寄存器 我们可以大致将vim中的寄存器想像成系统使用的剪贴板，我们平时用的剪贴板只有一个格子，每次复制/剪切了新的数据就会覆盖掉之前格子里的数据。而vim中的寄存器有多种，每种有不同的功能特性，而且每种寄存器又有一个到多个不等。总的来说vim中有下述十种寄存器。 匿名寄存器 \"\" 编号寄存器 \"0 到 \"9 小删除寄存器 \"- 26个命名寄存器 \"a 到 \"z 3个只读寄存器 \":, \"., \"% Buffer交替文件寄存器 \"# 表达式寄存器 \"= 选区和拖放寄存器 \"*, \"+, \"~ 黑洞寄存器 \"_ 搜索模式寄存器 \"/ 在命令模式下: register就可以看到各个寄存器的值了 0X02 如何使用这些寄存器 这些寄存器种类较多，只挑选一些我个人常用到的吧，更完善的一篇博客可以看这里 ","date":"2019-01-19","objectID":"/posts/vim-register/:0:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"如何使用寄存器 先简单说一下，下面会具体介绍的。当你选中一坨内容后想要复制他到命名寄存器\"a中，那就需要在选中后依次按下\"ay。其中\"a相当于是选中了一个寄存器，而y就是当前选中内容与寄存器之间的操作。因为是y所以就是将当前选中的内容复制到\"a中，如果是\"ap的话就是将命名寄存器\"a中的内容粘贴到选中的位置上来。 ","date":"2019-01-19","objectID":"/posts/vim-register/:1:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"匿名寄存器 匿名寄存器基本就是我们最常用的那个，平时d/x的删除操作都会将被删除的内容存到这个匿名寄存器中，而通过y复制的内容则会放到\"0的编号寄存器上，不过这个\"0寄存器会一直保持与匿名寄存器\"\"相同的内容。正因为如此才会出现上面提到的现象：“复制了一段内容A，选中B粘贴，原来B的位置就被A替换了；再选中C粘贴，替换C位置的不是A而是B”，因为在B被A替换时，匿名寄存器\"\"中就已经是内容B了。 ","date":"2019-01-19","objectID":"/posts/vim-register/:2:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"编号寄存器 编号寄存器中，\"0保存的是你复制的内容，\"1~\"9保存的是删除的内容、上次删除的内容、上上次删除的内容等历史9次删除的内容。 这也正是解决上述问题的关键所在了。下面我们来尝试这解决上面的问题，前三步是一样的，从第四步开始就有区别了。 手动替换第一处的break 复制break 找到下一处需要替换的地方选中 在命令模式下输入\"0p 找到下一处需要替换的地方选中 再命令模式下输入\"0p 重复操作直到完成 其中\"0p操作就是从编号为0的编号寄存器中粘贴内容。如果直接p的话是从默认的匿名寄存器中粘贴的，内容也就是刚刚被删掉的内容了。 ","date":"2019-01-19","objectID":"/posts/vim-register/:3:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"小删除寄存器 不足一行的删除操作会将被删除的内容放到小删除寄存器\"_中 ","date":"2019-01-19","objectID":"/posts/vim-register/:4:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"命名寄存器 命名寄存器就很好理解了，就相当于一些贴了标签的小盒子。\"ay就是复制内容到命名寄存器\"a中，\"bp就是从命名寄存器\"b中粘贴内容出来。 当你选中了一坨内容\"ay后，再选中另一坨\"ay，如你所想，之前的内容被替换了；但是需要注意的一点是，如果你选中了一坨内容\"ay后，再选中另一坨\"Ay就会将当前内容追加到\"a中。也就是说当使用大写字母表示命名寄存器复制时，是会追加而非覆盖的。 ","date":"2019-01-19","objectID":"/posts/vim-register/:5:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"黑洞寄存器 默认情况下删除内容时会将被删除的内容放到\"1和\"\"两个寄存器中，但是如果选中后使用\"_x或者\"_d的方式删除，则会将被删除的内容丢到黑洞中，也就是说并不会覆盖\"1和\"\"。所以使用黑洞寄存器也能解决上述问题 手动替换第一处的break 复制break 找到下一处需要替换的地方选中 \"_x删除 p粘贴过去 \"_x删除 p粘贴过去 重复操作直到完成 0X03 后记 vim果然是真的编辑器之神 ","date":"2019-01-19","objectID":"/posts/vim-register/:6:0","tags":["Linux","Vim"],"title":"vim 寄存器的使用","uri":"/posts/vim-register/"},{"categories":null,"content":"0X00 何为正版 正版的即“正确地使用版权”。而版权是属于版权所有人的，版权所有人提出使用条件，使用者只要符合条件，就算是正确地使用，就不违反版权法。 以上来自维基百科：“正版”词条 0X01 我的正版观念 正确的版权观念当然应该是在任何情况下都要“正确的使用版权”。但是几乎不会有人能做到这一点，我这里不谈政治正确，只是说一下我个人的正版观念。我的正版观念简单说来就是“在自己条件允许的情况下尽可能的使用正版”，当然这里的条件允许不只是指的经济条件，还有各种其他的因素。 比如说电影，按照最正确的正版意识来说应该是在上映时在电影院观看，院线下映后在各个视频网站观看或单独购买光盘/数字授权等。我个人只做到一部分，电影还在院线时我要么去电影院观看要么就等下映后在线观看，从来不下载观看“枪版”和“泄漏版”，不过下映比较久后也有可能会通过“torrent（也就是种子）”下载到本地观看。 音乐也是类似。我一般都是使用本地音乐播放器听歌，不论是在电脑还是手机上。不过每次发现新音乐都是通过在线播放器，当喜欢的新音乐达到了一定数量后就充值对应平台的会员把他们下载下来，继续使用本地播放器播放。 书籍则比较简单，选择性购买纸质版或Kindle的电子版就行了，坚决抵制盗版图书。 游戏和书籍比较类似，坚决抵制盗版游戏。尤其是现在PS4商店和Steam越发成熟，获得正版游戏的方式越来越简单，而且游戏也没有那么贵的情况下没有理由不购买正版。毕竟作为一个程序员非常清楚一款游戏背后要付出多少东西，所以一个 GTA V 卖一百多块真的不贵了，更不用说那些只卖几十块的独立游戏和国产游戏了。 软件方面打算单独说，作为一名程序员同样清楚一款优秀的软件背后所要付出的努力与心血。 0X02 开源不等于免费，更不等于无限制 在英语中，“自由软件”，即Free Software，这个词很容易被误解：Free一词既有免费的意思，也有自由的意思。而我们所谓的自由软件，则是“一类可以赋予用户指定自由的软件”。要解决这个问题，我们发布了自由软件的定义。为了方便理解，我们解释自由软件中Free，是自由言论中所说的自由，而非免费赠饮中的免费。这显然不是个理想的解决方案，它无法完全杜绝这一问题。一个意思正确，又没有歧异的词显然更好些，不过前提是这词不会引起其他麻烦。 上述内容出自GNU组织 关于“自由软件”和“开源”的常见误解 除了费用问题意外还有一个常见的误解“这不是开源软件吗，难道不是我随便怎么搞？”。其实不然，开源软件也都会拥有一个开源许可证，常见的有GPL/MIT/Apache等，使用这些软件也需要遵守他们所使用的许可证。这样才算是真正的“正确的使用版权”。 0X03 软件授权需要注意的地方 很多时候软件授权协议是一个比较麻烦的事情，以为自己购买了正版就随意使用，经常会违反对应的协议。 比如当你购买了一份Office授权后，在自己公司的电脑上激活了软件，在自己家里也登录了对应的账号，那么此时也就违反了Office的个人版用户协议； 比如当你使用Teamviewer时，从朋友家里连接到自己家里是免费的，因为用户协议说个人使用免费。但是从自己家里连接到公司就违反了个人用户的协议，虽然软件不一定可以识别到这种违规，但是这确实是不符合用户协议的； 某些IDE会区分社区版、个人版和企业版。社区版开发的软件不能售卖、个人版的IDE不能用与企业等。 Jetbarins家的协议就比较宽松，社区版可用于一切形式的开发，个人购买的也可以用于公司的商业开发。但是其中需要注意的一点是个人购买是授权给个人的，而企业购买则是授权给某一位置。简单地说就是“你自己买的Jetbarins的IDE离职后可以在家里用，也可以到下一家使用。但是如果你用的是公司提供的IDE授权，则不能带回家更不能带到另一家公司”。 0X04 后记 写这篇博文主要是为了让自己进一步的提升版权意识，并能为大家的版权意识尽一分力。 ","date":"2019-01-13","objectID":"/posts/genuine-and-pirated/:0:0","tags":["Other"],"title":"我眼中的「正版」与「盗版」","uri":"/posts/genuine-and-pirated/"},{"categories":null,"content":"0X00 时间都去哪儿了 好多人都会有这种情况，整整两天的周末回忆起来却不知道自己做了什么，明明也没看几部电影、没怎么打游戏，也没怎么学习，时间怎么就不见了呢？更为常见的一种是“等我有空了就搞”、“等我空了一定去一趟哪儿哪儿哪儿”、“下周六一定要去吃XXX”，然后就再也想不起来了。真正空下来的时候又觉得自己没事做，完全想不起来之前列出来比卷纸还长的todo list了。 实际上大家几乎不会真的有“无事可做”的时候，只是有空的时候想不起来之前的计划，定了计划又不定执行时间。最后到头来就落得这么一个不仅很闲又啥事都做不了的境况。比如你一定会有过上午还在说“有空了一定去一趟本地的博物馆看看”，下午的时候就趴在床上不知道要干啥的情况。这种在我们中太长见了，通常我们都是就这么趴在床上继续下去了。 以我个人来说，在主动管理自己时间的时候我一天下来有很多事要做，感觉自己很充实。那段时间里我玩游戏、看书、写代码甚至去一公里外的镇上吃一家好吃的包子都会列在我的计划中，严格按照计划行事。 后来大四实习的一年因为刚刚出来独自生活再加上刚开始工作觉的比较累，那一年基本就什么都没做。每天晚上对着电脑不知道在干吗，周末在家里也是完全懵逼的。都不说学习了，就连游戏和电影都没怎么碰过，所以将自己的时间管理起来指的当然不是说只于学习和提升自己，而是单纯的管理时间而已。把自己的时间管理起来了以后会发现自己的游戏通关更快了、看书效率更高了、电影看得更多了、好多计划过没有付诸时间的事情都从TODO中清理掉了。唯一边少的时间就是自己无聊发呆的时间，所以说来这是一门血赚的生意。换句话说周末两天趴在床上发呆玩手机是最咸鱼的，就算是起来看三部电影玩几个小时PS4都要比这发呆来的好吧。 最近这段时间又将时间管理起来了，才觉得自己的生活又充实了起来。不仅通关了几款早就想玩的游戏、看了好多部想看却没看的电影、就连好多本想看没看的书也都看完了。 0X01 日历方案 说是将自己的时间管理起来，那具体该怎么实施呢？最简单的一个方案就是日历了，几乎任何手机都带有日历和日程提醒功能，用它就可以了。每个周期定一个时间来安排下一个周期的日程。 比如每周日晚上睡前拿出半个到一个小时来安排接下来一周的任务，安排日程最重要的一点就是“时间精确”。当你把“参观省博物馆”的日程设置为“本周的周三到周五”，那么基本上就代表着这周是去不成博物馆了，因为八成会被拖到最后然后放弃掉。所以一定要精确到具体时间，比如“2018-11-25 14:00参观省博物馆”、“2018-11-25 10:00收拾屋子”这种。到了时间就去做该做的事，一定要有很高的自控力和执行力。 在系统日历中通常都可以给一个日程设置多个提醒，要利用好这一点。比如你计划下午要看电影，那就设置一个类似这样的日程“今天 15:00观看XX 电影，并添加一个提前半小时的提醒”。这样在14:30的时候你就会被提醒半小时后的看电影日程，便于安排此时手中正在做的事情。 0X02 Google日历 如果能够科学上网的话，建议Android用户使用Google日历。Google日历比一般系统自带的日历功能更加完善，而且还带有同步的功能。首先来说Google日历可以给不同的日程设置不同的颜色标签，我们可以使用颜色标签给这些日程分类或是区分他们的优先级和重要度。Google日历在登录了自己的账号后可以跟Chrome进行同步，在Chrome上安装一个Google日历的扩展之后能同步查看到手机上看到的日程，并且在对应的时间电脑也会弹出通知提示日程。 在Android上的Google日历体验是很不错的，建议有条件的同学都使用Google日历替换掉系统的日历功能。在IOS上我不清楚有没有Google日历可以用，不过Apple也有自己对应的方案呢。 0X03 Microsoft套件 微软套件中的outlook也是一个这样的工具，在邮件功能之外的日程管理也是很好用的。outlook可以从邮件中智能识别相关日程并导入（Gmail/Google日历也可以，Apple其实也可以）。如果是在Windows环境下使用outlook也是一个不错的选择，在Windows下微软的日程可以直接以系统通知的形式推送，在手机上安装一个outlook就可以实现手机与电脑的同步了。 0X04 Apple套件 Apple套件我是没用过的，毕竟没买过苹果的任何产品。不过IOS和Mac上同步日程还是完全没问题的，而且配合siri使用起来应该也是很方便的。 0X05 GTD软件 更好的选择是使用GTD软件，比如滴答清单这种，以滴答清单为例介绍一下这类软件。滴答清单是一个以任务为基础的时间管理软件，用户每次想到的任何东西都要在滴答清单中创建为一个任务，我们可以个任务指定“开始时间、提醒时间、优先级、评论备注、附件”等一系列属性。 我是在滴答清单中创建了多个目录，而且是支持多级目录的，每个目录下有着对应的任务，例如“工作内容：研发工作、工作会议、其他工作，个人提升：专业进阶、博客更新、业余领域、阅读进度，业余生活：个人生活、电影电视、兴趣爱好、社交活动”这样。 每次创建的任务默认放在一个叫做“收集箱”的地方，等一周下来收集箱里应该就有了好多任务，再找一个时间将收集箱里的任务一个个的配置时间并且分到对应的目录中。 而且滴答清单还集成了“番茄时间法”，这个就不多介绍了，可以搜索了解一下。对于我来说比较重要的是滴答清单集成的“统计总结”功能，每周最后的时候我都喜欢点开滴答清单的这个功能来看一下自己这周都完成了什么任务。也不是为了总结这一周我有多少多少的收获，单纯的是看着这些被划掉的任务觉得很爽，感觉特别有成就感。 0X06 最重要的一点 说了这么多，如果说这篇博文中最重要的一点，就是这段说到的“自控力”了。如果没有良好的“自控力”，就算用再好的工具，弹出再多的提醒，进行再周密的安排如果不去执行也是完全没有用的。 在这里希望所有看到这篇博文的同学都有不错的自控力 :) ","date":"2018-11-25","objectID":"/posts/time-manage/:0:0","tags":["Life"],"title":"如何将自己的时间管理起来","uri":"/posts/time-manage/"},{"categories":null,"content":"好多人的VPS或是服务器就只跑了一个小服务或是小程序，虽然我们多数人的VPS性能比较差但是只跑一个小服务一个小程序未免还是有些浪费。大可将其充分利用起来，毕竟性能再差也是一个24小时不停机且带有一个独立公网IP的机器。这里我介绍一下我跑在VPS上的一些服务和使用中的一些经验和建议，希望可以给各位带来些许的帮助吧。 0X00 操作系统 通常我们自己的VPS性能都不怎么强，很多人都是买的单核512M内存的，所以Windows Server系统通常是不用想了。那么众多的Linux发行版本又该如何选择呢？其实因为我们是个人使用，不用过分考虑发行版本的区别，哪个用着习惯就用哪个。我自己的VPS用过一段时间的Fedora和Debian，都是很不错的。如果是针对Linux新手的话我还是比较建议使用Ubuntu和CentOS，因为这两个系统的用户群体很大，出现问题时候在网络上也更容易找到相关的解决方案。 还有就是图形界面，Linux作为服务器操作系统时是完全没有必要安装图形界面的。想必购买了VPS的用户应该也多少能操作一下Linux，就更没有必要浪费系统资源和时间去安装图形界面和VNC了。 0X01 机场建设 大家如果购买的是国外的VPS那么多数情况下都是为了跨过长城，不过好多人在使用的都是基于命令行的Shadowsocks服务，不仅配置起来不太直观，分享给其他人用的时候也不好管理。有一个开源项目ignite可以部署一整套的Shadowsocks管理系统。这样你的VPS就不再是飞机跑道了，而是进化成了一个国际机场 :) 这是一个Go语言实现的基于Web的Shadowsocks管理工具。多个用户使用时会创建多个Docker容器，每个连接使用独立的环境与公网端口便于管理。 配置好后可以在admin端创建邀请码，每个邀请码对应着使用时间与可用的流量。将邀请码分发给其他人后，其他人登录到你的client端便可以使用激活码注册自己的用户，并且启动一个新的Shadowsocks连接。作为管理员可以实时观察各位用户的剩余时间和剩余流量，并且可以进行延长时间和重置流量等操作。 0X02 搭建博客 搭建博客也是一个常规操作了，通常的选择是两种：以Hexo为代表的静态博客和以Wordpress为代表的动态博客。这里更推荐使用Hexo，这也是处于对VPS性能的考虑。我们的VPS通常性能都不强劲，所以使用静态博客可以每次编辑完博文后编译上传，每次用户访问时也只是访问了几个静态文件，再加上Nginx的配合完全可以实现个人博客的需求。Wordpress类的动态博客虽然扩展性很强，评论和浏览量等常见功能的高度集成也很方便，不过问题在于博客的运行需要数据库来支持，这在我们的低性能VPS上通常会导致访问速度缓慢。 静态博客对比与动态博客还有一个好处就是非常便于迁移，针对静态博客只需要修改一下地址重新发布就算是完成了迁移；但是动态博客迁移不仅要重新配置PHP或是其他语言的运行环境，还需要备份并迁移数据库。 相比来说动态博客最大的优势就在于大量的插件与“动态”本身，很多比较大的网站也都是使用Wordpress搭建的。 0X03 自己的Github 众所周知Github上的私有仓库是收费的，国外的Gitlab与国内的gitee个人是可以使用免费私有仓库的，不过有时候为了折腾我们还是可以打造一个自己的私人Github。其中Gogs就是一个极易搭建的自助Git服务，可以简单的使用docker部署，甚至可以直接下载二进制文件运行。而且Gogs对系统配置要求极低，运行在VPS甚至是树梅派上都是没问题的。 部署好Gogs后也可以使用Web进行仓库与用户的管理，通过git命令进行版本控制push pull clone，就像Github一样。并且由于是国人主导开发的项目，整个系统与帮助文档等都是100%汉化的，对新手来说是相当有好的。 0X04 代理下载点 代理下载点就比较简单，每次遇到在国内网络环境中下载特别慢的文件，我都喜欢在VPS上下载好再从VPS上拖回来。在VPS上可以使用aria2来进行多线程下载，拖回本地的时候可以在VPS上长期启一个Nginx作为下载站，也可以在文件目录下通过命令python3 -m http.server临时启一个web服务，等文件拖回来了再关掉。 0X05 注意安全 毕竟我们的VPS都是处在公网环境下的，所以一定要注意安全。比如防火墙不要彻底关掉，用到哪个端口就开放哪个端口；ssh尽量使用公钥验证登录而非密码；对外开放的服务比如数据库或web端的密码不要使用弱密码。 一旦自己的VPS被拿下，轻则被删库跑路，重则拿着你的用户名密码到处撞库，搞不好就会有更多的损失。所以平时自己不同网站不同用户的密码也尽量区分开吧。 ","date":"2018-11-22","objectID":"/posts/using-vps-and-server/:0:0","tags":["Life","Linux"],"title":"把自己的 VPS/Server 用起来","uri":"/posts/using-vps-and-server/"},{"categories":null,"content":"0X00 前言 最开始使用Linux的时候一直搞不懂为什么Linux下会有这么多在命令行里操作文本的工具。一度以为这些玩意儿没啥大用，但是随着使用频率与强度的增加才发现这些东西是Linux上非常有魅力的部分。 0X01 cat cat命令是最简单的，cat filename就是将文件内容以文本的形式输出到命令行。这个命令主要是作为后面组合技的基础，威力到后面cat的作用才能发挥出来。 0X02 head head是查看文件的前面部分，默认情况下是前10行，可以使用-n参数指定具体的行数。如果你有一个文件巨大无比，比如说几个G，且你只关心前100行，那怎么搞呢。使用cat是不可能的了，毕竟你cat一下之后回去睡一觉屏幕都可能还没滚完，何况你还得翻回最上面查看开头的内容。 head filename -n 100 # 查看文件的前100行 0X03 tail tail类似于head命令，不过这个是用来查看文件最后的，同样支持使用-n参数来指定最后多少行。不过这个命令最多用于查看日志文件。例如你有一个日志文件每秒都在源源不断的写入新内容，此时又想看最新的日志怎么搞呢。tail提供了一个-f的参数，使用tail filename -f就可以输出文件最后的部分且每当新内容写入文件时会追加显示，直到你Ctrl + C为止。 这个技能也是后面组合技的基础技能之一，在组合技中更能发挥威力。 0X04 grep 查找文件中带有某些特征的行。一个例子：你想找到文件student_list.txt中所有带有“小明”的行，那就可以grep 小明 student_list。这样就可以看到这个文件中所有包含“小明”的行，并且“小明”二字通常会高亮输出，而不包含的则完全不会输出。 值得注意的一点是grep是使用正则表达式进行检索的，所以可以使用正则来匹配内容，所以grep的战斗力很强。比如我们可以使用grep配合正则表达式查找日志文件中所有符合我们要求的内容。 grep还有一种常用的方法，试想这样一个例子：你的一个目录里有大量的文件，你想找到包含db_password的行来修改刚刚更换的密码，但是已经忘了在几个地方配置过了，那该咋搞？可以在目录中使用grep -Rn \"db_password\" .来查找。这个命令可以递归的grep当前目录下所有的文件，找到含有db_password的行并带有文件名和行号。 grep是组合技中的神技，在grep的加持下组合技可以变得更强大。 0X05 ag 上面谈到的grep -Rn \"xxxxx\" .的方法固然好用，但是每次都写一便还是有些麻烦。所以有了这个命令ag，它可以完全替换掉这个grep -Rn \"xxx\" .的组合，不仅省时而且得到的输出也更人性化更好看。 通常系统不会自带ag命令，包含在一个名为the_silver_searcher的包中，如果找不到这个的话可以搜索安装一下。 0X06 jq jq是一个用于在命令行中解析输出Json内容并格式化高亮的工具，今天他才帮了我的大忙。jq命令可以将其中的字符串转码为人类可读的类型，并且进行缩进和高亮上色。 如果你有一个名为test_file.json的文件，内容为标准的Json，想要查看的话就可以使用jq . test_file.json命令来查看格式化并高亮之后的结果。jq工具也可以作为组合技的一部分提升工作效率。 0X07 组合技基础：管道 管道就是键盘上的那个小竖线|，在Linux中是一个非常重要的概念。简单的理解就是通过管道连接的两串命令可以将前面一串的输出作为后面一串的输入。 具体用法展示就在下面的组合技部分吧，直接在这里说也说不太清楚。 0X08 重定向 重定向类似于管道，只是重定向是将输出作为文件或使用文件作为输入。在命令中重定向是使用\u003c / \u003e这对大于好和小于号表示的。 echo \"hello,world\" \u003e new_file # 将本应该输出的hello,world写入到名为new_file的文件中 cat \u003c new_file # 将new_file的内容作为cat的传入参数 ","date":"2018-11-19","objectID":"/posts/linux-text-process-2/:0:0","tags":["Linux","Shell"],"title":"Linux上的简单文本处理","uri":"/posts/linux-text-process-2/"},{"categories":null,"content":"0X08-1 标准输出、标准错误、标准输入 系统中存在\"标准输出/标准输入/标准错误\"三个概念，通常标准输出与标准错误都是我们的终端，而标准输入是键盘。 一个命令的输出会分为“标准输出”与“标准错误”。顾名思义，通常的内容使用“标准输出”进行输出，“标准错误”是用来输出错误信息的。 直接使用 echo \"hello,world\" 的时候是将输出内容定向到“标准输出”的，默认是我们的终端。使用命令ping baidu.com \u003e output与ping baidu.com 1\u003e output是一样的，1\u003e代表标准输出，是默认的 使用命令ping zdlkfjwle \u003e output会发现还是有内容被打印到了终端上，是因为错误信息使用了“标准错误”来输出，想要重定向标准错误的输出就需要使用ping lkzsdjf 2\u003e output了 0X09 常用组合技 组合技才是本次的重点，这里介绍几种常用的组合技供大家参考。 追踪日志： tail -f nginx_xxx.log | grep timeout实时追踪在一个持续追加的日志文件，并且只展示其中包含timeout的行 追踪多个日志： tail -f nginx_success.log nginx_error.log | grep submit实时追踪多个文件，并展示其中包含submit的行 格式化一个Json文件： cat old_file.json | jq \u003e new_file.json将old_file.json格式化并生成一个新文件new_file.json 找到记不起来ip的服务器： history | grep ssh将你的命令行历史中所有包含ssh的找出来 查找一些进程： ps aux | grep java查找你的java进程 给文件行排序： cat student.txt | sort \u003e new_student.txt给一个名为student.txt的文件排序并生成一个新的 假装模糊搜索： find . | grep xxx | grep yyy | grep zzz你忘了文件在哪，文件名也记不全，可以用这种方式。在母目录执行，相当于找到所有当前目录下的文件中同时包含xxx和yyy和zzz的 ","date":"2018-11-19","objectID":"/posts/linux-text-process-2/:1:0","tags":["Linux","Shell"],"title":"Linux上的简单文本处理","uri":"/posts/linux-text-process-2/"},{"categories":null,"content":"0X00 install 安装nose：pip install nose 安装mock：pip install mock Python3 中mock模块已成为标准库，无需单安装 在任意目录下执行nosetests看到有输出就是已经安装好了nose。进入到Python shell中执行import mock没有报错也就是mock安装好了。 0X01 用于测试的代码 这里先贴出这次被测的代码simple_math.py，是一个非常简单的数字计算类。 #!/usr/bin/env python # coding=utf-8 class MyMath: def my_add(self, a, b): return a + b def my_subtraction(self, a, b): if a and b: return a - b 这里的代码是有问题的，毕竟是要拿来作为单元测试的样例的嘛。 0X02 编写单元测试 我们要针对上述文件创建一个新的test.py来测试其中的MyMath类。 #!/usr/bin/env python # coding=utf-8 from simple_math import MyMath mt = MyMath() def test_add(): res = mt.my_add(3, 5) assert res == 8 def test_subtraction_001(): res = mt.my_subtraction(233, 0) assert res == 233 def test_subtraction_002(): res = mt.my_subtraction(233, 11) assert res == 222 从代码中可以看到首先导入了需要测试的类MyMath，然后就写了几个test_开头的方法，方法内部是模拟调用MyMath中的方法，并将得到的结果与预期结果相互匹配，最终使用assert语法来判断是否返回了理想的值。测试代码写好之后在当前目录下执行nosetest -v来运行我们的单元测试吧，输出结果如下。 test.test_add ... ok test.test_subtraction_001 ... FAIL test.test_subtraction_002 ... ok ====================================================================== FAIL: test.test_subtraction_001 ---------------------------------------------------------------------- Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/nose/case.py\", line 197, in runTest self.test(*self.arg) File \"/home/shawn/Workstadion/utils_test/test.py\", line 17, in test_subtraction_001 assert res == 233 AssertionError ---------------------------------------------------------------------- Ran 3 tests in 0.002s FAILED (failures=1) 0X003 简单解释一下 上面的测试就执行完了，那我们来拆分一下这个单元测试的流程吧。 首先我们执行了nosetest -v命令，这个命令就是开始进行单元测试的，其中-v参数是用来展示更加丰富的输出的，如果不加这个参数的话运行结果会更清爽一些 执行命令之后nose会自己查找当前目录下名为test.py或其他以test_开头的python文件，并且执行这些文件中编写好的以test_开头的方法（就比如我们代码中的test_add/test_subtraction） 逐个执行上面找到的方法 输出最后结果 如果在执行单个test_xxx方法的时候没有抛出异常，那么就认为这个测试(test case)是通过了的，如果抛出了异常则认为此处的测试不能通过。但是在单元测试中有一点与普通Python程序不同，普通Python程序遇到抛出的异常时就会层层上抛，而这里会收集展示出来，然后继续运行下面的测试。 0X04 换一个姿势写测试 还是上面的被测代码，这次不是单纯使用多个方法来完成测试了。面向对象的思想也可以对应的放到单元测试中，比如我们针对MyMath类搞一个单元测试，这些测试内容也整理为一个类，不过值得注意的是**这里的类名一定要为TestClass**否则是运行不到的。 #!/usr/bin/env python # coding=utf-8 from simple_math import MyMath class TestClass: def test_add(self): mt = MyMath() res = mt.my_add(3, 5) assert res == 8 def test_subtraction_001(self): mt = MyMath() res = mt.my_subtraction(233, 0) assert res == 233 def test_subtraction_002(self): mt = MyMath() res = mt.my_subtraction(233, 11) assert res == 222 0X05 setup与teardown方法 上面的测试中我们每一个test_case中都有一行mt = MyMath()是不是感觉有些蠢，其实是可以避免这个方法的。定义两个方法setup/teardown，这些方法在每执行一个testcase的时候都会执行，不同的是setup执行在testcase之前，而teardown执行在testcase之后。下面例子中就是这样的，每次使用mt对象时都实例化一个新的，用完再删掉。 #!/usr/bin/env python # coding=utf-8 from simple_math import MyMath class TestClass: def setup(self): self.mt = MyMath() def teardown(self): self.mt = None def test_add(self): res = self.mt.my_add(3, 5) assert res == 8 def test_subtraction_001(self): res = self.mt.my_subtraction(233, 0) assert res == 233 def test_subtraction_002(self): res = self.mt.my_subtraction(233, 11) assert res == 222 虽然上面的mt对象没有必要每次生成新的，但是很多情况下其实是需要我们这么做的。考虑这么一种情况：有一个class Student需要测试，而且待测方法有非常非常多，不仅会计算Student实例的各个属性，还要对其进行更新、添加、删除等操作。那么这种情况下每次操作都生成一个新的Student实例并且在testcase结束之后删掉它就是非常有必要的了。 因为我们不能保证每个方法的幂等性，比如万一这个Student是男的，你在测试了student.change_sex()方法之后显然他就不再是男的了。那接下来再测试一些依赖与性别的地方时就会出问题，比如get_gender()方法你很有可能会写assert student.get_gender() == \"Male\"，那这个时候就出错了。 0X06 模拟一些对象 对，你没有对象的时候可以模拟对象出来(hhh。 还是，首先有一个场景：工作中编写单元测试，被测功能简单说是“从某一接口拿到数据，并对数据进行处理”。那么我们就可以写成这么一个操作 def test_operation_data_from_api(parms): response = get_data_from_api(parms) op_result = operation_data( id=response.id, order_type=response.order_type, administrator_name=response.administrator_name, ) assert op_result.status == 'success' 这段代码看起来是没有问题的，但是如果get_data_from_api调用的API是收费API呢，每次跑单元测试都要去请求一次吗？如果API巨慢无比，每次都要几秒钟才回得来，所有测试有需要调用上百次这个接口呢，我们就干等着吗？这显然是不合理的。此时就可以使用最初提到的mock来模拟数据从而解决上述问题。 import mock def test_operation_data_from_api(parms): response = mock.Mock( # Mock可以模拟几乎所有对象、方法等 id=3, order_type='cpu', administrator_name='root' )","date":"2018-11-03","objectID":"/posts/python-nose-mock/:0:0","tags":["Python","Test"],"title":"使用 nose 与 mock 对 Python 程序进行简单的单元测试","uri":"/posts/python-nose-mock/"},{"categories":null,"content":"平时使用Django REST framework的时候除了常用的几个字段类型之外其实没有哪些字段是必须的了，不过了解一下这些「非必需」的字段能给日常的编程任务带来大幅度的效率提升呢。 0X00 EmailField 首先是EmailField，这个字段本质上是CharField但是单纯的添加了一个完善的校验，可以免去我们手工编写正则和对应报错信息的过程，简单地定义之后就可以使用了。 user_email = EmailFIeld( min_length=5, max_length=30, allow_blank=False, ) 0X01 URLField 这里的URLField跟上面的EmailField类似，在CharField的本质上添加了对应的校验，使用时会校验该字段是否符合url的校验规则。 article_url = URLField( min_length=10, max_length=100, allow_blank=False, ) 0X02 IPAddressField IPAddressField也是一样，只是在CharField上添加校验。不过有一个针对ip的参数protocol，顾名思义，可以设定校验不同版本的ip地址。protocol参数的值从这三个中选一个both/IPv4/IPv6，其中默认选择的是both，也就是说同时会接受IPv4和IPv6两种地址。 source_ip4 = IPAddressField( required=True, protocol='IPv4', ) source_ip6 = IPAddressField( required=True, protocol='IPv6', ) 0X03 DecimalField 带有精度的 与FloatField不同的是，DecimalField会校验小数点位数，两个重要参数：max_digits数字总长度(不含小数点)，decimal_places小数点后几位。 # 不含小数点最长5位，精确到2位小数；也就是说最大为999.99 num_a = DecimalField( max_digits=5, decimal_places=2, ) # 不含小数点最长7位，精确到3位小数；也就是说最大为9999.999 num_b = DecimalField( max_digits=7, decimal_places=3, ) 0X04 DurationField 关于时间的字段用的最多的就是DateTimeField/DateField/TimeField了，但是这些都是时间点，而DurationField是时间段，就像Python中datetime.timedelta一样，使用的是[DD] [HH:[MM:]]ss[.uuuuuu]格式。 work_duration = DurationField( min_value=datetime.timedelta(days=12, hours=10, minutes=32, seconds=4, microseconds=3), max_value=datetime.timedelta(days=10, hours=1, minutes=2, seconds=4, microseconds=3), ) 0X05 ListField 列表类型，自身是一个列表，需要单独指定child的类型。create/update的时候需要提供类似这样的参数name_list = ['shawn', 'nwahs', 'shanw', 'sahwn']。 email_list = ListField( min_length=3, # 列表最少有3个元素 max_length=100, # 列表最多有100个元素 child=EmailField( # 列表中的元素为EmailField min_length=5, max_length=30, allow_blank=False, ) ) 0X06 DictField 与ListField很类似，DictField也需要单独指定child的类型，这里的child指的是dict的value，而key则默认为字符串。 email_list = DictField( child=EmailField( # dict中的元素为EmailField min_length=5, max_length=30, allow_blank=False, ) ) 0X07 HiddenField 有些字段需要用，但是又不需要用户提供，比如说某条数据的更新时间应该在更新数据的时候自动设置为当前时间，而非用户传递的值。这时就可以使用这个HiddenField 字段。这个字段对用户来说是隐形的，只需要自己在serializer中设定好就可以。 update_time = HiddenField(default=datetime.datetime.now()) 关于Django REST framework中Serializer fields更多的内容可以参考官方文档 ","date":"2018-10-13","objectID":"/posts/drf-fields/:0:0","tags":["Django","ORM","DRF"],"title":"Django REST framework 中不那么常用的 Fields","uri":"/posts/drf-fields/"},{"categories":null,"content":"上周参与了公司举办的针对应届毕业生员工的一次培训课程，讲到了职场新人需要注意的一些问题。每个人的收获都是不同的，针对我自己认为的重点总结一下也算是给两天的时间一个交代。 0X00 同理心与同情心 同情心和同理心我们大家多少都会听过，不过多数人可能都会像我一样没有思考过其中的区别。其实同情心大家都会有，比如你看到一个人流离失所，被家人朋友抛弃，身上破破烂烂蹲在马路边乞讨，多少都会有些同情他，比如给他一些零钱之类的。这能够算是同情心，但是同情心并不是帮助别人的良好方案。我们古人就有“嗟来之食”之说：一位乞讨的人在面对别人用脚踢来的食物时宁可饿死也不会吃一口。我们也都知道既然要给别人食物就不应该用脚踢过去。所以这个提供食物的人是具有同情心的，但是不具有同理心。同理心强调“感同身受”，这不是随随便便就能做到的。 具有同理心的人肯定不会用脚踢过去，可能会带着食物过去他旁边聊起天来。说一说自己以前窘困的时候也是饥不择食，真切的体会他此时的痛苦，在这种情况下大家才能互相坦诚的交流。 同事之间相处也是这样，一天你的同事说最近工作不顺利生活也不如意该怎么办呢？如果你冲上去说“人生不如意十有八九，早晚都会过去的”。听起来是在安慰他，但是实际上并没有用，甚至还会让他变得更丧更不开心。如果你能坐在他旁边分享分享自己工作不顺利的时候，再询问一下他的具体情况，最后再结合自己以前的经历给提供一些好的建议，他一定会好起来的。 这里不讨论“可怜之人必有可恨之处”等问题，也没有别的什么意思，只是举例而已 0X01 通过多个Why找到本质 有一个例子：有一天下午你同事买了一罐“死亡之翼”(一种高咖啡因含量的咖啡)，为了阻止下午工作的时候犯困。这个问题看起来很合理实际上如果我们尝试这通过几个“为什么”来找到问题的根本原因就可以更彻底的解决问题。 A: 为什么要买高因咖啡呢？ S: 因为下午会困「解决点1」 A: 为什么下午会困呢？ S: 因为晚上没睡好「解决点2」 A: 为什么晚上没睡好？ S: 因为三点钟才睡的「解决点3」 A: 为什么三点钟才睡的？ S: 因为昨天下午喝了太多咖啡「解决点4」 A: 为什么喝了太多咖啡？ S: 因为下午太困 看似像是一个智障一样的对话，如果我们从不同的地方着手解决就会得到不同的方案。当然这里只是一个例子哈，其实是真的挺智障的，哈哈。在通常逻辑下我们会因为下午困就去买一杯咖啡，但是如果多问一个“为什么”从而得到一个新的答案，就可以着手从「解决点2」来解决问题。「解决点2」的情况下我们可能会尽可能改善睡眠质量，比如改造卧室隔音，更换床垫被褥枕头等。但是如果我们再一次深入下去得到「解决点3」，就可以得到一个“早睡”的解决方案。如果继续深入下去得到「解决点4」，其实就得到了最终的原因：这是一个恶性循环，可能是因为某一天的特殊原因导致休息不好-\u003e下午就喝了很多咖啡-\u003e导致晚上睡不着-\u003e白天犯困-\u003e下午更困-\u003e下午喝大量咖啡-\u003e晚上睡不好。。。所以我们只要克服下来一天的下午，晚上就能睡个好觉，第二天也能精神饱满的工作了。 这个问题虽然说起来比较简单，但是真正运用起来的时候并不是这么简单。就比如上面的例子，“为什么晚上没睡好？”之后就会有多个原因，每个原因再追问下去可能还会得到更多的原因，从而远离真实的原因。 0X02 我说明白了吗？你听懂了吗？ 如果有人跟你说话的时候一直在问“你听懂了吗？理解吗？明白了吗？”，可能你会觉得他把你当成智障了。谁长期面对这种对话都是会不太舒服的，如果有人给你讲一件事情的时候，遇到重点难点问的是“我说明白了嘛？我说的清楚嘛？”，就会舒服多了。所以在跟别人交流的时候尽量多说“我说明白了吗？”而非“你听懂了吗？”。 0X03 如何升职加薪 老板手下有十个毕业生，工资都是5K，大家的能力分布在“4~8K”之间，老板现在要涨薪了会主要涨谁的呢？肯定是优先选择能力更强的嘛，假设要给其中一个涨薪到8K，肯定会选择实力本来就已经价值8K的。毕竟把工资给价值5K的员工具有赌博性质，不知道他未来什么时候会有或者根本会不会有8K的能力。其实一看就知道了，只是我们平时可能没有想到这里。 “应该是薪资追能力，而不是能力追薪资” 0X04 事实与故事 几乎所有人都会有这么一个行为逻辑“看到/听到事实-\u003e讲故事-\u003e产生情绪-\u003e采取行动”。比如有一天你看到一个人躺在一辆奔驰面前，地上瓜果撒了一地，奔驰的司机站在车旁边指着躺在地上的人大吼大叫。一般会怎么想呢？“呀！奔驰把人撞倒了还不想赔钱！Woc这人真坏，气死老子了！这些有钱人仗势欺人没一个好东西！” 其实这样的思维模式完美的契合了上面的流程： 看到/听到事实：地上躺着人，奔驰司机在吼叫 讲故事：司机撞了人还不想赔钱 产生情绪：开始变得愤怒 开始行动：内心下了“有钱人没一个好东西”的定论 实际上事实可能完全不是这个样子，你看到的事实只有“某一时刻奔驰司机正在对躺在地上的人大吼大叫”。你不知道这个人是碰磁的还是真的车祸受害者，甚至不知道瓜果是不是他的，奔驰车是不是“司机”的也不好说。 我们很容易就被上面的思维逻辑干扰，动不动就根据自己看到听到的事实去揣测一个“故事”出来并且对自己揣测出来的故事深信不疑。所以当发生这种事情的时候要限制住自己创造故事的能力，最起码要阻止自己讲“故事”变成“事实”。 ","date":"2018-09-19","objectID":"/posts/work-beginner/:0:0","tags":["Life","Other","Work"],"title":"职场新人培训总结","uri":"/posts/work-beginner/"},{"categories":null,"content":"0X00 When and Why 我是大三下学期即将结束的时候出来实习了，故事说来比较诡异。一天下午像往常一样背着包到教室准备上小学期的课程（小学期就是为期一个月的综合实践），一个朋友跑过来悄悄跟我说“hey 大家都在找实习了，你怎么还不着手找哇”。当时一听我就慌了，很慌的那种慌，然后那天下午把之前准备的简历投了出去。毕竟学期即将结束，其实我是想要回家过一个暑假的，所以我也就没打算真的找到实习，只是说尝试着面试一下赚一些面试经验。那既然目标是赚取面试经验就只投大厂呗，小公司面试经历的用处可能没有那么大。当天下午我就只投递了“知道创宇/知乎/陌陌”三家，结果还没下午课程还没结束呢就收到了创宇的面试通知。另外两家其中一家拒绝了我，另一家一直没有回馈信息。 后来面试还算比较顺利，拿到了offer后就咬牙放弃了最后一个暑假回家的机会，退掉了好不容易抢到的火车票，在2017-07-24就正式入职创宇的实习生岗了。所以说我几乎是被朋友忽悠出来实习的，再加上运气好拿到了创宇的offer，要不然我还要回家过暑假呢，哈哈 0X01 面试\u0026笔试 面试当天到了公司，在前台登记后给了我一份笔试题，内容是真的记不起来了，不过都比较简单。多是关于Python语言的一些问题和Linux系统操作的一些问题还有一些基础的SQL，没有问到关于我应聘岗位使用的Django框架。笔试题答好后我现在的部门总监就来开始面试了，除了问我笔试题的答题思路以外还问了些我搞不懂为什么问的问题。比如问了我“栈溢出攻击”，然而我并不懂安全而且部门也不是安全产品部门，所以我并没有答出来，后来还是面试官给我科普了一下“栈溢出攻击”是什么。我记得后面问的最具技术性的问题就是我学过什么Web框架，我说了解过一点点Flask后面试官问了我为什么选择优先学习Flask。在此之后就是“会不会用梯子？梯子是买的还是哪里的？会不会用Linux？用的什么浏览器？”这类更偏向“程序员文化”的东西了。最后面试结束的时候问了我简历上的个人博客和Github有没有在更新，我说在更新后面试就差不多结束了。 因为互相加了QQ，在入职之前还给我说过感觉我Github上项目的代码比较整洁，挺合格的。其实当时我Github上就一个小项目，很小很小的那种，所以说Github在求职过程中还是非常非常有用的。 0X02 入职时的水平 当时刚刚入职的时候，部门的技术栈几乎是全不懂（全不而非全部，是真的不会）。只有了解过一点点docker/flask/操作系统/网络等知识，对Linux的使用和操作比较熟悉，Python只有在语言上比较熟悉，而项目中大量使用的框架Django和git工具是从来没有用过的。 现在想来，其实不管什么语言什么框架的公司，熟练掌握”操作系统、网络、Linux、git“等这种通用的理论和技能都是绝对没有错的。所以大家不知道学什么的时候就可以去学习这些通用知识，任何时候学习通用知识和技能都是绝对不会错误的。 0X03 实习一年在干嘛 从最初入职的时候写单元测试开始，因为写单元测试可以比较快速的了解项目的业务逻辑，也会看大量的业务代码，是新手上手一个成熟项目的良好办法。之后就从单元测试-\u003e小功能-\u003e从0到1的小项目-\u003e几个小项目的主要维护人。现在就是在负责几个比较小的项目，感觉虽然没有扛起重担，但是起码能够独立担任一些小项目的各项适宜了。 这一年里也逐步的学习了更多的东西，目前来说工具上的git和docker已经和其他同事差不多同水平了，最重要的Django部分也是一直在变得更好。最近因为前端人手不够用，还开始接手一些前后端绑定的任务，着手写一些功能简单的Vue。 当然了，作为一个实习生中途肯定多少会有一些做零活的时候，比如搞一些Excel之类的。不过公司这点还是很棒，招实习生的目的就是未来成为正式员工，所以没有给安排很多零活，大多都是符合当前定位且对转正后工作又帮助的任务。 0X04 创宇真的好？ 最初面试的时候，我的面试官给我说”我们部门不加班的“。其实当时我是不信的，不过后来才发现是真的，实习整整一年后我提的加班都不到16小时（两个工作日）。这无疑是一个很大的诱惑吧，一家程序员不加班的公司谁不喜欢呢。 还有就是公司倡导的扁平化管理，虽然领导就在同一个办公室，或者就坐在你隔壁你对面，但是大家完全不会拘谨。互相之间只会喊X哥或者其他绰号，即使是上级叫下级也是直接叫X哥或者绰号，很少直接叫名字的。而且上级安排工作也是会真正的考虑到大家的感受，并不会强行安排工作到头上。 公司还会给提供培训的机会，比如参与各种相关交流会、分享会等，各种会议的入场费都是可以报销的。而且公司也会时不时举行内部的培训，很多时候都是针对技术的而非一味强调战略、管理、文化这些。上周才刚刚参与了公司专门针对应届毕业生的培训。整整两天的时间，公司的销售总监、人事行政总监等高管专程从北京飞到成都来给我们上课。 这还不是我对公司印象最好的地方，我觉得创宇最棒的地方在于你的同事会像学校里的老师一样给予指导。我的mentor曾多次站在我身后给我讲代码和业务逻辑超过半个小时，从来不会说因为自己没空就不提供指导，更不会因为自己没有好处拿就找借口不管。最近我在接触Vue的时候也是，前端同事多次站在我身后给我讲前端需要注意的东西，给我说需要看哪个教程，甚至会给我布置作业。这种细致入微的指导恐怕很多学校的老师都做不到吧。有着这么好的条件，不仅能够学到东西还能拿到不错的工资，能不喜欢才怪呢。 0X05 谁都有挫折 当然了，工作不会是一帆风顺的。最初开始做一些功能性任务的时候，受到挫折最大的一次，自己提交的一个commit总共只有200行左右的代码，在Review的过程给打回了80多90个修改意见，也就意味着每两三行就会有一个问题。那段时间写完代码都不敢提PR上去，提了PR上去之后整个人都是提心吊胆的。 在对语言和框架熟悉一些后，代码Review上的问题越来越少了，写代码也就快了起来。不过这样一来Bug也就随之出现了，最多的时候一个月出现过两次轻微的线上bug和一次小的生产事故。所以后来每次发布带有我代码的版本时，整个人也都是虚的，总觉的马上就会爆发线上BUG甚至是生产事故。 不管是之前的Code Review恐慌还是后面的BUG恐慌，都曾经有过”要不离职吧“的想法。后来想了想，虽然被项目经理叫出去谈过话，但是也只是让我再多注意一些，并没有很暴躁或是有让我走人的意思。所以应该还没有到走人的程度，毕竟作为实习生公司是可以随便让我走人的。那段时间我就一直在调整状态，也一直在学习，更要随时注意细节，防止再次发生问题。在经过这些事情之后，现在感觉比以前好了一些了，起码不会觉得自己要离职了。谁的职业生涯会是一帆风顺的呢，幸好我坚持过了那段压力很大的时候，感谢自己吧。 0X06 目前水平 这一年实习下来，在Python的语言和Linux上没有明显的进步，毕竟Python常用到的地方也就这些，那些高级用法用到了再去查一查也无可厚非，而Linux的卓面版也整整用了一年了，虽然没有太多运维技术上的提升但是起码用的更加熟悉了，也找到了一些适合我使用习惯的软件来支持我。 一年来进步比较大的就是Django和两个工具：Docker和git了。Django从入职时候的0基础，到现在可以独立负责一些小项目，应该也算是入门了吧。Docker和git两个工具平时用的很多但是却用不到比较深入的功能，所以已经和其他同事的水平差不多齐平了。 最近两个月以来，自从接触了SpaceVim之后使用Vim的比例大幅度提高，Vim的技能比原来高了好多，哈哈哈 0X07 未来的计划点 目前技术上最重要的就是Django/DRF框架和前端的Vue。Django/DRF再多学多用一些，争取尽快达到其他同事的水平和大家追平吧；Vue就慢慢来，争取能够自己独立完成一些前后端数据对接和简单逻辑的工作。 还有比较重要的一点就是提升自己的软实力，其实好多时候去分析遇到的问题都不是直接的技术不足导致的，所以时刻注意提升自己的专注力、思维发散能力、产品思维也是非常重要的呢。 ","date":"2018-09-18","objectID":"/posts/practice-year/:0:0","tags":["Life"],"title":"作为实习生的一年","uri":"/posts/practice-year/"},{"categories":null,"content":"为什么这里说是\"非入门级\"用法呢，因为我个人觉得这是我接触Django之后一段时间才开始了解的用法，但是说是高级用法又太夸张了，所以用了这么一个诡异的”非入门级“的定位。 下面的示例中使用下面的model，简单描述一下并非真实代码 from django.db import models class Staff(models.Model): name = models.CharField(max_length=10) age = models.IntegerField() class Order(models.Model): staff = models.ForeignKey( # 订单负责人 'Staff', null=True, on_delete=models.SET_NULL, ) price = models.IntegerField() # 订单的价值 0X00 使用Avg()/Sum()/Count()/Max()/Min() 这些方法的用法很简单，顾名思义。不过需要配合下面介绍的annotate()或aggregate()使用。 from django.db.models import Avg, Sum, Count, Max, Min name function Avg 求平均数 Sum 求和 Count 计数 Max 求最大 Min 求最小 0X01 使用annotate() 最基础的查询就是从一张表中查询符合某条件的字段，而使用annotate()可以得到一些我们手动计算得到的值，并将其作为Queryset中Item的一个属性来调用。 如果我想要查询每个人(Staff)手下有多少个订单(Order)，那么该怎么查呢，使用初级的用法可以写出类似下面的代码。但是有一个比较严肃的问题：会产生用户数量 + 1次的查询。这里只有少数用户问题不大，如果有上千甚至上万个用户，那么就会产生几千几万次查询，那对数据库的压力是很恐怖的。 staff_list = Staff.objects.filter() for staff in staff_list: order_count = Order.objects.filter(staff=staff).count() 使用annotate()方法就可以有效解决这个问题 staff_queryset = Staff.objects.filter().annotate(staff_order_count=Count('order')) for staff in staff_queryset: print(staff.staff_order_count) 这里的staff_order_count字段是表中并不存在的，是通过annotate()方法临时存储的一个字段。同样的，再一个annotate()方法中可以加入多个参数，使用同样的方法去统计和获取数据即可。 在annotate()中使用Count()一定要是有外键关联才行。例如本例中，Order表中有一个外键字段staff与表Staff关联起来了，那么就可以在Django中通过Staff.order_set来获取关联到Staff的Order，所以也就可以使用Count()方法来进行统计了。 生成查询的SQL语句也打出来方便理解。 SELECT \"Post_staff\".\"id\", \"Post_staff\".\"name\", \"Post_staff\".\"age\", COUNT(\"Post_order\".\"id\") AS \"x\" FROM \"Post_staff\" LEFT OUTER JOIN \"Post_order\" ON (\"Post_staff\".\"id\" = \"Post_order\".\"staff_id\") GROUP BY \"Post_staff\".\"id\", \"Post_staff\".\"name\", \"Post_staff\".\"age\" 0X02 使用aggregate() 使用aggregate()可以使得查询的返回值由一个Queryset变成一个dict，每个key和对应的value由自己计算得到。 如果我需要计算出所有人中年龄最大、最小、平均值该怎么办？初级用法可能需要先用一个查询得出所有人的年龄，然后再单独去计算最大最小平均值。写出类似如下代码，虽然目前问题不大，不过当逻辑复杂起来之后就会难以理解并且代码量较大。 queryset = Staff.objects.filter() age_list = [staff.age for staff in queryset] age_sum = sum(age_list) age_max = max(age_list) age_min = min(age_list) age_avg = (sum(age_list) * 1.0) / len(age_list) 但是如果使用aggregate()方法写出不仅逻辑清晰不易出错，而且代码量少了很多，更简单易读。 Staff.objects.filter().aggregate( age_sum=Sum('age'), age_max=Max('age'), age_min=Min('age'), age_avg=Avg('age'), ) 这段代码就会直接输出如下dict，需要的数据直接取即可。 { \"age_sum\": 71, \"age_max\": 30, \"age_min\": 19, \"age_avg\": 23.666666666666668 } 0X03 使用Case/When Django中的Case()/When()是非常实用一对方法，恰当使用可以大幅度减小统计功能的代码量、逻辑复杂度等。 假设有如下需求”年龄小于18的为未成年(1)，年龄在19到30之间的为青年(2)，年龄在31 到60的为中年(3)，其他为老年(0)“，那么使用Case/When方法再配合annotate()方法就可以优雅得实现功能。 Staff.objects.filter().annotate( age_tag = Case( When( age__lt=18, then=1， )， When( age__lt=30, then=2, ) When( age__lt=60, then=3, ) default=0, output_field=IntegerField(), ) ) # 上面的代码类似于这种 age_tag = 1 if age \u003c 18 else 2 if age \u003c 30 else 3 if age \u003c 60 else 0 # emm...这种更形象一点 if age \u003c 18: age_tag = 1 elif age \u003c 30: age_tag = 2 elif age \u003c 60: age_tag = 3 else: age_tag = 0 上面是计算一个QueyrSet中每一个item的情况，还有一种情况是统计一个model中所有数据，例如这个需求：”统计所有Order中，单价最高、最低和平均值“。使用Case()/When()也可以完成任务，并且比初级用法更好一些。 Order.objects.filter().aggregate( max_price=Max('price'), min_price=Min('price'), avg_price=Avg('price'), ) 内容整理的有点差，各位发现了什么疏漏和错误请及时联系我，防止误导别人。如果文章对大家带来了帮助，那我还是很开心的 嘿嘿 ","date":"2018-09-06","objectID":"/posts/django-non_beginner-level-usage/:0:0","tags":["Django","ORM"],"title":"Django 中的一些非入门级用法","uri":"/posts/django-non_beginner-level-usage/"},{"categories":null,"content":"自从一咬牙购入了一台Synology之后感觉网络和存储的体验提升了很多。在这里总结一下购买理由，方便犹豫不决的朋友做个参考。 0X00 来自百度云盘的惊悚事件 国内云盘一波洗牌过后，常用的几家都已经GG了，仅有一家百度还存活着。不过百度云说起来是免费的，实际如果使用的比较多的话就会发现，如果不充值为会员的话，速度就会非常的慢。所以说百度云盘也几乎是一个收费的网盘，超级会员算下来要将近30元/月。 其实收费无可厚非，毕竟大家花钱享受服务嘛。不过直到一天，在网上看到了这样一件事： 一个人在自己的百度云盘中上传了大量自己拍摄的照片（纯一手自己拍摄的），过段时间后因为一些原因从百度云盘手动删除了。又过了不短的一段时间 ，该用户再次将自己的这些照片上传至百度云盘，发现居然所有图片都是使用 急速秒传 的方式上传的。那么就发现了一个问题，急速秒传使用的是文件摘要值对比后选择性上传的一个技术。那问题就来了，该用户自己纯一手拍摄的照片，在没有分享给任何人的情况下删除了，而且是过了不短的一段时间再次上传，居然是秒传。那就意味着百度的服务器上存储了该用户的这些数据，但是用户已经删除了这些数据。意味着什么呢？百度根本没有删除用户的数据，在用户删除之后也只是不展示给用户看了而已。 现在想想是不是不寒而栗？ 0X01 大家的带宽也没那么大 还有一个原因就是\"此时此刻大家的带宽还没有那么大\"。说是云时代已经到来了，可是我们的手机还是需要64G，128G乃至更大的存储，并不能完全将数据存储到云端；我们的PC也是动辄TB级，并不能在线观看真正的蓝光原盘；甚至你的PS4也需要更大的硬盘以存储更多的游戏，不能从服务器上加载游戏；这些问题的一个交汇点就在“带宽\u0026流量”上。所以对于有大量数据存储与分享需求的用户，还是可以在家里部署一台NAS，毕竟现在的电脑、手机都是1000M的网卡，理论可以达到100m/s的传输速度还是要远远高于我们的互联网带宽。而且当你要备份数据到云端的时候，由于我们大多数都是ADSL的网络，会导致上传速度远低于下载速度。 在我的网络环境下，如果我要备份一个5GB的文件到互联网的云上，没有一个半小时几乎是完成不了的，但是如果是使用内网的NAS的话，就可以在一分半以内完成。 0X02 多设备同步目录 如果家中有多台设备需要共享一些文件，比如照片/文档等。在Synology中可以轻松实现多台设备同步目录，你在一台设备上进行的改动会瞬间同步至所有设备。虽然很多公网云存储提供了这项服务，但是毕竟这是内网环境的，硬盘就在你家里，资料的私密性会很高。 0X03 无处不在的文件 现在的NAS都可以配置外部介入，以我用的Synology为例。如果自己家的宽带可以申请公网IP最好，如果不能也可以在Synology的控制面板中配置外部连接。不管你身在何处，只要能连接到网络就可以访问到自己的文件，而且不需要购买花生壳等服务，速度也还可以接受。 0X04 NFS/SMB Synology中提供了共享目录的多种协议，在Linux上我使用NFS来挂载我的目录，Windows环境下使用SMB也是可以的。使用NFS或SMB挂载目录后就可以将NAS中的共享目录当作自己的磁盘来访问了，非常方便。 0X05 git server Synology的套件中心中提供了git server，可以在家中部署一个私有git server。安全可靠。 0X06 docker 高级的Synology版本提供docker服务，有了docker再加上远程访问和端口转发后，你的Synology就几乎成为了一台真正的服务器了。 0X07 VideoStadion Synology中的VideoStadion套件可以分析你NAS中存储的电影，自动帮你整理、下载封面、下载影片介绍、下载字幕，简直就是一个家用的多媒体中心。虽然几乎每次看NAS中的电影都是使用本地播放器在播放，不过还是要承认Synology中的VideoStadion套件是很棒的,可能只是我不太喜欢用而已。 0X08 DownloadStadion 这个是我眼中的大杀器。假设你只有一台笔记本，如果你想要下载几部高清电影空了看，那么就只能在晚上的时候挂机下载，白天了再暂停带走去工作。拥有了Synology后可以把十几个种子、磁力链接、http地址丢在DownloadStadion中，设定好下载时间就不用再管他了。我的DownloadStadion就是每周一至周五的2:00到19:00进行数据下载，其他时间空闲。 而且配合远程操作和搜索功能，可以实现在任何地方掏出手机命令家里的Synology下载文件，等你到家文件早就下载好了。 0X09 其他优势 理论上一台HTPC经过一番配置部署也可以达到Synology的效果，不过Synology最大的特点是一台HTPC所不能比拟的。Synology非常省电，通常情况下它只有10W左右的功率；而且Synology的服务很全面，基本上你需要的功能都在了，不需要你过多折腾；还有就是专业的NAS会很稳定，持续运行很久都没有问题。 0X0A 劣势 Synology最大的优势就是方便省事，但是最大的劣势却是“麻烦费事”。因为并不是所有人都有足够的计算机基础，对于大多数没有计算机基础的人来说这东西还是有点复杂，而且大多数人也并不会有购买一台NAS的需求。 0X0B 一些替代方案 如果不想购买一台NAS的话也有一些替代品，据我所知不错的方案有这几个： 自建“黑群晖”（据说很麻烦，坑很多） OwnCloud/NextCloud等云存储程序 使用OpenNAS自建功能完善的NAS ","date":"2018-08-31","objectID":"/posts/why-you-need-nas/:0:0","tags":["NAS","Life"],"title":"你为什么需要一台 NAS","uri":"/posts/why-you-need-nas/"},{"categories":null,"content":"0X00 内容比较少，不分标题 我们对Django中的model进行查询时通常是某个字段和一个常量 对比，比如下面这种写法 Student.objects.filter(name='shawn') Student.objects.filter(age=233) Student.objects.filter(gender__in=('F', 'M')) 如果遇到高级的查询可能会使用Q()查询，不过也只是进行多个条件的查询 Student.objects.filter( Q(name='shawn') | Q(gender='M') ) Student.objects.filter( Q(gender='F') | ~Q(age=233) ) 这里是我的另一篇介绍Q()的博文。 但是如果有这样一个需求：”查询订单中结束时间和开始时间的间隔大于45分钟的“。那应该怎么办的？因为订单的开始时间和结束时间都是一个字段，我们需要对比同一条数据中的两个字段。这时候可以使用F()来查询。 import datetime from django.db.models import F from my_project.models.Order forty_five_minutes = datetime.timedelta(minutes=45) Order.objects.filter(end_time__gt=F('start_time') + forty_five_minutes) # 查询结束时间大于开始时间加上45分钟的订单 Order.objects.filter(operator__age__lte=F('client__age')) # 查询订单操作员年龄小于等于客户的（我也不知道有啥用，例子而已） one_second = datetime.timedelta(seconds=1) OldMan.objects.filter(age__gt=F('age') + one_second) # 查询年龄比自己年龄多1S的大人？哈哈哈 本段内容的官方文档：https://docs.djangoproject.com/en/2.1/ref/models/expressions/#f-expressions 本段内容的另一篇博客：https://www.cnblogs.com/liuq/p/5946803.html ","date":"2018-08-29","objectID":"/posts/django-f-query/:0:0","tags":["Django","ORM","Python"],"title":"Django 中的 F()","uri":"/posts/django-f-query/"},{"categories":null,"content":" Python一切皆对象 0X00 困扰我的一个问题 前两天在工作上遇到了个问题，说来很简单：我要在每天的固定时刻统计系统中当天产生的一些数据并且用邮件发送给指定的人，又考虑到了程序的可复用性(统计其他日期)我并没有把参数写死，而是将其默认为当天的日期并可以指定参数。很容易我就写出了类似下面的代码。Ps.伪代码，不要过分纠结。 def export_statistic(export_date=datetime.date.today()): result = get_statistic_for_day(export_date) sendmail('今日数据统计结果', result, receiver_list) 并且将其配置在Celery中，每晚执行，并且在得到了第一天的正确数据后默认程序正确了。第二晚虽然收到了统计数据的邮件，但是发现日期是前一天的。以为是Celery或是服务器时间同步问题或是缓存等导致的，但是在多次检查后没有发现这个问题的根本原因。故临时使用crontab去执行这个定时任务，但这并不是长久之计。 0X01 到底发生了什么 纠结问题所在的时候突然想到“会不会函数的默认值在函数初次初始化的时候生成好就不再变了？”故而使用下面这段代码来检验自己的推断。 #/usr/bin/env python # coding=utf-8 import time from datetime import datetime def test(date=datetime.now()): print date if __name__ == '__main__': for i in range(10): test() time.sleep(1) 果然输出的结果和我以前的设想不同，按照我以前的想法应该是输出的几个时间间隔为1s，但是结果却是每一行都相同（果然我的1s不见了）。 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 2018-08-27 23:04:53.008236 0X02 那是为什么呢 Python一切皆对象 这句话看来真的不是说着玩儿的。其实Python中的一个函数也是一个对象，而对象就会有初始化的时候。Python的函数在作为对象进行初始化的时候就计算了“默认参数”。比如上面的例子，在函数def test(date=datetime.now())初始化的时候就已经计算了date=datetime.now()为2018-08-27 23:04:53.008326，所以以后每次调用的函数test默认参数都是这个值，有一个经典的案例可以参考。 #!/usr/bin/env python # coding=utf-8 def test(a, b=[]): b.append(a) print b if __name__ == '__main__': for i in range(10): test(i) 按照我之前的想法，输出的应该是[0], [1], [2]...这种，每次列表中只有一个元素，然而事实上是这样的。 [0] [0, 1] [0, 1, 2] [0, 1, 2, 3] [0, 1, 2, 3, 4] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3, 4, 5, 6] [0, 1, 2, 3, 4, 5, 6, 7] [0, 1, 2, 3, 4, 5, 6, 7, 8] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 我们来通过id()来检查一下原因。Python中的id(foo)会返回foo在内存中的唯一标识，稍作修改把代码改成如下 #!/usr/bin/env python # coding=utf-8 def test(a, b=[]): b.append(a) print id(b), b # 只是在这里新增了id(b)的输出 if __name__ == '__main__': for i in range(10): test(i) 我们可以看到，其实b=[]只执行了一次，每次使用的b并不是我们臆想中重新初始化的，而是一个现有的。 0X03 再次证实 自己写了几行代码来简单证明这个事情，按照我之前的臆想这个程序应该是看不到print输出的，因为看起来hello()并没有被调用到，但是其实呢？ #!/usr/bin/env python # coding=utf-8 def hello(a='hello'): print a return a def world(b=hello()): return b if __name__ == '__main__': pass 解释器在执行def hello(a='hello')的时候正常下去了，生成了一个hello()方法的函数对象，但是在执行到def world(b=hello())的时候也要将world()初始化为一个固定的对象，那么就势必执行了b=hello()。所以我们可以看到最终还是会有一个hello的输出。 1s不见了当然不是因为时间被转移走了-_- 文章参考自Python进阶-函数默认参数 珞樱缤纷-cnblogs ","date":"2018-08-27","objectID":"/posts/python-default-params/:0:0","tags":["Python"],"title":"关于 Python 函数默认值的小问题","uri":"/posts/python-default-params/"},{"categories":null,"content":"内容比较少，只是今晚翻看教程的时候发现的几个可以替换调我以前一些诡异操作的方法，将其整理贴出。 0X00 git blame 是谁在代码里下了毒？是谁用了一个超酷炫的方法解决了你解决不了的问题？当你想知道仓库中的某行代码是谁提交的，就可以使用这个方法。git blame hello.py可以看到hello.py这个文件所有行的提交人是谁，于何时提交的。而且这个命令最常用的是和grep合用，git blame hello.py | grep prinft(是谁写错了这个单词+_+) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 1) \u003cp align=\"center\"\u003e ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 2) \u003cimg src=\"https://s3.amazonaws.com/ohmyzsh/oh-my-zsh-logo.png\" alt=\"Oh My Zsh\"\u003e ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 3) \u003c/p\u003e ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 4) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 5) Oh My Zsh is an open source, community-driven framework for managing your [zsh](https://www.zsh.org/) configuration. ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 6) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 7) Sounds boring. Let's try again. ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 8) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 9) _Oh My Zsh will not make you a 10x developer...but you might feel like one.__ ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 10) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 11) Once installed, your terminal shell will become the talk of the town _or your money back!_ With each keystroke in your command prompt, you'll take advantage of the hundreds of powerful plugins and beautiful themes. Strangers will come up to you in cafés and ask you, _\"that is amazing! are you some sort of genius?\"_ ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 12) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 13) Finally, you'll begin to get the sort of attention that you have always felt you deserved. ...or maybe you'll use the time that you're saving to start flossing more often. 😬 ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 14) ^9544316 (Lars Schneider 2018-07-24 22:55:48 +0200 15) To learn more, visit [ohmyz.sh](https://ohmyz.sh) and follow [@ohmyzsh](https://twitter.com/ohmyzsh) on Twitter. 上面的例子是oh-my-zsh中README的部分输出 0X01 git commit –amend vim xxx.py git add . git commit -m \"naruto! sasuke!\" 如果你不小心把刚刚的commit写错了，现在还来得及后悔。 如果你刚刚commit，还没有进行新的改动，那么可以使用git commit --amend来修改上一次的commit。输入命令回车之后会打开你的编辑器，最上面的就是本次提交的commit message，动手修改之后保存就可以了。如果commit之后改动了很多才想起来那也可以先git add .再git stash，将改动先临时存起来再执行git commit --amend，修改好了commit message之后再用git stash pop把刚刚的改动给pop出来。 ps.在git里几乎一切都是来得及的，允许后悔药的存在。 0X02 git checkout – filename 不管出于什么原因，大家都有可能需要删除掉对某一个文件的改动。如果是所有的改动都需要删除，那么可以简单的git add .; git stash; git stash drop丢弃这些改动。如果改动是多个文件，但是只有一个文件需要回退，那就可以使用git checkout -- hello.py来删除hello.py的改动。 ","date":"2018-08-22","objectID":"/posts/git-simple-tips/:0:0","tags":["Git"],"title":"提升 git 新手效率的小技巧","uri":"/posts/git-simple-tips/"},{"categories":null,"content":"0X00 遇到了什么 我们使用git，绝大多数情况下都是大于等于一个人进行代码编辑，然后将自己的改动提交到github/gitlab/gogs等仓库，然后再通过pull request/merge request的方式进行代码合并。所以我们一般都是先从github上创建一个新的项目，然后按照向导在自己的本地git clone下来一个空项目，再提交代码上去；或者fork \u0026 clone的流程。 以前从来没有去想过github上的仓库是不是和我本地的相同，以至于今天第一次搭建自己的git服务时遇到了问题。我从服务器上一顿操作猛如虎mkdir xxx; cd xxx; git init; touch README.md; git add .; git commit -m \"init the repository\"，结果不小心成了二百五。因为这个仓库根本不能clone到我本地，经过一番搜索发现了git中仓库之间的关系没有这么简单。 0X01 git中一般的repository git中一般的repository通常有两个来源：git clone或者在git init。这种仓库通常是我们用来正常工作的仓库，我们的代码都在这里面。我们可以在仓库里尽情使用git add/rm/status/log等常见操作。也可以将代码push到origin/upstream等上游仓库，这类仓库是最常见的了。 仓库中不仅存有我们正在使用的代码，有两个文件.gitignore和.git。其中.gitignore自然不必多说，是用来判断哪些文件需要或不需要提交到repository中的；另一个.git就是git实现版本控制的重要文件了。我们对代码的改动、不同的分支、commit的变化都是在这个目录中存储的。 不建议直接动手修改.git目录中的任何文件，有可能会造成奇怪的问题而无法解决。 0X02 git中的bare repository 其实git中还有一种bare repository，这种仓库中文可以称之为裸仓库。这种仓库中没有我们正常使用的代码，也没有.gitignore和.git文件。不过可以把这种仓库理解成将.git目录下所有文件都拿到bare repository仓库的根目录了。bare repository存储不同分支与各个commit等与版本控制相关的数据，但是不会保存整整一份代码。 但是这种仓库可以使用git clone来clone仓库到本地，所以通常被当作共享仓库。例如你的团队没有在使用github或是gitlab等工具，那就可以在一台服务器上创建一个bare repository，然后大家均从此仓库clone代码，然后再一起提交至此以实现git的工作流。 要生成一个空的bare repository很简单，只需要在一个空目录中git init --bare就可以了，生成好之后就可以从该仓库clone代码了。 0X03 部署一个服务器上的repository 到这里部署一台合作用的服务器上的repository就变得容易多了。首先创建一个git使用的用户，再使用git用户创建对应的裸仓库，再从客户机clone到本地，接下来就可以正常使用commit/push/pull等操作了。 在服务器上创建一个git用户user add git 然后为其创建home目录mkdir /home/git 再修改所属人为gitchown git.git /home/git 切换成git用户并返回home目录su git; cd 在目录下创建一个空目录mkdir .ssh 在.ssh，目录中创建一个authorized_keys的文件，在里面填入自己的ssh公钥 在本地已经可以clone下来啦git clone ssh://git@xxx.xxx.xxx.xxx/xxx/xxx.git 为了安全可以修改git用户的默认shellusermod git -s /bin/git-shell ","date":"2018-08-19","objectID":"/posts/git-base-repository/:0:0","tags":["Git"],"title":"git 中的 bare repository","uri":"/posts/git-base-repository/"},{"categories":null,"content":"本来准备总结一下Linux桌面系统使用一年以来的一些感受，以及为什么选择Linux作为桌面系统工作学习的，但是构思了半个小时也没能想到要写些什么。所以还是来推荐一下一年以来在Linux桌面平台下的软件体验和推荐吧。由于我这一年多以来一直使用的是Fedora Workstadion，所以并不能保证这些软件能在其他平台下的体验与我一致，不过一般来说都是没有问题的呢。 0X00 vim + spacevim 首先说明我不认为Vim比IDE写代码更好用，但是我仍然在使用vim。一个原因是觉得使用盗版IDE有些不太道德，另一个也是想要提升一下自己的代码水平，毕竟vim给出的提示会更少一些。IDE用户可以在IDE上安装vim的操作插件，毕竟vim的操作方式还是能很大程度上提升效率的。 SpaceVim 呢是vim的一个插件打包配置集成之后的一个版本。由于它集成了大量精选插件和配置，所以用起来很舒服，就把他当成一个高级的文本编辑器来用就好的。spacevim还有官方的中文文档可以查阅，虽然学习成本相对高一些，但是带来效率和爽快感的提升可不是一点点的。 0X01 ulaunch ulaunch是一个快速启动器，按下快捷键后屏幕中间会出现一个搜索框，可以快捷打开软件和文件。配合一些插件可以实现在搜索框中翻译等工作，而且软件和插件都是Python写的可以轻松制作自己的插件。 0X02 plank plank是一个简单轻便的dock栏，在Fedora的官方仓库里就有的，直接使用包管理器就能一键安装。 0X03 audacity audacity是一个linux下的本地音乐播放器，简单好用，没有多余的复杂功能，推荐一波。 0X04 vlc vlc应该是Linux下最好的视频播放器之一了，各种格式的视频音频都不是问题。 0X04 filezilla 这是一款老牌的FTP工具，虽然大多数时候文件管理器已经可以满足我们对FTP的一些基本需求了，但是偶尔还是会有一些满足不了的东西，这时候filezilla就能发挥作用了，留一个有备无患嘛。 0X05 electronic wechat 鹅厂没有Linux下的微信，不过幸好有人制作了这个基于网页版微信的微信客户端，使用体验良好，推荐！ 0X06 electronic ssr shadowsocksR没的解释。对应的还有一个Shadowsocks-QT5也很棒，不过不支持SSR罢了，看需求选择。 0X07 zsh + ohmyzsh 命令行组合，再搭配上ohmyzsh的一些插件，不仅终端非常漂亮效率还非常高。说到zsh就不得不说fish，相对来说zsh是和bash高度兼容的，虽然比fish慢了一点，但我更喜欢与bash的高度兼容，根据喜好选择啦。 0X08 jq jq命令是用来格式化json输出的。比如你的curl -XGET xxxx和cat xxx.json的输出结果，通常都是没有格式化的，如果你安装了jq的话就可以cat xxx.json | jq把输出变成格式化的，而且带有高亮。注意哦，这个包在fedora中就直接叫做jq所以只需要dnf install jq就可以了，其他的发行版本自己找一下哈。 0X09 ag ag是又一个超短超好用的命令。有这样一个场景，你在代码库的根目录下，想要找几百几千个源码文件中的pdb该怎么办呢？熟悉linux的很容易写出grep -Rn \"pdb\" .，从而找到所有包含pdb的行。ag就是用来替换这条较长且经常使用的命令的，你只需要用ag pdb就可以实现同样的效果，而且输出结果还比grep的更美观更直接。 在fedora中这个包名是the_silver_searcher。 0X0A vnote 在github中搜索可以找到一个名为Vnote的项目，这个是我用过Linux下最好用的离线笔记软件了。支持加密、markdown、公式渲染、流程图渲染、多笔记本、多层目录等等特性。而且只需要跟自己的同步云盘配合一下就直接变成了一个云笔记，完美。 0X0B 结尾 Linux下的好软件并非只有这些，这些只是我在日常使用中发现的好用的工具。其中人人皆知的就没有再重复写进来，只写了一些不是很知名的或是奇怪用法的软件。如果各位有什么推荐的工具可以留言哈。 ","date":"2018-08-12","objectID":"/posts/linux-software-recommend-2/:0:0","tags":["Life","Linux"],"title":"Linux 下日常使用软件推荐","uri":"/posts/linux-software-recommend-2/"},{"categories":null,"content":"0X00 Sentry是什么 Sentry是一个统一收集整理程序异常错误的服务。如果你有一个程序在跑，并且配置了日志，那么可以轻松的找到程序出错的地方；甚至可以在报错后发邮件通知自己以便抓紧处理。但是如果你的团队有10个项目和50个人，并且这50个人并不是每人只负责一个项目，此时此刻该怎么办呢？难道为每个项目都配置很多人，并且在人员变动和项目变动的时候都去再修改吗？这样就未免有点傻了，Sentry就是用来做这个的。 你可以在Sentry上为每个项目创建一个Project用于收集项目的错误，再为每位成员创建用户，由用户去关注项目，就可以实现上述复杂的功能了。 0X01 如何部署 为了流程简洁，默认安装了docker和docker-compose。 # 将一个配置好的使用docker-compose部署Sentry给clone下来 git clone https://github.com/getsentry/onpremise.git cd onpremise # 创建所需的目录 mkdir -p data/{sentry,postgres} # 生成一个secret-key docker-compose run --rm web config generate-secret-key 上述命令执行完成后，输出的最后一行类似乱码的东西是我们所需的secret-key，将其复制粘贴之docker-compose.yml文件的SENTRY_SECRET_KEY后面，形似 SENTRY_SECRET_KEY: '*********************' SENTRY_MEMCACHED_HOST: memcached 然后初始化数据库docker-compose run --rm web upgrade，执行过程中会要求创建一个管理员账户，根据流程的提示操作既可。操作完成后执行docker-compose up -d就可以将整套Sentry服务启动起来了。此时如果想要访问Sentry的web服务需要访问ip:9000，可以按照下面的方法为其配置nginx的反向代理。 0X02 Nginx反向代理 向nxing配置的http模块内部添加下面的配置，并重新载入/重启Nginx后就可以使用域名访问Sentry了。注意将其中的xxxx.example.com改为自己解析了的域名。 server { listen 80; server_name xxxx.example.com; location / { proxy_pass http://127.0.0.1:9000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Host $remote_addr; } client_max_body_size 10m; } 到此为止Sentry就算是配置完成了。 0X03 集成到自己的程序中 在Sentry的web服务中创建一个新的Project，创建时需要选择自己程序的类型，比如是Django还是Flask或是其他，创建好后会有提示，按照提示就可以将Sentry集成到自己的程序中了。过程都非常简单，比如在纯Python程序中集成一个Sentry也就四五行的事情，而在Django中也不过10行代码。 0X04 配置发送邮件 此时的Sentry还只能接收错误，如果想要使其能够发送邮件，需要修改docker-compose.yml文件，找到几个以SENTRY_XXX开头的地方，在这里配置好自己需要使用的发件配置，例如 SENTRY_SERVER_EMAIL: sender@example.com SENTRY_EMAIL_HOST: smtp.example.com SENTRY_EMAIL_PASSWORD: examplepassword SENTRY_EMAIL_USER: sender@example.com SENTRY_EMAIL_PORT: 587 # 如果使用TLS加密且采用465端口失效时可以尝试将端口改为587试试 SENTRY_EMAIL_USE_TLS: 'True' 然后把生成的容器停止删除并重建一下，就可以了。 docker-compose stop docker-compose rm # 上面两个操作在新版本的docker-compose中可以使用docker-compose down来替代 docker-compose up -d 0XFFFF Done! 整体的大方面的配置就是这样了，具体的可以慢慢在web界面摸索一下，就这样啦 \\口-口/ ","date":"2018-08-08","objectID":"/posts/docker-deploy-sentry/:0:0","tags":["Docker","Sentry","Python"],"title":"使用 Docker 部署 Sentry 服务","uri":"/posts/docker-deploy-sentry/"},{"categories":null,"content":"0X00 什么是“事务” “事务”简单的说就是把一些数据库操作打包起来，要么就全部执行要么就全部不执行。 假设有一个操作是新建一个学生信息，有多张表分别记录了“基本信息、家庭信息、学校信息等“，那么就需要分成多步来新增这个学生的信息。但是如果在添加了”基本信息和家庭信息“两张表的内容后在添加”学校信息“时出现了错误那么数据库中就会存在该学生部分信息，从而使得数据库中的数据出现错误。 如果将这些操作放到一个”事务“中执行就可以在中途出现错误的时候所有数据库操作都不生效，当顺利执行完成之后使所有数据库操作都生效。总的来说就是”事务中出现错误则所有数据库操作都不生效，否则所有数据库操作均生效“。 具体可参见下面几个链接 Database transaction - Wikipedia MySQL Document 0X01 将事务绑定到http请求 在Django中实现数据库事务最简单明了的方法就是在数据库的配置中添加一个ATOMIC_REQUESTS=True。添加这个操作后Django会把每一个请求到响应的过程视为一个”事务“，从而在请求获得正确响应时应用这些数据库操作。如果在处理请求的过程中抛出了异常那么Django就会回滚这次事务。 ***** 注意： ** 此时的所有处理的所有请求都会被包装成事务！ 如果需要声明某些view不需要使用事务，那么可以通过使用如下方法阻止view使用事务 from django.db import transaction @transaction.non_atomic_requests def my_view(request): do_stuff() 而且事务会对数据库和Django造成更大的压力，所以此种方式并不是最好的使用事务的方法。 0X02 使用@transaction.atomic装饰器 我们使用事务通常是明确知道哪些方法需要使用，而不是像上面那种直接给所有view添加事务再一一排除那些不需要的。 所以Django中也有一种直接给某个方法标记为使用事务的方法：使用@transaction.atomic装饰器。例如下面的代码，如果你只需要给某个view使用事务或者只是需要给某个其他的方法使用事务，那么就可以使用这种方法来装饰function，从而使其在一个事务中。 from django.db import transaction @transaction.atomic def my_view(request): do_stuff() @transaction.atomic def some_func(): do_stuff() 0X03 使用with transaction.atomic:语句 使用@transaction.atomic装饰器是直接给某个function打包为一个事务，但是在某些情况下还会有更小粒度的事务需求，下面给出一个情况。 from django.db import transaction from my_project import models def some_func(): models.AAAAA.objects.filter().update(aaa='456') models.BBBBB.objects.filter().update(bbb='789') models.CCCCC.objects.filter().update(ccc='123') print '江XXX，亦XXX' '........' 如果这段代码在更新model：BBBBB的时候出现了错误，那么其实AAAAA是更新了的，且由于抛出了异常所以BBBBB和CCCCC都没有被更新。此时数据库中的数据就是不符合我们要求的。这个时候就可以使用with transaction.atomic的方法来将一个代码块打包为一个事务。 from django.db import transaction from my_project import models def some_func(): with transaction.atomic: models.AAAAA.objects.filter().update(aaa='456') models.BBBBB.objects.filter().update(bbb='789') models.CCCCC.objects.filter().update(ccc='123') print '江XXX，亦XXX' '........' 此时AAAAA/BBBBB/CCCCC这三个model的操作已经”同生同死“了。 参考自： Django中文文档 ","date":"2018-07-10","objectID":"/posts/django-transaction/:0:0","tags":["Django","ORM"],"title":"Django 中事务的三种简单用法","uri":"/posts/django-transaction/"},{"categories":null,"content":"0X00 Django Model中的空 Django的Model常见两个与空有关的参数：null和blank。其中null是数据库层面的是否允许为Null，而blank则将空处理为空值。比如一个CharField的blank=True，那么这个字段在没有赋值的情况下入库，这个字段就会是空字符串而不是Null。 In [13]: s = Student() In [14]: s.age=1 In [15]: s.save() In [16]: s.name Out[16]: u'' 如果将blank=False再不赋值该字段进行保存则入库的就是Null。 In [3]: s = Student() In [4]: s.age = 1 In [5]: s.save() In [6]: s.name In [7]: type(s.name) Out[7]: NoneType 所以换句话说，null=True是数据库层面允许存储Null，而blank=True则是允许存入\"空字符串\"等表示空的值。 0X01 Django REST framework中的空 在Django REST framework的serializer中的字段，有三个与空有关的，都是在创建或更新中生效。分别是allow_blank/allow_null/require这三个。其中allow_blank=True表示着CharField/ListField等允许传入\"\"/[]等空值；allow_null=True表示着允许传入{\"name\": null, \"age\": null}这种null空；require=True则表示着字段必填，如果name字段被设定了require=True那么在POST/PUT/PATCH等创建或更新数据时这个字段是必须要填写的。 ","date":"2018-05-23","objectID":"/posts/django-drf-null-and-blank/:0:0","tags":["Django","Python","DRF","ORM"],"title":"Django 与 Django REST framework 中的这些 \"空\"","uri":"/posts/django-drf-null-and-blank/"},{"categories":null,"content":"0X00 Model中要注意的几点 ","date":"2018-05-22","objectID":"/posts/drf-tips/:0:0","tags":["Django","DRF"],"title":"Django REST framework 中要注意的几个点","uri":"/posts/drf-tips/"},{"categories":null,"content":"verbose_name 和 help_text 属性 Model中通常第一个参数指定的是verbose_name，还要手动指定一个help_text属性。其中verbose_name属性是用来我们自己读的，而help_text是用于提供字段描述类的功能，比如在DJango Admin中verbose_name会变成字段的中文名，而help_text则会变成改字段的描述。 ","date":"2018-05-22","objectID":"/posts/drf-tips/:1:0","tags":["Django","DRF"],"title":"Django REST framework 中要注意的几个点","uri":"/posts/drf-tips/"},{"categories":null,"content":"unicode 方法 每一个Model类我们最好都要重写一下这个__unicode__方法，使之返回一个有意义的数据。比如一个学生信息的Model，我们不去重写这个方法，最后在ipython中或是项目中直接调的话就是这个样子的\u003cQuerySet \u003cStudent: 1\u003e, \u003cStudent: 2\u003e, \u003cStudent: 3\u003e]\u003e。如果我们重写了这个方法 def __unicode__(self): return '({gender}){name}'.format(gender=self.gender, name=self.name) 那么返回值就是``\u003cQuerySet \u003cStudent: (男)小明\u003e, \u003cStudent: (女)小红\u003e, \u003cStudent: (女)小兰d\u003e]\u003e`。不仅是在调试过程中还是程序里都会有不错的效果。 ","date":"2018-05-22","objectID":"/posts/drf-tips/:2:0","tags":["Django","DRF"],"title":"Django REST framework 中要注意的几个点","uri":"/posts/drf-tips/"},{"categories":null,"content":"关于choices 在设计Model中常会用到choices属性，比较好的用法是这样的。命名的时候使用在字段名后加choice的全大写，也就是：GENDER_CHOICE。 GENDER_CHOICE = ( ('male', u'男'), ('female', u'女'), ('other', u'其他'), ) gender = models.Charfield( u'性别', help_text=u'性别', max_length=100, choices=GENDER_CHOICE, ) 0X01 Serializer中要注意的几点 ","date":"2018-05-22","objectID":"/posts/drf-tips/:3:0","tags":["Django","DRF"],"title":"Django REST framework 中要注意的几个点","uri":"/posts/drf-tips/"},{"categories":null,"content":"针对list方法的Serializer 还是上面学生信息的这个例子，前端调用GET方法后想要得到的明显是男、女、未知这种，所以我们应该为所有类似的字段搭配返回一个对应的可读字段。例如在获取学生信息时可以这样写Serializer。 class ListStudentSerialzier(serializers.ModelSerializer): gender_cn = serializers.SerializerMethodField() def get_gender_cn(self, obj): return obj.get_gender_display() class Meta: model = Student fields = ( 'id', 'name', 'gender', 'gender_cn', 'birthday', 'hobbys', ) 在针对List的Serializer中添加字段gender_cn，顾名思义就是性别的中文，定义为serializers.SerializerMethodField()，定且在下面跌一个名为def get_gender_ch(self, obj)的方法，组装好所需要的数据返回就可以了。参数中的obj就是都应的实例化对象，在此处也就是一个Student对象。 最后要在Meta中的fields里加上这个字段。 另外，如果将Model中一个字段定义为CharField且Serializer处使用ListField进行校验存储的话，数据库中就会是类似\"[1, 2, 3, 4, 5]\"的“列表样子的字符串”。如果想让这种类型的字符串以一个正确的列表方式返回，例如字段hobbys，那么应该像gender_cn一样编写一个get方法，使用json.loads()的方式将“列表样子的字符串”转换为真正的列表返回。 ","date":"2018-05-22","objectID":"/posts/drf-tips/:4:0","tags":["Django","DRF"],"title":"Django REST framework 中要注意的几个点","uri":"/posts/drf-tips/"},{"categories":null,"content":"针对Create和Update的Serializer 针对Create和Update的Serializer是要向Django推数据的，所以需要注意字段的合法性校验。要注意Serializer与Model中字段类型的对应，例如Serializer中的ListFIeld其实就是Model中的CharField。 ","date":"2018-05-22","objectID":"/posts/drf-tips/:5:0","tags":["Django","DRF"],"title":"Django REST framework 中要注意的几个点","uri":"/posts/drf-tips/"},{"categories":null,"content":"0X00 普通的查询 from django.db.models import Q queryset.filter(Q(age=233)) # 找到233岁的人 queryset.filter(Q(name='shawn')) # 找到名为shawn的人 这种查询方式与普通的方式比起来没什么区别。 queryset.filter(age=233) queryset.filter(name='shawn') 0X01 AND from django.db.models import Q queryset.filter(Q(age__range=(18, 25), Q(gender='F'), Q(beautiful=True)) # 找到18到25岁的漂亮女生 把多个条件用逗号分割开就可以了，或者使用\u0026符分割开。 0X02 OR from django.db.models import Q queryset.filter(Q(gender='F') | Q(province=u'四川') | Q(name=u'王铁蛋')) 这里用|符号分割开筛选条件，最终筛选得到的是\"所有女生、四川人和叫王铁蛋的人\"，也就是说相当于分别筛选了这三个条件，最终取了并集。这种查询如果用普通方法进行查询就会很麻烦，可能要写成下面这样： queryset.filter(gender='f') | queryset.filter(province=u'province=u'四川') | queryset.filter(name=u'王铁蛋') 0X03 NOT 使用Not查询的方式就比较诡异了 from django.db.models import Q queryset.filter(~Q(gender='M')) # 找到非男生 0X04 组合技 有的时候经常需要查询同样的条件多次，这种方法就可以一次编写查询条件多次执行 import operator from django.db.models import Q q1 = reduce(operator.and_, Q(gender='F', age__range=(18, 25))) q2 = reduce(operator.or_, Q(gender='M', age__range(3, 5))) queryset.filter(q1) queryset.filter(q2) ","date":"2018-05-20","objectID":"/posts/django-q-query/:0:0","tags":["Django","Python","ORM"],"title":"使用 Django 中的 Q 对象查询","uri":"/posts/django-q-query/"},{"categories":null,"content":"在Linux下安装软件通常会使用包管理工具自动处理依赖问题，在Fedora下一般使用dnf包管理工具。一般我们会给自己的源设置为国内的镜像源，比如 https://mirrors.tuna.tsinghua.edu.cn https://mirrors.ustc.edu.cn https://mirrors.163.com 但是有的时候还是避免不了从国外源下载数据，这种情况下经常出现速度巨慢无比甚至会断开的情况。这种时候我们可以给dnf设置通过代理连接网络，这样一来下载速度就会快得多了。 sudo vim /etc/dnf/dnf.conf编辑dnf的配置文件，添加如下配置，保存后再执行dnf命令就可以使用代理的方式连接了。 proxy=socks5://127.0.0.1:1080 proxy_username=shawn proxy_password=shawn 需要注意的几点： 本示例只是针对我自己的电脑，如果你自己的Shadowsocks配置跟我的不同，请根据自己的配置自行修改（没有密码的可以不写后两项） 示例中使用了socks4://的协议，如果自己有其他方式的代理也可以使用，比如http://等 当不再使用的时候记得将配置注释掉，以防连接国内源也使用代理 ","date":"2018-03-14","objectID":"/posts/dnf-socks5-proxy/:0:0","tags":["Linux","Proxy"],"title":"Fedora 中 dnf 命令使用 Socks5 代理","uri":"/posts/dnf-socks5-proxy/"},{"categories":null,"content":"git stash save/apply/pop 在用git的时候经常会有需要临时切分支等操作，但是如果当前工作区进行了修改就不能直接切分支。这时候呢就得把当前的代码暂存起来，可以这么操作： git add . git stash 这样就吧上次commit到现在的修改都暂存起来了，可以使用git stash show来查看暂存区。我以前就是这样的，每次由两个或是两个以上的stash之后就蒙圈了，不知道那个stash做了哪些改变。虽然git stash show可以看到每个stash修改了哪些文件，但是还是不能准确的定位到自己需要的stash。 后来发现git stash后面还能继续接参数，这里得感谢git plugin for oh-my-zsh。当临时保存一些修改的时候可以这样：git stash save \"fix:xxxxx\"，有多个stash的时候也可以用git stash show来看到每个stash的备注，就方便多了。 git on  new_branch [$?] via simple took 2s ➜ git stash show stash@\\{0\\} stash@{0}: On new_branch: feature:xxxxx (38 seconds ago) stash@{3}: On new_branch: create new file named hello.py (4 days ago) stash@{1}: On new_branch: fix:xxxxx (53 seconds ago) stash@{4}: WIP on master: 75e1918 add a (7 weeks ago) stash@{2}: On new_branch: create file zzz (4 days ago) 其中git stash pop是应用一个stash，并删除这个stash。git stash apply是只应用不删除。 git on  new_branch [$] via simple ➜ git stash pop stash@\\{0\\} On branch new_branch Changes to be committed: (use \"git reset HEAD \u003cfile\u003e...\" to unstage) new file: a Dropped stash@{0} (163b1e2391b4c8bd792701ac4318d928e0e12556) git on  new_branch [$] via simple ➜ git stash apply stash@\\{1\\} On branch new_branch Changes to be committed: (use \"git reset HEAD \u003cfile\u003e...\" to unstage) new file: a 善用git可以大幅提升效率哦 ","date":"2018-03-04","objectID":"/posts/git-stash/:0:0","tags":["Git"],"title":"使用 git stash save 将暂存区命名","uri":"/posts/git-stash/"},{"categories":null,"content":"0X00 遇到了一个问题 前段时间自己的电脑重装了系统，然后公司内网的Docker hub出了点问题，没办法继续开发。后来经过一波Google找到了一个可以备份与恢复Image的方法，使用docker save / docker load命令。 0X01 备份与导入镜像 首先查看自己的镜像，然后找一个准备备份的镜像，找到他的IMAGE ID，假设选中的是ubuntu的镜像，那就使用dokcer save 0458a4468cbc --output ubuntu.tar就可以把ubuntu.tar文件拷贝到其他电脑上。 REPOSITORY TAG IMAGE ID CREATED SIZE nextcloud latest 9d7d01184cbf 9 days ago 593MB rabbitmq latest 72cee1616e73 2 weeks ago 127MB ubuntu latest 0458a4468cbc 2 weeks ago 112MB redis latest 861cc310cd91 3 weeks ago 107MB mysql 5.7.17 9546ca122d3a 10 months ago 407MB mongo 3.4.2 5bc602c0b7fe 10 months ago 360MB 到另外一台电脑上执行docker load --input ubuntu.tar就可以把这个镜像导入进去。 ","date":"2018-02-15","objectID":"/posts/save-load-docker-image/:0:0","tags":["Docker"],"title":"Docker 中备份与恢复镜像","uri":"/posts/save-load-docker-image/"},{"categories":null,"content":"tag：菜鸡程序员、游戏玩家、杂食系读者、户外萌新 菜鸡程序员 虽然最初从二年级家里就有了电脑，然后小学的时候就在拆主机装系统了。但是实际上高一的时候，受《电脑报》的影响才真的了解了「程序员」这个工作，觉得很酷，于是在某个午休时间我从学校「越狱」出去到县城里仅有的小新华书店买了本《C语言从入门到精通》，然后用教室里拿来放 PPT的电脑写下了我的第一行 hello, world。但是后面由于时间着实不够，即使书都翻烂了其实也没写过几行代码 🤣 直到上了大学才开始真正的学习编程，虽然现在已经工作了几年，但是见得多了就越来越觉得自己菜，经常说自己是：CRUD 工程师、API Caller、bug creator。 游戏玩家 从小就开始玩游戏，有幸体验了从 8bit 一路到 4k 光追的技术力提升。还记得小时候家里为了不让我玩游戏就把鼠标收走了，后来我学会用键盘打开街机模拟器然后打拳皇；把电脑电源线藏起来，我就学会了拔电饭锅线（据说很多人都会这个技能）；再后来给电脑设密码，我就学会了装双系统。 现在手上有一台 XBox Series S 和一台 Switch 是用来玩游戏的，还有一台女朋友的旧电脑可以偶尔拿来玩玩 PC 上的小游戏。听起来好像很幸福的样子，这么多游戏和设备可以玩，不过事实上拿出整块的时间来玩游戏却是一件很奢侈的事情。好多游戏买了之后一直没时间玩，只狼、巫师3、大表哥2、神界原罪2、黑荆棘角斗场、动森、群星等等等等都是已经买了但是只玩了一小会儿就先放下了的，更不要说 XGP 游戏库里那些茫茫多的游戏了。 不过话虽这么说，我还是偶尔会打开旷野之息去海拉鲁大陆转一转、还是会偶尔打开动森去没怎么建设的岛上看一看、也会偶尔打开马车8不开辅助跑几圈150CC的比赛、更会去极限竞速地平线里去开一开车库里那些一辈子都买不起的车 🫣 杂食系读者 喜欢看各种不同类别、比较离谱的各种书，拿几本今年读完的来举例子：《君主论》是一本教你如何做一个君主，里面讲了如何通知本土国民如何管理刚刚攻打下来的城池、《蛤蟆先生去看心理医生》通过一个小故事来帮助我们进行心理的自我疗愈、《李诞脱口秀工作手册》本是李诞写给自己公司内部的工作手册，讲述了如何做一个合格的优秀的脱口秀演员、《曾医生让你早知道》知名科普医生的一些医学科普知识点合集、《爱的艺术》经典的解读何谓爱和如何爱的书。 我自己读书的宗旨就是：平时打折了就买几本口碑好的或者感觉有趣的，无聊了就拿出来看看，毕竟看看书总应该比看集电视剧更有意义一点点🤏🏻，但是太硬核看不下去或者单纯懒得看了也就不强迫自己。自己有点像是在玩游戏，不知道下一个任务目标是什么又有点无聊的时候，就跑去郊外打怪升级挖矿采集。获得的经验和物资不一定有多大用处，起码肯定是对自己有益的。 户外萌新 从 2020 年 10 月份第一次出门徒步/登山，当时直接就跑到海拔 4000m 的巴朗山了（当然不登顶，要不也不止 4000m）。因为没啥经验又加上晕车、高反、大雾、迷路，导致那次徒步体验非常非常惨，但是也就种下了日后徒步的种子。后面陆陆续续又走过几条线路就慢慢喜欢上走在路上的感觉了，虽然身体是累的但是因为在山上可以什么都不想，大脑只需要思考下一步落在哪儿，还是非常棒的。 虽然到现在也跟着走了好多条线路了，但是爬升能力还是不太行 😢 每次走平路山路都觉得轻轻松松，但是只要一爬升，必定是被连续超车。还记得有一次在山间小路爬升的时候，跟在我后面的一个小盆友纠结了半天最后还是跟我说了：「叔叔，能让我先过去吗」 🤣 徒步/登山除了放空自己的另一大乐趣就是看风景了。说出来可能会被嘲笑，自己直到大学毕业（甚至说直到现在）都没有过一次真正意义上的旅行。而且因为家在北方沿海的大平原，以前从来没有见过川西的景色，所以现在每次出发还都会惊叹于各种美景。不论是山上的日出日落、满天星辰、云海翻腾还是茂密的竹林、巍峨的雪山都让我心向往之 🤓 博客历史 2015-03 在 CSDN 上注册了博客并写了第一篇博文 Vim的基础配置 2016-05 从腾讯云那里拿到了学生优惠，开始在服务器上搭建独立博客，选用Typecho 2016-11 重新安装博客系统，配置更安全的数据库和 http，修改主题 2017-03 正式启用 https 2017-09 迁移至 Github 并和 Typecho 同时更新 Github新博客 2017-10 弃用老的 Typecho 博客，正式迁移至 Github，且域名不变 2018-xx 忘记什么时候把博客迁移到 VPS 上并启用 HTTP/2 和 https 的了 2018-12 更换的新域名了programmer.work 2019-02 又换了域名，work 的域名权重太低，完全搜索不到just666.com 2019-08 按 hexo 字数统计插件的数据，我博客已经写了10W字了 2019-10 更新了这篇「关于我」的介绍 2020-06 博客文章超过 100 篇了 2020-06 再次更新了这篇「关于我」的介绍 2020-09 发现 next 更新到 8.0 了我还在用 5.x，隧更新 2021-06 本来想换成 Wordpress 来着，后来想了想有这时间不如写篇内容，所以放弃。但是还是更新了Next主题版本 2022-05 从海外的自建服务器迁移至腾讯云 web 托管（访问速度好快啊） 2022-07 再次更新「关于我」 2025-01 尝试将博客使用 Ghost 重新部署 ","date":"2017-12-31","objectID":"/about/:0:0","tags":null,"title":"about","uri":"/about/"},{"categories":null,"content":"0X00 enumerate是什么 enumerate()是一个Python自带的函数，用来同时遍历刻碟带对象和索引值． 0X01 enumerate怎么用 如果不在不使用enumerate()的情况下去除一个字符串列表中的字符串中的空格，那么通常会写出下面这种程序． #!/usr/bin/env python # -*- coding: utf-8 -*- my_list = [\" 搞个大 新闻\", \"我作 为一 个长者\", \"比 谁跑 的都快\", \"哪 个国家 我没 去过\", \"比你们不 知道 高到 哪里去了\", \"谈 笑风生 \", \"当然 支持 啊\", \"遵循 基本法 的 \"] index = 0 for item in my_list: my_list[index] = item.replace(' ', '') index = index + 1 可以看到光是处理空格都用了四行，而且还并不怎么优雅．那么可以使用enumerate()来修改一下这个程序． #!/usr/bin/env python # -*- coding: utf-8 -*- my_list = [\" 搞个大 新闻\", \"我作 为一 个长者\", \"比 谁跑 的都快\", \"哪 个国家 我没 去过\", \"比你们不 知道 高到 哪里去了\", \"谈 笑风生 \", \"当然 支持 啊\", \"遵循 基本法 的 \"] for index, item in enumerate(my_list): my_list[index] = item.replace(' ', '') 语法大概就是这for 索引, 对象 in enumerate(可迭代对象)．用起来不仅干净优雅而且可读性也更强了． ","date":"2017-12-24","objectID":"/posts/python-enumerate/:0:0","tags":["Python"],"title":"Python 中的 enumerate() 方法","uri":"/posts/python-enumerate/"},{"categories":null,"content":"我们在编写Python程序的时候会发现在我们的目录中可能会出现与源代码同名的pyc文件生成，比如有一个源码文件是hello.py那么可能会生成一个hello.pyc文件出来．这个pyc文件是Python的字节码文件，就类似于Java中的hello.class一样． Python虽然是解释性语言，但还是可以有一个编译的过程，只不过是编译成字节码文件罢了．如果我们的源码带有包含关系，比如源码a.py里面import了源码文件b.py，那么在执行python a.py的时候就会将b.py编译成b.pyc． 下次再运行python a.py的时候解释器会检查b.py与b.pyc的修改时间，如果一致就代表源码没有修改过，那么就可以直接调用b.pyc来更快的执行程序，如果时间不同那就证明b.py被修改过，则会重新编译b.py． 其中pyc文件的主要作用是用来加快程序执行速度的，虽然编译出来的pyc是二进制文件，不能看到内部的内容，但是还是不要把这种方式作为保护源码的方法．因为这种pyc文件是可以轻易被反编译的，有很多开源库可以轻松的反编译pyc文件，甚至都有web程序来反编译pyc文件，比如这个tool.lu就可以通过上传文件pyc文件的方式反编译． ","date":"2017-12-23","objectID":"/posts/python-pyc-simple/:0:0","tags":["Python"],"title":"Python 中的 pyc 文件","uri":"/posts/python-pyc-simple/"},{"categories":null,"content":"0X00 使用uwsgi启动Django 首先安装uwsgi，pip install uwsgi就可以装好．然后找到Django生成的wsgi.py文件，这文件通常实在与项目名同名的app目录下的，比如我的项目名为django_test那么这个文件应该就在django_test/wsgi.py．然后执行uwsgi --http 0.0.0.0:8080 --wsgi-file django_test/wssgi.py就可以用uwsgi启动你的Django项目了. Django自带的python manage.py runserver用于调试还是可以的，不过如果用于生产环境的不论是安全性还是性能都不足以满足生产环境的需要. 0X01 使用supervisor维持服务在线 我们知道由于Linux系统的进程管理机制，导致在bash里启动的程序如果退出bash就会自动被kill掉．所以我们需要一个能让进程一直活下去的方法，其中nohup是我们常用的方法，不过这种方法也只是能让进程活在后台罢了，平时跑个脚本或是其他小程序还可以，如果是部署一个服务的话就不合适了．这里我们选用supervisor来维持服务． 这个工具也是Python写的，所以可以使用pip安装pip install supervisor．然后我们可以在任意位置创建一个supervisor的配置文件django_test_supervisor.conf，启动supervissor的时候会指定这个文件，所以这个配置文件不像是其他配置那样全局使用．配置文件简单中的命令和日志需要自行修改，其中命令要修改成自己使用uwsgi启动Django的命令，日志位置要存在，不要没有那个目录． [unix_http_server] file=/tmp/supervisor.sock ; (the path to the socket file) [inet_http_server] ; inet (TCP) server disabled by default port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface) [rpcinterface:supervisor] supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface [supervisorctl] serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL for a unix socket [supervisord] nodaemon=false # 值为false时supervisor在后台运行 logfile=/data/log/supervisord.log # supervisor　的日志 pidfile=/data/supervisord.pid # supervisor　的日志 [program:django_test] command=uwsgi --http 0.0.0.0:8080 --chdir /home/shawn/django_test --wsgi-file django_test/wsgi.py ＃　要执行的命令，其中chdir指定的是项目的目录 stopsignal=HUP stopasgroup=true killasgroup=true autorestart=true stdout_logfile=/data/log/uwsgi.log # uwsgi　的日志 stderr_logfile=/data/log/uwsgi.log # uwsgi　的日志 stdout_logfile_maxbytes = 20MB stderr_logfile_maxbytes = 20MB [group:django_test] programs=django_test 写好配置文件之后启动supervisor，supervisord -c django_test_supervisor.conf就可以把服务启动起来了． 启动起来之后可以进入控制台去管理任务．supervisorctl -c django_test_supervisor.conf可以进入到supervisor的命令行，并且可以看到管理的进程．可以通过start django_test/stop django_test/restart django_test等命令去操作进程． 0X02 后记 现在服务已经可以直接上线了，但是通常情况下我们会让Nginx代理一下到uwsgi，这样静态文件什么的就不会再走一次Python程序了． ","date":"2017-12-23","objectID":"/posts/uwsgi-supervisor-dployer-django-app/:0:0","tags":["Django","Python","uWSGI","Supervisor"],"title":"使用 uwsgi 和 supervisor 部署 Django 程序","uri":"/posts/uwsgi-supervisor-dployer-django-app/"},{"categories":null,"content":"0X00 安装fabric 使用pip可以轻松地安装fabric pip install fabric 0X01 初次调用 在当前目录下创建一个名为fabfile.py的文件，填写文件内容如下： # coding=utf-8 import fabric def test(): print 'hello,world' 然后在当前目录下执行命令fab test就可以看到一条hello,world输出了。 0X02 浅显的道理 根据上面简单的例子可以看出来fab命令执行的时候会默认找到当前目录下的fabfile.py文件，找到后会用fab命令的参数去匹配fabfile.py中的函数名，执行相应的功能。 实际上当前目录可以没有fabfile.py，如果当前目录的上级目录中有fabfile.py是会采用上级目录中的fabfile.py的。而且文件名也不一定用fabfile.py，假设取了一个名为asdf.py的文件，那么只需要执行fab -f asdf.py就可以采用这个fabfile了。 0X03 执行本地命令 作为一个可以用来自动化运维和远程部署的库，运行本地命令是一个必不可少的功能。 # coding=utf-8 from fabric.api import local def list_home_dir(): local('ls ~/.') 这里的local方法就是执行本地命令，并将输出打印出来。然后运行fab list_home_dir就可以看到自己~目录下的文件们了。当然，也可以带参数的。 def cp_file(file_a, file_b): local('cp %s %s' % (file_a, file_b)) 调用这个方法的时候可以通过fab cp_file:\"file_a=~/hello.py,file_b=~/hello_b.py\"这个命令将~/hello.py复制到hello_b.py。 0X04 执行远程命令 fabric用处最大的一点就是远程执行命令了。使用下面这段代码，运行fab list_home后fabric会使用你提供的登录信息通过SSH登录到远程机器上执行ls ~/.的命令。其中run()就是在远程机器上执行命令。 # coding=utf-8 from fabric.api import run, env env.hosts = ['qcloud.just666.cn', ] env.user = 'root' env.password = '5L2g5b2T5oiR5YK75ZWK' def list_home(): run('ls ~/.') 0X05 多台机器执行相同的命令 有的时候我们有多台机器，需要执行相同的命令，这种时候就可以用env.passwords来解决问题。通过hosts指定用户名和主机，用passwords指定密码，就可以同时登录到多台机器上了。 #!/usr/bin/env python # -*- coding: utf-8 -*- from fabric.api import env, run # 指定主机 env.hosts = [ 'root@qcloud.just666.cn', 'root@aliyun.just666.cn', 'root@aws.just666.cn', ] # 指定密码 env.passwords = { 'root@qcloud.just666.cn': '5L2g5b2T5oiR5YK75ZWK', 'root@aliyun.just666.cn': '5L2g5b2T5oiR5YK75ZWK', 'root@aws.just666.cn': '5L2g5b2T5oiR5YK75ZWK', } def hello(): run('ls ~/.') 其实我也不是很懂，既然env.passwords都已经制定了用户名，主机和密码为什么还要用hosts再指定一次呢？不是很懂，注释过hosts，就不能用了。 0X06 多台机器执行不同命令 不过通常情况下我们是有不止一台远程机器的，要不然也不会需要什么自动化了。那么假设我们有三台机器，分别是mysql/apache/nginx这三个服务，那么我们可以这么写脚本。这样我们可以通过fab start_firewalld启动三台机器的防火墙，使用fab start_mysql/start_httpd/start_nginx分别启动在这三台机器上的三个服务。 #!/usr/bin/env python # -*- coding: utf-8 -*- from fabric.api import env, run, roles env.hosts = [ 'root@mysql.just666.cn', 'root@apache.just666.cn', 'root@nginx.just666.cn', ] env.passwords = { 'root@mysql.just666.cn': 'zhangHAO8', 'root@apache.just666.cn': '5L2g5b2T5oiR5YK75ZWK', 'root@nginx.just666.cn': '5L2g5b2T5oiR5YK75ZWK', } env.roledefs = { 'all': [ 'root@mysql.just666.cn', 'root@apache.just666.cn', 'root@nginx.just666.cn', ], 'mysql': ['root@mysql.just666.cn', ], 'apache': ['root@apache.just666.cn', ], 'nginx': ['root@nginx.just666.cn', ], } @roles('all') def start_firewalld(): run('systemctl start firewalld') @roles('mysql') def start_mysql(): run('systemctl start mysql') @roles('apache') def start_httpd(): run('systemctl start httpd') @roles('nginx') def start_nginx(): run('systemctl start nginx') 0X07 并发任务 通常情况下fabric是穿行执行任务的，假设有100台机器要执行相同的命令，虽然我们批量化了，但是他们依旧是串行的，导致效率比较低。这种时候我们可以采用@parallel装饰器来使方法变成并行的。比如有一个方法def test_speed用来测试机器的网速，需要持续一分钟才行，并且有很多台机器，那么这个@parallel就可以发挥作用了。 from fabric.api import parallel @parallel def test_speed(): run('test_speed') 此时这些任务就是并行的了，会快很多。不过如果真的有非常多的机器要并行，那么fabric可能扛不住，可以给并行数量设置一个上限@parallel(pool_size=10)，这样就是最高10个并行任务了。 0X08 传文件 文件传输用的是scp的原理，分成两个方法，分别是get/put，用法也非常简单就像普通的cp一样。get('remote_file', 'local_file')和put('local_file', 'remote_file') from fabric.api import env, get, put env.hosts = ['qcloud.just666.cn', ] env.user = 'root' env.password = '5L2g5b2T5oiR5YK75ZWK' def get_vimrc(): get('~/.vimrc', '~/.vimrc') def put_vimrc(): put('~/.vimrc', '~/.vimrc') 0X09 切目录 在fabric中切目录要配合Python的with语法使用。共有cd/lcd这两种切目录方法，对应的是远程目录和本地目录。 from fabric.api import env, cd, lcd env.hosts = ['qcloud.just666.cn', ] env.user = 'root' env.password = '5L2g5b2T5oiR5YK75ZWK' def list_local_dir(): with lcd('/'): # 切到本地根目录 local('ls') # 在目录下执行命令 with lcd('~'): # 切到本地主目录 local('ls') # 在目录下执行命令 def list_remote_dir(): with cd('/'): # 切到远程根目录 run('ls') # 在目录下执行命令 with cd('~'): # 切到远程主目录 run('ls') # 在目录下执行命令 0X0A PATH 有的时候我们需要临时添加PATH，可以通过这种方法。 from fabric.api import env, run, path env.hosts = ['qcloud.just666.cn', ] env.user = 'root' env.password = '5L2g5b2T5oiR5YK75ZWK' d","date":"2017-12-10","objectID":"/posts/python-fabric/:0:0","tags":["Python"],"title":"Python 自动化运维与远程部署：fabric","uri":"/posts/python-fabric/"},{"categories":null,"content":"0X00 可迭代对象 Python中的列表，元组，字典，文件都是可迭代对。可迭代对象简单地说就是可以用for i in xxx:来遍历的对象。 my_list = [1, 2, 3, 4, 5, 6, 7] for i in my_llist: print i my_dict = { 'a': u'苟利国家生死以', 'b': u'岂因祸福避趋之' } for i in my_dict: print i 不过如果数据量非常非常庞大的时候，会很影响程序的性能。这种时候就可以使用生成器来解决这个问题。 0X01 生成器 生成器的用法和普通的可迭代对象差不多，最大的特点就是：“用的时候才去计算”。这里写一个简单的例子，演示一下情况。第一种生成超大列表的方式要逐项计算完才算弄出来了这个10000长的列表，而后者是生成器，只是声明了怎么算，并没有真的去算，所以在速度上才是完全不能比的。 #!/usr/bin/env python # coding=utf-8 import time if __name__ == '__main__': start = time.time() [x**x for x in range(10000)] # 这里生成的是一个超大的列表 end = time.time() print end - start start = time.time() (x**x for x in range(10000)) # 这里生成一个超大的生成器（注意看，括号不一样） end = time.time() print end - start 运行结果如下： shawn in ~ λ python hello.py 6.76475715637 0.000135898590088 0X02 yield关键字 那是时候自己生成弄一个生成器出来了，Python中提供的yield关键字就是用来干这个的，通过这个关键字可以创造自己的生成器。 还是上面同样的例子，稍加改动 #!/usr/bin/env python # coding=utf-8 import time def create_list(): # 创建一个普通的列表 my_list = [] for x in range(10000): my_list.append(x**x) return my_list def test_yield(): # 使用yield for x in range(10000): yield x**x if __name__ == '__main__': start = time.time() a = create_list() end = time.time() print end - start start = time.time() a = test_yield() end = time.time() print end - start 运行起来看到的时间差距和刚刚演示的也差不多 shawn in ~ λ python hello.py 6.45007514954 0.0080668926239 ","date":"2017-11-12","objectID":"/posts/python-iteration-yield/:0:0","tags":["Python"],"title":"Python 中的迭代、生成和 yield 关键字","uri":"/posts/python-iteration-yield/"},{"categories":null,"content":"0X00 怎样正确使用分支 通常情况下一个git仓库要保持三个及以上的分支，基本的分支明明如下： name function master 正常运行的稳定版本 develop 正常运行的开发版 feature 添加新功能的分支 hotfix 紧急修复bug的分支 如果你已经fork了一份代码到自己本地，当你想添加一个新功能比如「用户管理」的时候，就应该先换到develop分支，然后由这个分支创建一个新的名为feture_add_usermanager的分支。在新分支里编写代码后将代码提交一个PullRequest到自己的develop分支，合并起来后再提交一个PullRequest到团队的仓库中，等待团队其他成员review后就可以正式将代码合并到团队的develop中了。等下一次发布新版本的时候就可以将团队的develop分支合并到团队的master分支中了了。 如果中途项目出现了严重bug(不能登录)需要即使修改上线，那就从自己的master分支上新建一个名为hotfix_cantlogin的分支，修改完后直接提交PullRequest到团队的master分支，review且合并后就可以将该分支删除了。 0X01 合并多次commit 有的时候会出现这么一种情况：连续的n次commit解决的都是同一个问题，为了让最终的提交记录清晰明了，可以将这几次的commit合并为一次。看下面的实例： commit bc83e15e3245ad4064fdd9fe2bf105252d0c51fc (HEAD -\u003e develop) Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Mon Nov 6 00:52:43 2017 +0800 fix usermanager commit affc0acb9a3de30288f144ae2d0b28a9fd60af4b Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Mon Nov 6 00:52:33 2017 +0800 fix usermanager commit fcf13d712bd3dbd483442ffd316741e04acaaa3e Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Mon Nov 6 00:52:23 2017 +0800 fix usermanager commit 2812e2928913e9b3a7650e389602b8bb10ca388b Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Mon Nov 6 00:52:08 2017 +0800 fix usermanager commit f7f273d9c36d748183ac3e6a90f06ff14ed42f95 Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Mon Nov 6 00:51:52 2017 +0800 add user_manager 最近的四次提交内容都是一样的，修改了四次bug最终才解决了问题，因为最近四次的commit是相同含义的修改，所以最好合并在一起，可以使用这个方法 # 最后接的id是你合并的这些commit之前的一个 git rebase -i f7f273d9c36d748183ac3e6a90f06ff14ed42f95 ＃ 输入命令之后到vi环境的界面 pick 2812e29 fix usermanager pick fcf13d7 fix usermanager pick affc0ac fix usermanager pick bc83e15 fix usermanager # 我们将其修改为 pick 2812e29 fix usermanager squash fcf13d7 fix usermanager squash affc0ac fix usermanager squash bc83e15 fix usermanager # 保存退出，进入commit message修改的界面 这里会展示出合并的这几次commit的message，我们选择性的修改一下将这几次的commit message整合一下，继续保存。这次保存之后就相当把这几次的commit合并成一次了。 注意，如果你是从仓库中clone下来的项目，那么有可能在你合并完几个commit后再git push会提示错误。假设以前是有100次commit的，现在你合并了最后的三次，再push，就会和远端的仓库出现分歧。一般情况下都是在确认自己的代码没问题之后，使用git push -f的方式强行把代码推上去，这样就会覆盖掉最后的几次commit，以你新push的代码为基准了。 0X02 迁移一次commit 有的时候我们需要将某个分支中的某次提交复制到另一个分支，具体使用情景有很多。这里展示一个使用样例： git log 查看master分支的commit，只有这一次 commit 2959f92753d84bff5b125b15cd9497c4d8dff637 (HEAD -\u003e master) Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:09:07 2017 +0800 完成XXX功能 切换到dev分支查看commit，有四次 commit bba77c7faf7821802b108688a17d8e6655975b7e (HEAD -\u003e dev) Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:11:47 2017 +0800 整理XXX commit 9001f8250ce47a067c9af50bf62af025b872c3bc Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:11:37 2017 +0800 添加XXX commit 6e4171ad632a7627be08ec7c9afaf64f55ccc98d Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:11:21 2017 +0800 修复XXX commit 2959f92753d84bff5b125b15cd9497c4d8dff637 (master) Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:09:07 2017 +0800 完成XXX功能 如果此时我想将那个“添加XXX”的commit移到master分支，可以使用`git cherry-pick` 可以看到“添加XXX”的那次commit的ID为`6e4171ad632a7627be08ec7c9afaf64f55ccc98d` 我们复制这个ID，切到master分支，使用`git cherry-pick commit_id` shawn in ~/test on master λ git cherry-pick 6e4171ad632a7627be08ec7c9afaf64f55ccc98d [master f77b496] 修复XXX Date: Sun Nov 12 16:11:21 2017 +0800 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 b shawn in ~/test on master λ git log commit f77b49677efb1ba967dbc323332e2f8495fdbca1 (HEAD -\u003e master) Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:11:21 2017 +0800 修复XXX commit 2959f92753d84bff5b125b15cd9497c4d8dff637 Author: shawn \u003cshawnbluce@gmail.com\u003e Date: Sun Nov 12 16:09:07 2017 +0800 完成XXX功能 这样一个commit就复制过来了。如果有冲突的话需要将冲突解决完才能合并。 ","date":"2017-11-12","objectID":"/posts/git-simple/:0:0","tags":["Git"],"title":"git 初步使用经验","uri":"/posts/git-simple/"},{"categories":null,"content":"0X00 推荐一波Linux下的软件 Linux对于普通用户可能确实没有那么友好，但是对于计算机“专业”人士来说就好多了。我从接触Linux到现在也有个三两年了，而且用Linux桌面也有一段时间了。这段时间里也发现了不少好用的软件和工具，在这里整理一下也向大家推荐一波。这些工具有些是用来提升工作效率的，有些是用来娱乐的等等。。不过每一个都是我离不开的好工具。 非常重要的一点是，我推荐的这些软件除了为知笔记以外都是 免费的 ，而且还有一大半是 开源的 。 0X01 zsh Linux真正效率高的地方就在于Terminal，那我们就需要选一个好用的SHELL。目前绝大多数Linux发行版本都默认使用Bash，这是一个非常好用非常成熟的SHELL。但是有一个更好用的Shell叫做zsh，可以从官网安装也可以直接apt install zsh就装上了。这个shell一定要配合着oh-my-zsh来使用。oh-my-zsh可以让你的SHELL发挥最大的效率。 0X02 vim vim 的好处就不多说了。在这里推荐一套开源的配置，在自己没有配置vim的情况下直接安装就好了。 wget https://raw.githubusercontent.com/vince67/v7_config/master/vim.sh bash vim.sh 这个脚本官方只支持Mac和Debian系，如果是其他系需要自行修改一下这个脚本里的内容。 例如：如果是CentOS的话就将里面安装软件的apt-get修改成yum，安装就能顺利进行了。 这个安装好了之后vim常用的插件和配置就都搞定了，默认情况下写Python就可以拥有非常好的用户体验。具体用法简单看一下~/.vimrc就可以了解到了。 0X03 htop 在Linux命令行中查看系统资源比如CPU、内存、进程等要用到top、free等命令，现在一个htop就解决了问题，而且是彩色输出还支持鼠标点击排序。 sudo apt install htop sudo yum install htop 0X04 mycli MySQL是我们常用的一款数据库了，有的时候需要连到数据库里查一些东西或是一些什么操作。通常我们会选用mysql命令来连接数据库，但是这个工具挺不好用的，所以才会出现了这么一款神器mycli。由于是用Python写的，还封装了pip，所以安装起来很简单，一条命令pip install mycli就搞定了。这个工具和mysql命令用法是完全一样的，他的特点就是支持自动补全和SQL高亮，而且输出默认是使用less展示的，可以直接用键盘上下滚动，不需要鼠标键盘乱换着用。 0X05 WPS WPS是在Linux下MS Office的最佳替代品了。虽然功能和易用性上不如MS Office，但是他免费，在Linux下没有广告，而且对MS Office的支持率还是很高的。通常只是写个简单的文档或者展示PPT什么的是没有问题的。不过WPS暂时还没有开源，倒是有一个开源的替代品LibreOffice，但是实在是不好用，有兴趣的同学可以加入LibreOffice的社区，帮助LibreOffice变得更好。 对了，WPS在Linux下的启动速度奇快，在我PCI-E Nvme的硬盘下每次都是秒开，比Office 2016要更快一筹。 0X06 Chrome/Firefox 这俩就没的说了，我就介绍一下我正在用的几个Chrom扩展吧 Adblock Plus 去广告神器 crxMouse Chrome Gestures 鼠标手势神器 Draw.io Desktop 一个在线画流程图的ChromAPP Google翻译 可以设置为选中翻译，因为Google翻译没有被墙，所以很好用 LastPass 一款免费的密码管理扩展，安全可靠方便易用 Postman 一个用于快速模拟http请求的ChromeAPP Proxy SwitchOmega 一个用于设置浏览器代理的扩展，配合SS使用感觉良好 Search by Image 以图搜图，开启后网页每张图右下角都会出现一个小Logo，点击Logo就可以用Google搜索这张图 Tamepermonkey 一个JS脚本管理器，神器 9.1. AC-baidu 去百度广告 9.2. 百度广告清理 去百度广告可能需要两个才行 Wappalyzer 开发者必备，可以显示出当前网页/网站所使用的技术和框架 Vimium 用vim快捷键来操作浏览器，神器 0X07 为知笔记/蚂蚁笔记 在我所知道的范围内，Linux环境下体验还不错的笔记软件就这两款。给大家列个表对比一下 为知笔记 和 蚂蚁笔记 特性 为知笔记 蚂蚁笔记 客户端开源 是 是 服务端开源 否 否 收费情况 免费体验100天，过后60一年 免费（不能同步）、50/年（支持同步，流量较少）、150/年（流量中等 一键变博客 不支持 支持 移动端体验 好 较差 总的来说，为知笔记每年要花60块钱，可以获得不错的使用体验；蚂蚁笔记每年要花150块钱才能获得不错的体验。不过由于蚂蚁笔记的服务端开源，所以可以将服务端部署在自己的服务器上以免费获得最完整的功能。主要就是看大家对哪点的需求更多了。 0X08 网易云音乐 网易云音乐就没必要过多介绍了。官方推出了deb包，几乎所有基于Debian的Linux都能一键安装。也有民间玩家制作了rpm包，没有用过不知道体验怎么样。还有民间高手在做的命令行版本的网易云音乐，功能已经相当完善了。目前有Python版本的和NodeJS版本的，有兴趣的同学可以参与到开发过程中来。 0X09 Atom/vscode 这两个应该是目前为止图形界面下最好用的两款开源编辑器了。Atom是由Github制作的，vscode是由Microsoft制作的。一个是拥有全世界最厉害程序员的平台，一个是全世界最厉害的软件公司之一。自然软件的质量没的说了，我个人也只是轻度使用，所以主要是看外观和心里偏向而已。比如我就比较喜欢用Atom。 0X0A Nylas 目前为止Linux下最有名的两款邮件客户端应该是雷鸟和EVO了，不过前段时间出来的这款Nylas特好用，我一直在用。最重要的一点是可以不用梯子就访问Gmail。不过目前遇到的唯一一个坑就是连接QQ企业邮箱的时候有问题，每次都会报错，但是又能收到信，很奇怪。给社区反馈过，也没得到答复。然而我还是强烈推荐这款软件，真的是好用。 0X0B Guake 这个名字对于大学生来说非常不友好，也不知道怎么这么好的软件怎么就取了个\"挂科\"的名字呢，哈哈。我们在用Linux的时候偶尔会需要输入一两行命令，或者是需要用htop长期看着自己的性能指标。那这时候每次都打开一个终端再输命令吗？当然不用。装一个Guake，给他设定一个快捷键，每次需要临时调出终端的时候只需要按一下快捷键，终端就会从屏幕顶部弹下来，还能设置失去焦点自动隐藏。这样用起来就很舒服了。 0X0C Virtualbox 这款软件想必也不需要介绍，是目前免费桌面虚拟机软件中最好的一款了。大量人在用破解的VMwareWorkstadion，虽然比VirtualBox好用一点，但是我们还是应该按版权来，不想花钱就用VirtualBox吧，况且也挺好用的。比如你在长期用Linux的环境下偶尔也需要用一下Windows，比如你长期用Fedora偶尔也要用一下Ubuntu，这就是VirtualBox出场的时候了。 0X0D TeamViewer 平时在Windows下大家都用习惯了QQ的远程协助，那么到了Linux下应该怎么办呢？Teamviewer应该就是最好的选择了。其实Teamviewer在性能上是要比QQ的远程协助更强的。注意一点：这款软件对个人免费，但是要在公司里用的话是要花钱买的。 0X0E 深度终端 深度终端是我目前在Linux里用过最好用的终端模拟器了。自带的主题很漂亮，对中文支持完美，而且还集成了SSH管理功能。 0X0F Shadowsocks-QT 翻越长城必备神器 Shadowsocks-QT。目前为止这东西是最好用的，但是不能像Windows和Mac中的那样直接支持PAC，需要浏览器的扩展来配合实现PAC功能。 0X10 Jetbrains Jetbrains他们的产品简直是厉害，如果是学生的话可以去申请全部软件的专业版。我用过的有WebStorm/PyCharm/IntellJ IDEA/DataGrip/Android Studio这五款，其中AndroidStudio免费，PyCharm和IDEA提供社区版。他们的软件最大的特点就是统一性强，你用过其中一款之后再去用其他的很快就能上手。而且都非常智能，这才是IDE应该有的样子。 0X11 Audacious 如果你有把音乐下载到本地再听的习惯，那么这款软件就特别适合。这款软件特别小，界面简洁，使用简单。是我用过Linux下最好用的离线播放器了。 0X12 Wireshark 一款可以用来分析网络流量包的软件，可以在官网下载最新的也可以在包管理中安装使用。这应该是开发者必备的工具之一了吧，有的时候为了分析一波流量或者看看自己的请求到底是不是有问题等等。。。当然，如果是大黑客的话还能有更炫酷的应用场景。 0X13 Dia 一款用来画图的开源软件，可以简单的画出流程图，数据库模型等等。。使用体验还是不错，如果只是轻度使用的话足够了。 0X14 Stellarium 如果你是一个理科生甚至以为天文爱好者，那你一定对宇宙的浩瀚很痴迷。这款软件能实时模拟星空的运动轨迹，还有各种行星、恒星的数据，官","date":"2017-09-29","objectID":"/posts/linux-software-recommend/:0:0","tags":["Linux"],"title":"用好 Linux 之：软件推荐","uri":"/posts/linux-software-recommend/"},{"categories":null,"content":"0X00 virtualenv好用但有瓶颈 virtualenv固然好用，可以给你每一个Python项目创建一个独立的Python环境互不干扰。有三五个Python项目的时候用的很开心，有十几个项目的时候还凑合，如果有更多的项目virtualenv就会出现瓶颈。因为virtualenv会给每一个Python虚拟环境创建一个目录来保存相关文件，项目一多这个虚拟环境的目录也就多了起来，每次在多个环境之间source ../../../xxx/bin/active 和 deactive 也挺烦的，并且很容易把某些环境搞丢。不过开源世界最不缺的就是解决问题的方法了，既然有人遇到了这个问题，那么八成就已经有了解决这个问题的好办法。 0X01 virtualenvwrapper 这个东西名字确实有点长，顾名思义就是把virtualenv包装起来。首先来安装一波这个东西 sudo apt install virtualenvwrapper # Debian系 sudo yum install virtualenvwrapper # RHEL系 安装好后要进行简单的配置 vim ~/.bashrc # 添加一条环境变量，可以根据自己用的shell来修改 向文件中添加 WORKON_HOME=~/Envs 表示将未来所有的虚拟环境都放在 ~/Envs 中。然后创建这个目录 mkdir -p $WORKON_HOME 。最后source一下安装文件，source /usr/bin/virtualenvwrapper.sh 会显示创建了很多文件，到这里就安装完成了。 如果source的时候没有这个virtualenvwrapper.sh文件，那就用which virtualenvwrapper.sh找一下，不过八成都是在/usr/bin/virtualenvwrapper.sh 0X02 把它用起来 以前用virtualenv的时候要每次source xxx/bin/active，用完了再deactive，这次就方便多了。下面列出几个常用命令。 | 命令 | 功能 | | – | | mkvirtualenv blog | 创建一个名为blog的虚拟环境，并切换过去 | | workon blog | 切换到名为blog的虚拟环境中 | | workon | 列出当前所有的虚拟环境 | | rmvirtualenv blog | 删除名为blog的虚拟环境 | | deactive | 退出当前所处的虚拟环境 | 下面演示一下这个用法 # 创建一个新的虚拟环境，名为blog [root@localhost ~]# mkvirtualenv blog New python executable in blog/bin/python Installing Setuptools..............................................................................................................................................................................................................................done. Installing Pip.....................................................................................................................................................................................................................................................................................................................................done. virtualenvwrapper.user_scripts creating /root/Envs/blog/bin/predeactivate virtualenvwrapper.user_scripts creating /root/Envs/blog/bin/postdeactivate virtualenvwrapper.user_scripts creating /root/Envs/blog/bin/preactivate virtualenvwrapper.user_scripts creating /root/Envs/blog/bin/postactivate virtualenvwrapper.user_scripts creating /root/Envs/blog/bin/get_env_details # 创建一个新的虚拟环境，名为student_admin (blog)[root@localhost ~]# mkvirtualenv student_admin New python executable in student_admin/bin/python Installing Setuptools..............................................................................................................................................................................................................................done. Installing Pip.....................................................................................................................................................................................................................................................................................................................................done. virtualenvwrapper.user_scripts creating /root/Envs/student_admin/bin/predeactivate virtualenvwrapper.user_scripts creating /root/Envs/student_admin/bin/postdeactivate virtualenvwrapper.user_scripts creating /root/Envs/student_admin/bin/preactivate virtualenvwrapper.user_scripts creating /root/Envs/student_admin/bin/postactivate virtualenvwrapper.user_scripts creating /root/Envs/student_admin/bin/get_env_details # 查看所有的虚拟环境 (student_admin)[root@localhost ~]# workon blog student_admin # 切换到blog环境 (student_admin)[root@localhost ~]# workon blog # 删除student_admin环境 (blog)[root@localhost ~]# rmvirtualenv student_admin Removing student_admin... # 再看一下所有环境，student_admin已经不在了 (blog)[root@localhost ~]# workon blog # 退出当前环境 (blog)[root@localhost ~]# deactivate [root@localhost ~]# ","date":"2017-09-19","objectID":"/posts/python-virtualenvwapper/:0:0","tags":["Python","Virtualenvwapper"],"title":"Python 使用 virtualenvwapper 管理虚拟环境","uri":"/posts/python-virtualenvwapper/"},{"categories":null,"content":"0X00 *args是什么 我们知道Python3中的print从一个关键字变成了一个函数，那么调用的时候我们可以这样调用这个函数，可以随便接受几个参数。 \u003e\u003e\u003e print(1) 1 \u003e\u003e\u003e print(1, 2, 3) 1 2 3 \u003e\u003e\u003e print(1, \"hello\", 6.66) 1 hello 6.66 那么如果我们想自己实现类似这样‘变态’的函数该怎么实现呢？这就需要用到*args了，可以将一个非键值对的可变数量的参数列表传给一个函数（换个书佛啊：可以传n个参数给函数，而且n不是固定的），举个例子就容易理解多了。 def say_something(*args): for i in args: print i print '--------' say_something(1) say_something(1, 2, 3) say_something('hello') say_something('hello', 'world') 运行这个例子的输出就是这样的 1 -------- 1 2 3 -------- hello -------- hello world -------- 还有一个更棒的例子来自Gitbook def test_var_args(f_arg, *args): print(\"first normal arg:\", f_arg) for arg in args: print(\"another arg through *args:\", arg) test_var_args('yasoob', 'python', 'eggs', 'test') 输出是这样的 ('first normal arg:', 'yasoob') ('another arg through *args:', 'python') ('another arg through *args:', 'eggs') ('another arg through *args:', 'test') 这个例子完整的说明了\\*args的用法，我们传入的第一个参数被函数指定的f_arg接收到了，其余的都被*args接收到了。 0X01 **kwargs是什么 写代码的时候还会有一种函数调用，大概是这个样子json.dumps(dict_data)和json.dumps(dict_data, indent=4)。当然，实现这种的方式有一个最简单的方案就是def dumps(input_data, indent=0)。在可选参数只有一两个的时候这种方式固然是好用的，但是如果像是requests这种库中的常用方法，有很多很多个可选参数那就该用上这个**kwargs了。顾名思义这个就是keyworkargs的意思，也就是说是带有key的可变参数。可以这样定义一个函数 def foo(**kwargs): for key in kwargs: print key print kwargs[key] print '-----' foo(a=1, b=2, c=3, d=4, e=5) 运行出来的结果可想而知： a 1 ----- c 3 ----- b 2 ----- e 5 ----- d 4 ----- 0X02 合在一起怎么用 值得一提的是如何把这两个放在一起用，这里列举个例子来演示一下 #!/usr/bin/env python # coding=utf-8 def foo(name, sex, *args, **kwargs): print 'name is ', name print 'sex is ', sex print 'other is ', args for key in kwargs: print key, ' is ', kwargs[key] def bar(*args, **kwargs): print 'args is ', args print 'kwargs is ', kwargs foo('shawn', '???', 'hello', 'world', hobby='computer', number=666) print '--------------------------' bar('shawn', '???', 'hello', 'world', hobby='computer', number=666) 输出结果是这样的 name is shawn sex is ??? other is ('hello', 'world') hobby is computer number is 666 -------------------------- args is ('shawn', '???', 'hello', 'world') kwargs is {'hobby': 'computer', 'number': 666} 这里有需要注意的一点：参数的名字不一定非要是*args和**kwargs，所以我们定义函数的时候不一定是def foo(*args, **kwargs):，也同样可以定义成def bar(*hehe, **haha):，这里真正标识的是星号而不是名字。不过建议命名的时候符合大家的习惯。 ","date":"2017-09-05","objectID":"/posts/python-args-kwargs/:0:0","tags":["Python"],"title":"Python中的 *args和 **kwargs","uri":"/posts/python-args-kwargs/"},{"categories":null,"content":"0X00 安装环境 我们在Python开发和学习过程中需要用到各种库，然后在各个不同的项目和作品里可能用的版本还不一样，正因为有这种问题的存在才催生了virtualenv的诞生。virtualenv可以在电脑上创建一个虚拟环境，可以针对每一个项目创建一个虚拟环境，这样就不用担心各个不同的项目用不同版本的库的时候出现的冲突了。 ** 下面的内容只适用于Linux/OSX，未经Windows环境测试 ** 要使用这个功能还是需要安装，安装virtualenv肯定就得直接用pip安装了，pip install virtualenv就可以轻松装上了。装好之后我们就可以来测试一波了。 0X01 初始化一个空的工作环境 首先在一个空的环境中执行virtualenv --no-site-packages test_env，就是在当前目录创建一个名为test_env的虚拟环境。这里--no-site-packages参数是指不从全局的Python中携带任何第三方库。就比如说你在全局Python中安装了xxx库，在不用这个参数来创建虚拟环境时，虚拟环境中也会带着这个库；但是加上了这个参数，虚拟环境中就是一个纯净的Python，没有这些库。 root in ~ λ virtualenv --no-site-packages test_env New python executable in /root/test_env/bin/python Please make sure you remove any previous custom paths from your /root/.pydistutils.cfg file. Installing setuptools, pip, wheel...done. 然后可以通过source test_env/bin/activate可以进入（激活）到这个虚拟环境里去。进入到虚拟环境中之后，通常情况下你的命令提示符最前面会出现一个括号，括号里面写着你虚拟环境的名字。 这里说是虚拟环境，其实一切都是真实的。只是说你在激活了这个环境，在这个环境下用pip安装的库都放在 test_env 中。 也可以通过deactivate来退出这个环境。 0X02 批量导出和安装库 比如我们开发了一个项目，里面用到了pymongo/requests/flask/pymysql等等等等十几二十个库，还要指定特定的版本，那么当把一个项目从机器A迁移到机器B的时候就会很麻烦。需要手动记录每个库和版本，还要逐个去安装，非常麻烦。所以针对这个问题pip已经有了非常完善的解决方案。 (test_env) root in ~ λ pip freeze \u003e requirements.txt # 导出已安装的库 这个命令可以导出当前环境中安装好的所有第三方库，并且是以一个标准的格式导出的。所以一般一个标准的python项目的根目录都会有这个名为requirements.txt的依赖文件。 既然可以一次性导出，那么必然可以一次性安装喽。通过这种方式就可以将上面导出的特定版本的所有库一次性全装上。配合virtualenv可以快速的部署一个Python项目，并且不会搞乱其他的Python项目环境。 (test_env_1) root in ~ λ pip install -r requirements.txt ","date":"2017-08-17","objectID":"/posts/python-virtualenv/:0:0","tags":["Python","Virtualenv"],"title":"Python 中 Virtualenv 和 pip 的简单用法","uri":"/posts/python-virtualenv/"},{"categories":null,"content":"内容参考自python - 操作RabbitMQ 0X00 安装环境 首先是在Linux上安装rabbitmq # 环境为CentOS 7 yum install rabbitmq-server # 安装RabbitMQ systemctl start rabbitmq-server # 启动 systemctl enable rabbitmq-server # 开机自启 systemctl stop firewall-cmd # 临时关闭防火墙 然后用pip安装Python3的开发包 pip3 install pika 安装好软件之后可以访问http://115.xx.xx.xx:15672/来访问自带的web页面来查看和管理RabbitMQ。默认管理员的用户密码都是guest 0X01 简单的向队列中加入消息 #!/usr/bin/env python3 # coding=utf-8 # @Time : 2017/6/13 19:25 # @Author : Shawn # @Blog : https://blog.just666.cn # @Email : shawnbluce@gmail.com # @purpose : RabbitMQ_Producer import pika # 创建连接对象 connection = pika.BlockingConnection(pika.ConnectionParameters(host='115.xx.xx.xx')) # 创建频道对象 channel = connection.channel() # 指定一个队列，如果该队列不存在则创建 channel.queue_declare(queue='test_queue') # 提交消息 for i in range(10): channel.basic_publish(exchange='', routing_key='test_queue', body='hello,world' + str(i)) print(\"sent...\") # 关闭连接 connection.close() 0X02 简单的从队列中获取消息 #!/usr/bin/env python3 # coding=utf-8 # @Time : 2017/6/13 19:40 # @Author : Shawn # @Blog : https://blog.just666.cn # @Email : shawnbluce@gmail.com # @purpose : RabbitMQ_Consumer import pika credentials = pika.PlainCredentials('guest', 'guest') # 连接到RabbitMQ服务器 connection = pika.BlockingConnection(pika.ConnectionParameters('115.xx.xx.xx', 5672, '/', credentials)) channel = connection.channel() # 指定一个队列，如果该队列不存在则创建 channel.queue_declare(queue='test_queue') # 定义一个回调函数 def callback(ch, method, properties, body): print(body.decode('utf-8')) # 告诉RabbitMQ使用callback来接收信息 channel.basic_consume(callback, queue='test_queue', no_ack=False) print('waiting...') # 开始接收信息，并进入阻塞状态，队列里有信息才会调用callback进行处理。按ctrl+c退出。 channel.start_consuming() 0X03 万一消费者掉线了 想象这样一种情况： 消费者从消息队列中获取了n条数据，正要处理呢结果宕机了，那该怎么办？在RabbieMQ中有一个ACK可以用来确认消费者处理结束。就有点类似网络中的ACK，消费者每次从队列中获取了数据之后队列不会立刻将数据移除，而是等待对应的ACK。消费者获取到数据并处理完成之后会向队列发送一个ACK包，通知RabbitMQ这堆消息已经处理妥当了，可以删除了，这时候RabbitMQ才会将数据从队列中移除。所以这种情况下即使消费者掉线也没有什么问题，数据依旧会在队列中存在，留给其他消费者处理。 在Python中这样实现： 消费者有这样一行代码channel.basic_consume(callback, queue='test_queue', no_ack=False)，其中no_ack=False表示不发送确认包。将其修改为no_ack=True就会在每次处理完之后向RabbitMQ发送一个确认包，以确认消息处理完毕。 0X04 万一RabbitMQ宕机了呢 虽然有了ACK包，但是万一RabbitMQ挂了那数据还是会损失。所以我们可以给RabbitMQ设置一个数据持久化存储。RabbitMQ会将数据持久化存储到磁盘上，保证下次再启动的时候队列还在。 在Python中这样实现： 我们声明一个队列是这样的channel.queue_declare(queue='test_queue')，如果需要持久化一个队列可以这样声明channel.queue_declare(queue='test_queue', durable=True)。不过这行直接放在代码中是不能执行的，因为以前已经有了一个名为test_queue的队列，RabbitMQ不允许用不同的方式声明同一个队列，所以可以换一个队列名新建来指定数据持久化存储。不过如果只是这样声明的话，在RabbitMQ宕机重启后确实队列还在，不过队列里的数据就没有了。除非我们这样来声明队列channel.basic_publish(exchange='', routing_key=\"test_queue\", body=message, properties=pika.BasicProperties(delivery_mode = 2,))。 0X05 最简单的发布订阅 最简单的发布订阅在RabbitMQ中称之为Fanout模式。也就是说订阅者订阅某个频道，然后发布者向这个频道中发布消息，所有订阅者就都能接收到这条消息。不过因为发布者需要使用订阅者创建的随机队列所以需要先启动订阅者才能启动发布者。 发布者代码： #!/usr/bin/env python3 # coding=utf-8 # @Time : 2017/6/13 20:21 # @Author : Shawn # @Blog : https://blog.just666.cn # @Email : shawnbluce@gmail.com # @purpose : RabbitMQ_Publisher import pika # 创建连接对象 connection = pika.BlockingConnection(pika.ConnectionParameters(host='115.xx.xx.xx')) # 创建频道对象 channel = connection.channel() # 定义交换机，exchange表示交换机名称，type表示类型 channel.exchange_declare(exchange='my_fanout', type='fanout') message = 'Hello Python' # 将消息发送到交换机 channel.basic_publish(exchange='my_fanout', # 指定exchange routing_key='', # fanout下不需要配置，配置了也不会生效 body=message) connection.close() 订阅者代码： #!/usr/bin/env python3 # coding=utf-8 # @Time : 2017/6/13 20:20 # @Author : Shawn # @Blog : https://blog.just666.cn # @Email : shawnbluce@gmail.com # @purpose : RabbitMQ_Subscriber import pika credentials = pika.PlainCredentials('guest', 'guest') # 连接到RabbitMQ connection = pika.BlockingConnection(pika.ConnectionParameters('115.xx.xx.xx', 5672, '/', credentials)) channel = connection.channel() # 定义交换机，进行exchange声明，exchange表示交换机名称，type表示类型 channel.exchange_declare(exchange='my_fanout', type='fanout') # 随机创建队列 result = channel.queue_declare(exclusive=True) # exclusive=True表示建立临时队列，当consumer关闭后，该队列就会被删除 queue_name = result.method.queue","date":"2017-06-13","objectID":"/posts/python-rabbitmq/:0:0","tags":["Python","RabbitMQ"],"title":"使用 Python 操作消息队列 RabbitMQ","uri":"/posts/python-rabbitmq/"},{"categories":null,"content":"0X00 什么是REST风格的API 众所周知http协议有GET/PUT/POST/PATCH/DELETE等众多方法，还能在提交请求和发送响应的时候携带数据。REST风格的API就是使用了这些HTTP特性的API。针对一个URL可以有多种动词(方法)来表示不同的操作。 更多详细的内容可以点击查看阮一峰的博客：理解RESTful架构 0X01 怎么选用HTTP动词 常见的动词有这五种，可以对应自己的需求选用 | 动词 | 类似的SQL关键字 | 功能 | | —- |: —- :|: —- :| | GET | SELECT | 获取资源 | | POST | CREATE | 创建资源 | | PUT | UPDATE | 更新资源（需要提供改变后的完整资源） | | PATCH | UPDATE | 更新资源（需要提供改变的属性） | | DELETE | DELETE | 删除资源 | 0X02 设计URL REST风格的API因为可以用HTTP的动词，所以URL中是不带有动词的，如果我要获取某个学生的信息应该是[GET] http://api.example.com/student/id=12345678900。HTTP动词理论上是能满足各种情况下的需求的，所以URL中只应该出现名词而不应该出现动词。这里用阮一峰举的例子来说明一下 | 动词 | 路径 | 功能 | | –|— | | GET | /zoos | 列出所有动物园 | | POST | /zoos | 新建一个动物园 | | GET | /zoos/ID | 获取某个指定动物园的信息 | | PUT | /zoos/ID | 更新某个指定动物园的信息（提供该动物园的全部信息） | | PATCH | /zoos/ID | 更新某个指定动物园的信息（提供该动物园的部分信息） | | DELETE | /zoos/ID | 删除某个动物园 | | GET | /zoos/ID/animals | 列出某个指定动物园的所有动物 | | DELETE | /zoos/ID/animals/ID | 删除某个指定动物园的指定动物 | 0X03 状态码 状态码是HTTP中的一大优势，一个响应可以只靠状态码来判请求结果。这些是常见的状态码，自己设计API的时候要严格按照规范来设计状态码，可以提高代码和API的可读性和可理解性。 状态码 信息 请求类型 含义 200 OK [GET] 服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。 201 CREATED [POST/PUT/PATCH] 用户新建或修改数据成功。 202 Accepted [*] 表示一个请求已经进入后台排队（异步任务）。 204 NO CONTENT [DELETE] 用户删除数据成功。 400 INVALID REQUEST [POST/PUT/PATCH] 用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。 401 Unauthorized [*] 表示用户没有权限（令牌、用户名、密码错误）。 403 Forbidden [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。 404 NOT FOUND [*] 用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。 406 Not Acceptable [GET] 用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。 410 Gone [GET] 用户请求的资源被永久删除，且不会再得到的。 422 Unprocesable entity [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。 500 INTERNAL SERVER ERROR [*] 服务器发生错误，用户将无法判断发出的请求是否成功。 0X04 如何用Python实现 Python有大量第三方库可以实现REST风格的API，我这里选用的是相对轻量化的一个 Flask。安装这个库最简单的方式还是用pip，根据环境变量的不同可能具体命令有所不同，在我的Linux上是用pip3 install flask就可以直接安装好的。 安装好后进入Python的交互式界面输入import flask如果没有出现Import Error就是安装好了。 0X05创建数据库和表 现在可以开始设计API了。既然是成绩管理系统，那么首先就要创建一个数据库，我这里的数据库是用的MariaDB。 列名 类型 含义 键 id int 编号 主键 name varchar(10) 学生姓名 number char(11) 学号 python float Py成绩 cpp float c++成绩 os float 操作系统成绩 network float 计算机网络成绩 total float 总分 ave float 平均分 0X06 创建Py脚本 from flask import Flask, request app = Flask(__name__) # 从这里指定路径、方法、返回数据 @app.route('/', methods=['GET']) def index(): return '\u003ch1\u003ehello,world\u003c/h1\u003e' with app.test_request_context(): app.run() 这段代码写好之后运行起来会在本地监听5000端口(默认的)，然后当你用浏览器访问http://localhost:5000/的时候就像你返回\u003ch1\u003ehello,world\u003c/h1\u003e，在浏览器页面下看到的就是一行大号的hello,world。因为在浏览器的地址栏输入URL按回车之后就是向那个URL发送了GET请求，也就正好调用了index()方法。 这里先将与API无关的代码填好，下面开始正式完成各项功能。其实也就是连接了数据库而已 from flask import Flask, request import json import pymysql app = Flask(__name__) database = pymysql.connect(\"db_host\", \"db_username\", \"db_password\", \"db_name\") cursor = database.cursor() @app.route('/', methods=['GET']) def index(): return '\u003ch1\u003ehello,world\u003c/h1\u003e' with app.test_request_context(): app.run() 0X07 实现一个构造返回Json数据的方法 首先我们选择使用Json来作为数据传输格式，因为Json相对XML来说更轻量一点，现在也更流行。规定客户端每次请求会后服务器都会返回下面这样类型的Json数据 { \"time\": \"unix_time\", “e_msg\": \"error_message\", \"search_list\": { \"item0\": { \"name\": \"name\", \"number\": \"number\", \"python\": \"marks\", \"os\": \"marks\", \"network\": \"marks\", \"cpp\": \"marks\", \"total\": \"marks\", \"ave\": \"ave\" },\"item1\" : { \"name\": \"name\", \"number\": \"number\", \"python\": \"marks\", \"os\": \"marks\", \"network\": \"marks\", \"cpp\": \"marks\", \"total\": \"marks\", \"ave\": \"ave\" } } } API提供增删查改功能，增删改只通过状态码就可以判断执行结果，只有查询的时候才会需要从响应中获取数据。 0X08 增加一条新的数据 添加一条新数据按照标准应该使用动词POST，根据URL中只有名词不用动词只有名词的标准，隧将URL设计成http://localhost/student，再依据标准添加版本号上去，变成http://localhost/v1/student。 具体功能代码实现如下， @app.route('/v1/student', methods=['POST']) # 路径为/v1/student，方法为POST def add_student(): data = request.get_data().decode('utf-8') # 将客户端传来的数据解码 json_data = json.loads(data) # 将数据转为Json # 从Json中获取数据 name = json_data['name'] number = str(json_data['number']) python = json_data['python'] cpp = json_data['cpp'] os = json_data['os'] network = json_data['network'] # 计算总分平均分 total = python + cpp + os + network ave = total / 4 # 查询数据库中是否有该学生的信息 sql = \"SELECT COUNT(*) FROM student.marks WHERE number=\\\"%s\\\"\" % number cursor.execute(sql) co","date":"2017-06-03","objectID":"/posts/flask-rest-api-simple/:0:0","tags":["Flask","Python"],"title":"使用 Flask 设计实现一套 REST API【成绩管理系统】","uri":"/posts/flask-rest-api-simple/"},{"categories":null,"content":"在写一些系统脚本或者自动化运维脚本的时候经常会用到os库，这里做个整理，方便查找 ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:0","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.sep 获得当前操作系统使用的目录分隔符，比如Windows就会得到\\而Linux/Unix就会得到/ ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:1","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.name 获得当前使用的操作系统，Windows是NT内核，所以会得到nt，而Linux/Unix用户则会得到posix ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:2","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.getcwd() 获得当前工作目录，即当前Python脚本工作的目录路径。 ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:3","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.getenv() 用来获得环境变量 os.getenv('PATH') ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:4","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.environ 可以获取并修改环境变量 print(os.environ['PATH']) os.environ += 'D:/testdir/bin/' print(os.environ[\"PATH\"]) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:5","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.listdir() 列出某目录下所有的目录和文件 print(os.listdir()) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:6","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.remove() 删除文件 os.remove('D:/test.file') ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:7","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.system() 运行Shell或者CMD命令 os.system('ifconfig') ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:8","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.linesep 获取当前平台使用的行终止符。例如，Windows使用\\r\\n，Linux使用\\n而Mac使用\\r。 ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:9","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.split() 获得一个列表，list[0]是路径的，list[1]是文件名 path = 'D:/game/gtav/bin/gtav.exe' print(os.path.split(path)[0]) print(os.path.split(path)[1]) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:10","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.isfile()和os.path.isdir() 判断路径是不是文件/目录 print(os.path.isfile('D:/game/gtav/bin/gtav.exe')) print(os.path.isdir('D:/game/gtav/bin')) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:11","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.existe() 函数用来检验给出的路径是否真地存在 path = 'D:\\\\hadoop-2.6.5\\\\bin' print(os.path.exists(path)) path = 'D:\\\\hadoop-2.6.5\\\\bin\\\\hadoop' print(os.path.exists(path)) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:12","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.chdir(dirname) 切换工作目录，相当于cd的命令 os.chdir('D:/game/gtav/') print(os.getcwd()) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:13","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.getsize(name) 获取文件大小，以字节为单位 size = os.path.getsize('D:/iso/debian-8.6.0-amd64-DVD-1.iso') print(size/1024/1024/1024, 'GB') ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:14","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.abspath(name) 获取绝对路径，如果在Python工作目录下有一个文件file.txt，那么我就可以直接open('file.txt')，也可以用该方法获得其绝对路径print(os.path.abspath('file.txt'))。也可以用来规范路径字符串print(os.path.abspath('D:/game\\gtav\\bin/gtav.exe')) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:15","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.normpath(path) 专门用来规范路径 path = 'D:/test/sdf\\zfb' print(os.path.normpath(path)) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:16","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.splitext() 获取文件名和扩展名 path = '/home/shawn/hello.py' print(os.path.splitext(path)) ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:17","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.join(path,name) 连接目录和文件名，可以不用自己添加分隔符，能减少bug率提升跨平台性 ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:18","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.basename(path) 获取路径中的文件名 ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:19","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"os.path.dirname(path) 获取路径中的目录名 ","date":"2017-04-01","objectID":"/posts/python-os-library/:0:20","tags":["Python"],"title":"Python 中 OS 库的常用方法","uri":"/posts/python-os-library/"},{"categories":null,"content":"0X00 HDFS的设计 HDFS作为GFS的开源实现，和GFS是高度一致的。在HDFS中有着下面的优点 对超大文件支持良好，由于其分布式实现，可以存储超大文件，甚至单个文件大小可以超过集群中任意一台机器的磁盘大小 采用流式数据访问，一次写入、多次读取是最高效的访问模式。因为Hadoop作为一个大数据处理平台，并没有频繁的写入操作，只是在需要的时候一次将大量的数据写入然后在对这些数据进行读操作 Hadoop并不需要运行在昂贵且高可靠的硬件之上，单个节点可能性能可靠性都参差不齐，但是由于其高可靠性的设计，使之能在遇到节点故障时继续运行且不让用户察觉到明显的终端 但是HDFS也不是适用于各种场景，一下的几种场景就不适 低时间延迟的数据访问，因为HDFS是以时间延迟为代价针对高数据吞吐量优化的，所以HDFS不适用与低延迟的数据访问 大量的小文件，因为Namenode的设计是将文件系统的元数据存储在内存中的，所以理论上HDFS中的文件最大数量受限于Namenode的内存容量，因此大量的小文件会占用Namenode大量的内存 HDFS目前不支持有多个写入者的操作，也不支持修改文件系统中的文件 0X01 数据块的设计 我们知道在传统的文件系统中就是分块的，寻址开销与分块的大小成负相关，磁盘利用率与分块大小成负相关。且HDFS的设计就是用来处理大文件的，所以将块设计的很大，默认为64MB且好多时候采用的是128MB的设置。 在HDFS中的块设计和传统文件系统有些不同，在传统文件系统中假设一个块为4kb，如果一个文件只有1kb则仍然会占用4kb的空间，但是在HDFS中一个小于块大小的文件并不会占据整个块的空间。 0X02 Namenode和Datanode HDFS中有两种节点，Namenode和Datanode。 其中Namenode管理文件系统的命名空间，他维护着文件系统树和整棵树内所有的文件和目录。 其中的Datanode负责存储并检索数据块，且定期向Namenode发送存储的块的列表。 0X03 Namenode容错机制 Namenode有两种常用的容错机制，第一种是实时将自己的操作和文件同步到NFS上，且是原子操作所以NFS上会有和Namenode完全相同的文件。另一种方式是运行一个辅助的Namenode，定期通过编辑日志合并命名空间镜像。 0X04 联邦HDFS 毕竟HDFS是为处理海量数据诞生的，所以避免不了海量的集群来搭建HDFS，但是前面也说过因为设计的问题导致一个拥有大量文件的集群会对Namenode的内存造成严峻的考验，这时候可以使用联邦HDFS来解决。在联邦环境下可以配置多个Namenode，每个Namenode负责维护一个命名空间卷。也就相当于每个Namenode负责一个目录树中的子目录，这样就可以保证在HDFS中有大量文件的时候也不会对Namenode造成太大的威胁。 0X05 高可用性 虽然Namenode有了备份但是还是存在Namenode的单点问题，也就是说当Namenode出现故障之后依旧会对HDFS整个文件系统造成影响，虽然有备份但是还是要等到有下一台Namenode节点上线之后才会运行，所以当时运行的MapReduce等程序依然会终止。针对这种问题Hadoop在2.x中做出了适当的处理。可以配置一对活动-备用的Namenode用于做热备份。 Namenode之间可以共享编辑日志，且使用高可用的方式实现共享存储 Datanode需要同时向两个Namenode发送数据块处理报告，因为要保持两个Namenode完全相同 在这样的配置之下当活动的Namenode故障以后备用的Namenode可以在几十秒内实现任务接管。且在备用的Namenode也失效的情况下还可以通过配置来指定另一台备用Namenode用于做冷启动。 ","date":"2017-03-06","objectID":"/posts/hdfs-simple/:0:0","tags":["HDFS","Hadoop","FS"],"title":"浅析 HDFS","uri":"/posts/hdfs-simple/"},{"categories":null,"content":" String类中每一个看起来会修改String值得方法，实际上都是创建了一个全新的String对象，以包含修改后的字符串内容。而最初的String对象则丝毫未动。 —《Java编程思想》第13章 0X00 String常量池 如果使用常用的方式定义两个内容完全一样的字符串，那么Java使用常量的方式，也就是说第二个字符串并没有生成一个对象而是引用了之前的字符串，导致他们的本质是一样的，所以当使用==判断两个字符串对象是否是同一个对象的时候，会显示是同一个对象。但是如果我们每次声明一个字符串的时候使用new String()的方式，则会每次创建一个String对象，两者就不是同一个对象了。 public class Main { public static void main(String args[]) { // 两个相同的字符串引用自同一处 String str1 = \"hello,world\"; String str2 = \"hello,world\"; System.out.println(str1 == str2); // 这两个字符串就是每次生成一个新对象 String str3 = new String(\"hello,world\"); String str4 = new String(\"hello,world\"); System.out.println(str3 == str4); } } 0X01 StringBuilder 字符串的拼接在Java中非常方便，但常用的使用+来拼接字符串效率很低，在需要拼接的次数不是很多的时候不会影响多少效率，但当需要拼接的字符串数量很多的时候就需要使用StringBuilder来拼接。该类中有一个append()的方法，就是将一个字符串连接到本对象的字符串后面。下面我们来对比一下这两个拼接方法的速度差异。 public class Main { public static void main(String args[]) { String string = \"hello,world\"; long start = new Date().getTime(); // 循环连接1W次 for (int i = 0; i \u003c 10000; i++){ string += \"hello,world\"; } long middle = new Date().getTime(); StringBuilder stringBuilder = new StringBuilder(\"hello,world\"); // 循环连接1000W次 for (int i = 0; i \u003c 10000000; i++){ stringBuilder.append(\"hello,world\"); } long end = new Date().getTime(); System.out.println(\"使用+连接耗时: \" + (middle - start)); System.out.println(\"使用StringBuilder连接耗时: \" + (end - middle)); } } 执行结果如下 使用+连接耗时: 850 使用StringBuilder连接耗时: 263 速度差异很明显，使用加号连接只连接一万次就耗时800多毫秒，而使用StringBuilder即使连接一千万次也只需要200多毫秒。 该类中还有其他的方法insert()指定位置插入 / replace()分片赋值 / substring()提取子字符串 / reverse()翻转字符串 / toString()返回字符串 使用演示 public class Main { public static void main(String args[]) { StringBuilder stringBuilder = new StringBuilder(\"0123456789\"); stringBuilder.insert(5, \"hello,world\"); // 插入字符串 System.out.println(stringBuilder.toString()); stringBuilder = new StringBuilder(\"0123456789\"); stringBuilder.replace(3, 5, \"hello,world\"); // 分片赋值 System.out.println(stringBuilder.toString()); stringBuilder = new StringBuilder(\"0123456789\"); stringBuilder.substring(3, 5); // 提取子字符串 System.out.println(stringBuilder.toString()); stringBuilder = new StringBuilder(\"0123456789\"); stringBuilder.reverse(); // 翻转字符串 System.out.println(stringBuilder.toString()); } } 0X02 String的其他方法 String类中有很多方法，这里有几个常用的 public class Main { public static void main(String args[]) { String string; boolean bool; string = \"hello,world\"; bool = string.equals(\"hello,world\"); // 判断字符串是否相同 System.out.println(bool); string = \"hello,world\"; bool = string.contains(\"hello\"); // 检查字符串中是否有另一个字符串 System.out.println(bool); string = \"hello,world\"; bool = string.startsWith(\"he\"); // 是否以某个字符串开头 // bool = string.endsWith(\"ld\"); // 结尾 System.out.println(bool); string = \"hello,world\"; string = string.replace(\"world\", \"java\"); // 字符串搜索替换 System.out.println(string); string = \"hello,world\"; string = string.toUpperCase(); // 全转成大写 // string = string.toLowerCase(); // 小写 System.out.println(string); string = \" hello,world \"; string = string.trim(); // 去掉字符串两头的空白 System.out.println(string); } } 0X03 String的正则方法 简单的正则匹配可以直接使用String类中的方法，比如查看字符串是否符合某正则表达式的matches()和切割字符串的split()。 public class Main { public static void main(String args[]) { String string = \"java0python00cpp\"; System.out.println(string.matches(\"[\\\\w\\\\d]+\")); // 检测是否能匹配 System.out.println(); String[] strings = string.split(\"\\\\d+\"); // 按正则表达式分割字符串，返回字符串数组 for(int i = 0; i \u003c strings.length; i++){ System.out.print(strings[i] + \" \"); } } } ","date":"2017-02-13","objectID":"/posts/java-string/:0:0","tags":["Java","String"],"title":"Java 中的字符串","uri":"/posts/java-string/"},{"categories":null,"content":" 文章中的代码仅在Python3中测试成功，没有在Python2中测试。 0X00 退出程序，显示错误信息 写脚本的时候经常会有执行出错，出错的时候可以用一句话把程序退出并且打印错误信息 raise SystemExit('error message') 0X01 输入密码 有的时候需要输入用户名和密码，使用input()输入用户名自然没有问题，但是用相同的方法输入密码的时候时使用明文的。长期用Linux的可能对Linux中密码的输入比较有印象，输入密码的时候是密文，且没有任何提示，包括星号，所以用这种方法输入密码是非常安全的。使用petpass库可以简单的输入用户名和密码，输入用户名最简单还是input()，如果要获取当前登录的用户名就可以使用getpass.getuser()，输入密码就可以使用getpass.getpass()来实现Linux中的那种密码输入。 如果测试的时候有问题可以在命令行下测试，比如Windows的CMD或者Linux的终端 #!/usr/bin/python # coding=utf-8 import getpass if __name__ == \"__main__\": user = input(\"Username:\") # user = getpass.getuser() passwd = getpass.getpass() 0X03 执行命令 在Linux中想要用Python代替Shell必然会出现在Python中调用命令的时候，那么这个时候就可以用这个方法来执行命令并获得输出内容。subprocess.check_output([])这个方法的参数是一个列表，列表里是一个或多个字符串，就像下面介绍的那样把命令通过空格拆分开，放到这个列表中。check_output返回的是一个二进制串，可以对其进行编码转变成人类可读的字符串。 这种方法只在Linux里测试过 毕竟没几个人会在WIndows下写脚本是吧。 #!/usr/bin/python # coding=utf-8 import subprocess if __name__ == \"__main__\": out_bype = subprocess.check_output(['ls', '/dev']) out_text = out_bype.decode('utf-8') print(out_text) 0X04 复制/移动 文件/目录 在Python中复制移动文件和目录非常简单，尤其是在不考虑链接的情况下。 #!/usr/bin/python # coding=utf-8 import shutil if __name__ == \"__main__\": shutil.copy('/etc/passwd', 'passwd') # 将/etc/passwd复制到当前目录，等同于Linux中的 cp /etc/passwd passwd shutil.copytree('/etc', 'etc') # 复制目录 shutil.move('passwd', 'mima') # 移动文件，也可以重命名，和Linux中的mv命令一样 0X05 获取终端大小 在Linux中一般是在终端下工作，那么有的时候需要知道当前终端大小来控制输出的字符串长度。 #!/usr/bin/python # coding=utf-8 import os if __name__ == \"__main__\": sz = os.get_terminal_size() columns = sz.columns lines = sz.lines print(sz) print(columns) print(lines) 0X06 os.walk() 经常需要遍历一个目录，来获取目录中的内容，如果只需要查看一个目录，那么使用os.listdir()就足够了，如果只判断一个文件是否为目录，则os.path.isdir()也足够了。但是有的时候我们需要逐层遍历目录，且区别对待目录和文件，那么通常会自己手写一个递归的方法来解决。这样虽然能解决问题，但是毕竟多写了代码且效率还不高，其实os库里有一个方法值得我们使用就是os.walk()。这个方法接收一个目录作为参数，返回一个迭代器，每次迭代是一个元组，元组有三个元素，第一个元素是当前路径，第二个元素是当前目录下的目录名，第三个元素是当前目录下的文件。具体的可以看代码注释 #!/usr/bin/python # coding=utf-8 import os if __name__ == \"__main__\": files = os.walk('D:/movie') # 这里调用了方法，传入一个路径，返回一个可迭代对象 for i in files: # 开始迭代 print('path_name: ', i[0]) # 输出当前路径 print('dir_name : ', i[1]) # 当前目录下的目录 print('file_name: ', i[2]) # 当前目录下的文件 print('-----------------') 这个输出是下面这样的 path_name: D:/movie dir_name : ['加勒比海盗', '机械师', '火影忍者', '蜘蛛侠'] file_name: ['V字仇杀队.mkv', 'wikileaks-720p.mkv', '湄公河行动.mkv', '盗梦空间.mkv', '神奇动物在哪里.mp4', '绝地逃亡.mkv'] ----------------- path_name: D:/movie\\加勒比海盗 dir_name : [] file_name: ['加勒比海盗1：黑珍珠号的诅咒.mkv', '加勒比海盗2：聚魂棺.avi', '加勒比海盗3：世界尽头.avi', '加勒比海盗4：惊涛怪浪.mkv'] ----------------- path_name: D:/movie\\机械师 dir_name : [] file_name: ['机械师1.mkv', '机械师2：复活.mp4'] ----------------- path_name: D:/movie\\火影忍者 dir_name : [] file_name: ['火影忍者：博人传.mp4', '火影忍者：忍者之路.mkv', '火影忍者：终章.mp4'] ----------------- path_name: D:/movie\\蜘蛛侠 dir_name : [] file_name: ['蜘蛛侠1-2002.mkv', '蜘蛛侠2-2004.mkv', '蜘蛛侠3-2007.mkv', '超凡蜘蛛侠1-2012.mkv', '超凡蜘蛛侠2-2014.mp4'] ----------------- 0X07 修改配置文件 在Linux中有大量的配置文件，Windows中也有一些ini格式的配置文件，语法都一样的。那么用脚本来修改配置文件当然不必要全部读完整个文件后正则匹配，有一个非常简单且好用的方法。下面是我用来做测试的实例配置文件，命名为1.ini放在D:/根目录。 [home] phone = On kindle = Off learn = False [school] phone = On kindle = On learn = True 可以看到，手机无论在哪都开机，Kindle只有在学校才用，学习也只有在学校才学。那么我们可以通过下面的方式来读取和修改这个配置文件。 #!/usr/bin/python # coding=utf-8 from configparser import ConfigParser import sys if __name__ == \"__main__\": cfg = ConfigParser() # 实例化一个对象 cfg.read('D:/1.ini') # 读取配置文件 tables = cfg.sections() # 获取标签 print(tables) phone_value = cfg.get('home', 'Phone') # 获取home标签下Phone的值 print(phone_value) kindle_value = cfg.get('school', 'Kindle') # 获取school下Kindle的值 print(kindle_value) learn_value = cfg.get('school', 'learn') # 获取school下learn的值 print(learn_value) cfg.set('home', 'learn', 'True') # 修改home下的learn为True f = open('D:/1.ini', 'w') # 用可写模式打开文件 cfg.write(f) # 将数据写回 f.close() # 关闭文件 0X08 打开浏览器 不管是要给用户展示一个页面还是要将数据用HTML形式展示出来，都需要打开浏览器，这个在Python中可以用一行代码来搞定。webbrowser.open_new('http://blog.just666.cn')可以打开一个新的浏览器窗口，并打开这个链接，webbrowser.open_new_table('http://blog.just666.cn')可以在当前浏览器窗口新开一个标签。（需要先导入webbrowser这个包） ","date":"2017-01-31","objectID":"/posts/python-magic-5/:0:0","tags":["Python"],"title":"Python 奇技淫巧 (五) 系统脚本","uri":"/posts/python-magic-5/"},{"categories":null,"content":" 文章中的代码仅在Python3中测试成功，没有在Python2中测试。 0X00 指定编码 每个文本文件都是以某一编码格式保存的，如果解码格式和文本格式不同就会出现乱码，在Python中可以简单的控制用什么编码来打开文件以读写文件。使用open打开文件的时候指定一个encoding参数就可以使用其他而非默认编码打开文件了。这里用到了一个打开文件的方式是with open() as f:这样，这样做的话在这个with下面的代码块中可以直接调用f这个文件对象，并且执行到with代码块之外的时候会自动关闭文件，不需要再手动关闭文件。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': # 使用utf-8编码，写模式，打开文件D:/test.txt with open('D:/test.txt', 'w', encoding='utf-8') as f: f.write('你好，世界') # 写一行汉子 # 使用utf-8编码，读模式，打开文件D:/test.txt with open('D:/test.txt', 'r', encoding='utf-8') as f: print(f.read()) # 因为是编码相同所以可以正常读出文字 # 使用latin-1编码，读模式，打开文件D:/test.txt with open('D:/test.txt', 'r', encoding='latin-1') as f: print(f.read()) # 因为使用的编码格式不同，所以会出现乱码 0X01 输出重定向 在Linux中可以对命令的输出进行重定向，将本应该输出到屏幕的东西输出到指定的文件里，在Python中也是可以简单做到这一点的。假设一个已经用写入模式打开的文件对象f，在输出文字的时候就可以直接这样调用print('hello,world', file=f)就可以直接将输出的内容重定向到文件中。这里需要注意的就是文件必须已经用可写模式打开，且是文本模式。 0X02 指定分隔符和结尾 我们可以使用这样一条语句打印多个字符串print('hello', 'world', 'hello', 'python')，会直接将字符串连接到一起，默认没有分隔符且使用系统默认作为结尾符号。可以给print()指定两个参数来设置分隔符和结尾符。print('hello,', world', 'hello', 'python', sep=',', end='\\n')这里指定了使用逗号分隔开这些字符串，并且使用\\n作为结尾符号。如果使用空字符串作为结尾符号，输出的时候最后就不自动换行。 0X03 读写二进制文件 有一个最常见的二进制文件读写实例：从网上下载东西到本地。比如有一个url是http://blog.just666.cn/usr/themes/Themia/img/sj/134.jpg，那怎么把这个图片下载到本地呢？可以使用下面这段代码。先找到url，然后使用urlopen打开这个网络文件并获取到文件内容，最后用二进制模式写入到新的本地文件就可以了。 #!/usr/bin/python # coding=utf-8 from urllib.request import urlopen if __name__ == '__main__': web_img = urlopen('http://blog.just666.cn/usr/themes/Themia/img/sj/134.jpg') # 使用urlopen打开一个url web_img = web_img.read() # 获得文件内容，当然这里是二进制的所以没有可读性 # 新打开一个文件，使用二进制写入模式 with open('D:/hey.jpg', 'wb') as f: # 在w后面指定一个b也就是二进制写入模式 f.write(web_img) # 将新文件写入进去 0X04 路径名 在Python中可以使用os.path处理路径名的问题，比较常用的三个方法os.path.basename()、os.path.dirname()、os.path.join()，分别用来显示一个完整地址的最后文件名、显示某完整地址文件的目录地址、将目录和文件拼接起来。因为Python比较强大，所以可以做到容错效果，比如说在Windows中地址是这样的D:\\game\\steam\\steamapps\\csgo，但是如果我写成了Linux下的格式D:/game/steam/steamapps/csgo也是没有问题的，照样可以用这些方法处理，没有影响。 #!/usr/bin/python # coding=utf-8 import os if __name__ == '__main__': path = '/var/www/html/index.html' # 这里是一个完整地址的文件 print(os.path.basename(path)) # 可以显示文件名 index.html print(os.path.dirname(path)) # 可以显示当前文件的地址位置 /var/www/html/ print(os.path.join('D:/', 'hehe.exe')) # 将连接拼起来编程 D:/hehe.exe 0X05 小技巧 检验文件是否存在：os.path.exists('D:/test.txt')如果文件存在则返回True否则就是False 获取文件元数据：os.path.getatime('D:/test.txt')查看修改时间 getsize获取文件大小 ","date":"2017-01-24","objectID":"/posts/python-magic-4/:0:0","tags":["Python"],"title":"Python 奇技淫巧 (四) 文件\u0026I/O","uri":"/posts/python-magic-4/"},{"categories":null,"content":" 文章中的代码仅在Python3中测试成功，没有在Python2中测试。 0X00 任意个参数 Python中一般定义函数是这样的def add(a, b)，参数的个数是固定的，那么怎么才可以接收任意多个参数就像rm 1.txt 2.jpg 3.mp3 4.cpp这样？很简单，使用*和**就可以。下面代码里第一个参数a接收到了hello,world而*b则接收到了其余所有的参数，将其作为一个元组。 #!/usr/bin/python # coding=utf-8 def add(a, *b): print(a) return b if __name__ == '__main__': x = add('hello,world', 2, 3, 4, 5) print(x) 0X01 添加注解 在Python中定义函数的同时可以也给函数添加注解，注解可以帮助我们在调用函数的时候起到一个提醒的作用。虽然几十行的代码不会遇到看不懂的情况，但是在修改别人代码或者编写一个大项目的时候必然会有这种问题。我们可以直接在代码中加注释来解释说明，但是使用注解还是要比注释来得简单方便。不过通过注解注解指定的类型不像是C语言那样有实际意义，就算是你传入的参数和返回的值不是按照注解来的也不会报错。 #!/usr/bin/python # coding=utf-8 def add(a: int, b: int) -\u003e int: # 这里声明了a和b都是int型，返回值也是int型 return a + b if __name__ == '__main__': print(add(3, 5)) 0X02 默认参数 我们常用的一些内置函数是有好多个可选参数的，不过我们不需要每个参数都要传入，因为Python可以给参数设置默认值，如果没有传入那个参数就会选择使用默认值，比如下面这个add函数。 #!/usr/bin/python # coding=utf-8 def add(a = 3, b = 5): return a + b if __name__ == '__main__': print(add()) # 没有任何参数，默认使用３和５，最后结果则为8 print(add(1)) # 传入了参数a为1，最后结果则为6 print(add(4, 6)) # 传入了参数a和b分别为4和6，最后结果为10 print(add(b = 3)) # 值传入了b参数为3，所以最后结果为6 print(add(b = 3, a = 10)) # 指定参数的话也可以不按顺序 0X03 函数mini 匿名函数 这里称之为匿名函数感觉还是有点别扭，因为这儿定义的函数并不是真的匿名，也是有名字的，因为函数自身非常短小倒不如称之为函数mini。在Python中有一个关键字lambda，可以定义一个匿名函数，使用这个关键字定义函数的时候函数声明、返回值、函数体只能写成一行。这样的函数功能肯定不能很强大，不过确实能减少代码量，少写好多重复的代码。正式代码的第一行就定义了一个函数，名为add，参数是x和y，返回值是x+y。所以说标准是这样的函数名 = lambda 参数 ： 返回值。这里还有个例子：my_sqrt = lambda x : math.sqrt(x)。注意，在匿名函数里什么if-else、while、try-except都是不能用的，总之你的函数就只能写一行。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': add = lambda x, y : x + y print(add(3, 5)) print(add(2, 7)) print(add(1, 9)) ","date":"2017-01-21","objectID":"/posts/python-magic-3/:0:0","tags":["Python"],"title":"Python 奇技淫巧 (三) 函数","uri":"/posts/python-magic-3/"},{"categories":null,"content":" 文章中的代码仅在Python3中测试成功，没有在Python2中测试。 0X00 split升级 字符串有一个split方法，可以用某个字符或字符串把源字符串切开。但是存在一个弊端，切割位置是固定的，不能灵活切割。有这样一个需求，将这个字符串hello 1 wrld 2 python 3 linux切割开，以每个数字为分隔符。这样标准的str.split就不能完成任务了。但是在re模块中有一个re.split可以完成这任务。这个方法的分隔符不是使用准确不变的字符/串而是使用正则表达式。 #!/usr/bin/python # coding=utf-8 import re if __name__ == '__main__': my_str = 'hello 1 wrld 2 python 3 linux' res = re.split('[0-9]', my_str) print(res) 这里使用的正则表达式就是普通的字符串形式，而不需要re.compile进行编译。有了这个方法就可以更加灵活地切割字符串了。 0X01 字符串开头结尾的匹配 当我们有一堆的url，想在url中找到http开头且.jpg结尾的图片文件，以前我总是直接str.strip('http://') == str来判断开头是不是’http://‘但是这样太蠢了，而且也不是很靠谱、因为万一开头不是而结尾是的话就会误判。这里有两个方法可以非常简单地做出这种判断：str.startswith()和str.endswitch()两个。一个是用来判断字符串是否以xxx开头、另一个是用来判断字符串是否以xxx结尾。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': url = 'http://blog.just666.cn/img/01.jpg' print(url.startswith('http://')) print(url.endswith('.jpg')) 这种方式可以有一个简单的改变，使用列表推导式来批量判断。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': url_list = ['http://hey.sdf.we/sdfw.jpg', 'http://asdf.ser.x/zxvw.jpg', 'http://sdf.re.xcv/ind.html', 'http://zx.er.cxv/held.html', 'http://zx.sdf.vs/hell.jpg'] # 这里是列表推导式 jpg_list = [jpg for jpg in url_list if jpg.endswith('.jpg')] print(jpg_list) 也可以将后两行换成print(all(jpg.startswith('.jpg') for jpg in url_list))就会输出False因为并不是所有都以’.jpg’结尾。 还可以使用匹配的方式，比如你需要在N多url中找到’http/ftp’这两个协议的url，可以这么写 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': choices = ['http://', 'ftp://'] choices = tuple(choices) # 这里必须要使用元组类型 url = 'http://www.baidu.com' print(url.startswith(choices)) url = 'ftp://192.168.1.2' print(url.startswith(choices)) url = 'https://www.taoba.com' print(url.startswith(choices)) startswith和endswitch两个函数完全可以被正则表达式替代，但是对于简单匹配来说没必要用正则表达式，这两个函数比正则要快且可读性搞书写快。 0X02 Shell通配符 在匹配字符串的时候不仅可以使用比较复杂的正则表达式，还可以用比较简单的通配符。使用通配符需要注意的一个问题就是大小写。在Linux/Unix/Mac上要区分大小写，在Windows上不区分大小写。fnmatch下有两个方法，fnmatch按操作系统来判断到底区不区分大小写，而fnmatchcase则强制区分大小写。使用方法如下： \u003e\u003e\u003e from fnmatch import fnmatch, fnmatchcase \u003e\u003e\u003e filename = 'hello.c' \u003e\u003e\u003e fnmatch(filename, '*.c') True \u003e\u003e\u003e fnmatch(filename, 'hell?.c') True \u003e\u003e\u003e fnmatch(filename, 'hellO.c') False \u003e\u003e\u003e fnmatch(filename, 'hello.c') True 0X03 查找替换 将字符串A中所有的某个子字符串B替换为另外的字符串C，可以简单的使用字符串的replace函数 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': text = 'hello world hello python' print(text.replace('hello', 'hey')) 还有一种使用re模块的方案，可以使用正则匹配来查找并替换。re.sub()方法可以做到这一点。这里sub的第一个参数是匹配的正则表达式，第二个参数是替换的字符串（其中\\1 \\2 \\3表示匹配的编号），第三个参数就是待匹配替换的字符串了。这个例子将1/19/2017转变为2017/1/19 #!/usr/bin/python # coding=utf-8 import re if __name__ == '__main__': text = 'hello world 1/19/2017' print(re.sub(r'(\\d+)/(\\d+)/(\\d+)', r'\\3-\\1-\\2', text)) 0X04 Unicode大法好 我们有的时候会遇到一些奇怪的字符串问题，比如看起来明明完全一样的两个字符串在对比的时候居然不相等。得益于Python3使用的Unicode我们可以简单的对字符串统一规范。 #!/usr/bin/python # coding=utf-8 import unicodedata if __name__ == '__main__': s1 = 'char: \\u00f1' s2 = 'char: n\\u0303' print(s1) print(s2) print(s1 == s2, ' ', len(s1), ' ', len(s2)) # 改一下编码 s1 = unicodedata.normalize('NFC', s1) s2 = unicodedata.normalize('NFC', s2) print(s1) print(s2) print(s1 == s2, ' ', len(s1), ' ', len(s2)) 这里面用到的那个奇怪的字符我也不知道是什么，是在《Python Cookbok》这本书上找的例子。就是说看起来\\u00f1这个字符和n\\u0303是一样的，但是很明显前者是一个字符而后者是两个字符，所以我们在对比的时候才会出现字符串不相同甚至长度不同的问题。然后引入了unicodedata模块之后用里面的normalize方法可以将字符串规范化，s1 = unicodedata.normalize('NFC', s1)就是将s1采用NFC方式规范。所谓NFC方式就是 全组成 也就是说“如果可能的话就是用单个代码点，也就是s1那种方式”（这里和近场通讯的NFC很明显没半点关系）。可选的除了NFC还有NFD（尽量使用组合字符，也就是s2那种方式），还支持NFKC和NFKD这里就自行Google一下吧。 0X05 字符串对齐 有的时候我们需要对字符串进行对齐操作，比如在终端中模拟界面之类的。可以使用C语言风格的%10S这种去替代，但是有更好用简单的方法，就是使用字符串内置的ljust/rjust/center方法。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': text = 'Main Menu' print('左对齐：', text.ljust(30)) print('右对齐：', text.rjust(30)) print('中对齐：', text.center(30)) print('左对齐填充：', text.ljust(30, '+')) print('右对齐填充：', text.rjust(30, '=')) print('中对齐填充：', text.center(30, '*')) 还有一个炫酷的融合函数叫format。这个函数接收两个参数，第一个参数是待处理字符串，第二个参数是选项。具体选项如下：其中’^‘是居中，’\u003e‘是右对齐，’\u003c‘是左对齐，后面跟着的数字是宽度，对齐字符前面是填充字符。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': text = 'Main Menu' print(format(text,","date":"2017-01-19","objectID":"/posts/python-magic-2/:0:0","tags":["Python"],"title":"Python 奇技淫巧 (二) 字符串、文本","uri":"/posts/python-magic-2/"},{"categories":null,"content":" 文章中的代码仅在Python3中测试成功，没有在Python2中测试。 0X00 *表达式 从某个可迭代对象中分解出N个元素，但是这个可迭代的对象可能会超过N，会出现too many values to unpack异常。 比如我这儿有N个统计信息，因为第一次和最后一次的信息不准确需要删除掉，而将中间的信息保留下来，那么就可以这么弄。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': grade = [23, 45, 42, 45, 78, 98, 89, 97, 69, 77, 88, 50, 65, 99, 98] first, *new_grade, last = grade print(new_grade) 这里的赋值就是将第一个和最后一个赋给了first和last，而中间的给了new_grade 0X01 定长队列 有一种情况：程序在运行的时候会记录日志，比如说web程序的访问历史。如果我们需要只保留最后的1W条数据，那么很快能想到使用一个列表，每次插入数据的时候判断长度，然后对应的append和del。但是有一个更简单且更快速的方法就是使用collections.deque()。下面的例子中有一个1024长的列表，我们列表里只存最新的7条。 #!/usr/bin/python # coding=utf-8 # 导入一个包 import collections if __name__ == '__main__': # 当做一种数据解雇来用就可以 auto_queue = collections.deque(maxlen=7) my_list = range(1024) for i in my_list: auto_queue.append(i) print(auto_queue) 运行之后可以看到，列表里只保存了最后插入的七条数据。 0X02 最大最小的几个元素 当我们有一个列表，需要找到列表里最大的N个元素时，一般会想到先排序然后分片，这想法当然不多，但是还有一个更好用的方法： #!/usr/bin/python # coding=utf-8 import heapq if __name__ == '__main__': my_list = [34, 234, 56, 56, 345, 456, 23, 213, 456, 8, 98, 43, 2, 67] print('max: ', heapq.nlargest(3, my_list)) # 找到最大的三个 print('min: ', heapq.nsmallest(2, my_list)) # 找到最小的两个 我这里用列表来演示，但是这个方法支持更复杂的数据结构。比如我有一个列表，列表里包含很多个字典，字典里是学生考试信息，那么我就可以用考试分数来找到前三名： #!/usr/bin/python # coding=utf-8 import heapq if __name__ == '__main__': my_list = [{'name': '小明', 'grade': 56}, {'name': '小红', 'grade': 87}, {'name': '小刚', 'grade': 67}, {'name': '小志', 'grade': 46}, {'name': '小逗逼', 'grade': 99}, {'name': '小华', 'grade': 85},] print('max: ', heapq.nlargest(3, my_list, key=lambda s: s['grade'])) print('min: ', heapq.nsmallest(3, my_list, key=lambda s: s['grade'])) key 后面的 lambda s: s['grade']是用了一个 匿名函数 。列表里唯一的值就是排序的关键字。更多关于更多关于匿名函数 如果N相对总数据量来说很小，可以用heapq.heapify()获得更好的性能。这个函数会将原来的集合转变成列表并以 堆 的形式排序。而堆最重要的一个特性就是最小的那个元素一定在第一位。所以我们可以利用这个性质来获取最小的前N个。 #!/usr/bin/python # coding=utf-8 import heapq if __name__ == '__main__': my_list = [234, 324, 456, 567, 345, 23, 546, 567, 98, 45, 2, 576] heapq.heapify(my_list) print(my_list) # 查看排序结果 print(heapq.heappop(my_list)) # 取第一个元素，并重拍 print(heapq.heappop(my_list)) print(heapq.heappop(my_list)) 0X03 优先级队列 普通队列都是按照FIFO(first in first out)来增删数据，有些特殊情况需要给每个元素设定优先级，push元素的时候设定优先级，pop的时候找到优先级最高的。比如说操作系统的任务调度就是这样的，会给每个进程设置优先级。不过当然，不会使用Python实现的了。这里的内部也是用堆来实现的，所以在15行的位置用了-priority来让堆反向排、 #!/usr/bin/python # coding=utf-8 import heapq # 这个类就是队列类 class PriorityQueue: def __init__(self): self._queue = [] # 队列元素 self._index = 0 # 索引 def push(self, item, priority): heapq.heappush(self._queue, (-priority, self._index, item)) self._index += 1 def pop(self): return heapq.heappop(self._queue)[-1] # 队列中的数据类型 class Item: def __init__(self, name): self.name = name # 只有一个属性、name def __repr__(self): return 'Item({!r})'.format(self.name) # 将格式化好的字符串返回 if __name__ == '__main__': my_queue = PriorityQueue() # 实例化一个优先级队列 my_queue.push(Item('内核'), 99) # 内核的优先级最高了 my_queue.push(Item('文件复制'), 40) my_queue.push(Item('CS:GO'), 75) print(my_queue.pop()) # 找到优先级最高的 0X04 一键多值 我们可以轻松的写出用一个键对应多个值的字典，只需要让键对应到列表或者集合就好了，但是要啰里啰嗦写一大堆东西。其实可以用一个内建的方法来解决这个问题。通过这个方法可以快速创建这种字典，也可以像操作普通列表一样操作里面的数据。 #!/usr/bin/python # coding=utf-8 from collections import defaultdict if __name__ == '__main__': my_dic = defaultdict(list) my_dic['name'].append('李华') my_dic['qq'].append('66666') my_dic['qq'].append('23333') my_dic['qq'].append('88888') print(my_dic) ```python # 0X05 分片命名 Python中分片非常好用，有的时候会在程序中出现很多分片，管理起来特别麻烦。可以通过这种方式给分片命名，下次再次调用的时候可以直接使用分片的名字。 ```python #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': data = 'shawn 17 M' name = slice(0, 5) age = slice(7, 8) sex = slice(9, 10) print(data[name]) print(data[age]) print(data[sex]) 0X06 词频统计 从一个序列中找到出现次数最多的元素。Counter对象还可以进行简单的加减，比如a序列里出现了10次’hello’而b序列里出现了3次’hello’，那么a+b的话’hello’的值就会变成13。 #!/usr/bin/python # coding=utf-8 from collections import Counter if __name__ == '__main__': data = ['hello', 'world', 'hey', 'hello', 'world', 'jack', 'hey', 'york', 'hey', 'hello', 'hello'] word_count = Counter(data) print(word_count.most_common(1)) # 这个参数1可以更改，表示的是出现次数最","date":"2017-01-15","objectID":"/posts/python-magic-1/:0:0","tags":["Python"],"title":"Python 奇技淫巧 (一) 列表、集合、字典","uri":"/posts/python-magic-1/"},{"categories":null,"content":"0X00 实例方法 Python中的实例方法是在面向对象编程中用到的最多的方法类型了。 实例方法 从字面理解就可以，就是说这个方法是属于实例的。每次实例化一个对象出来，这个对象都会拥有这个方法。从下面代码中就可以看得出来，这里我定义了一个实例方法’get_name()’，定义实例方法不需要任何特殊的修饰符。 #!/usr/bin/python # coding=utf-8 class Student: def __init__(self): self.name = None # 一个实例方法 def get_name(self): return self.name if __name__ == '__main__': a = Student() a.name = '小明' print a.get_name() b = Student() b.name = '小红' print b.get_name() 从运行结果可以看出来，针对每一个实例，调用实例方法的输出是不同的，也就可以证明这个方法是属于某个实例的。 小明 小红 0X01 静态方法 静态方法用的也很多，比如我们写正则表达式的时候经常会用到表达式的编译，一般都是这么写的’re.compile()‘这里就是一个静态方法。可以看到我们在调用编译方法的时候是并没有实例化一个re对象的。所以可以知道 静态方法 就是不需要实例化对象即可调用的方法。下面有一个例子，例子中还是上面的Student类，但是定义了一个静态方法’say_hello()’，因为这是一个静态方法，所以不需要实例化对象即可调用。正因为这些特点，在定义静态方法的时候没有一个默认的参数self。 #!/usr/bin/python # coding=utf-8 class Student: def __init__(self): self.name = None def get_name(self): return self.name # 这里使用装饰器定义了一个静态方法 @staticmethod def say_hello(): print 'hello,world' if __name__ == '__main__': Student.say_hello() 运行结果如下。我个人觉得静态方法的最大作用就是实现一些工具类，比如某些固定的重复的操作之类的。 hello,world 0X02 类方法 使用类方法需要弄清楚类中属性的种类。类里有两种属性，一种是 类属性 一种是 实例属性 。顾名思义，类属性就是说这个属性是属于类的，这个类的所有实例共享着一个属性。实例属性就是属于实例的属性，每个实例的实例属性间不共享。下面这个例子里可以看到，类属性和实例属性是可以重名的，但是调用的时候要用’cls’或者’self’来制定到底调用的是哪个属性。下面的例子中有一个类属性’name’和一个实例属性’name’。 #!/usr/bin/python # coding=utf-8 class Student: # 声明并赋值了一个类属性 name = '默认的类属性' def __init__(self): self.name = '默认的对象属性' @classmethod def set_class_name(cls): cls.name = '修改的对象属性' def set_object_name(self): self.name = '修改的对象属性' @classmethod def get_class_name(cls): return cls.name def get_object_name(self): return self.name if __name__ == '__main__': a = Student() print a.get_class_name() print a.get_object_name() a.set_class_name() a.set_object_name() print a.get_class_name() print a.get_object_name() 通过运行结果可以清晰地看出类属性的使用方式。运行结果如下。首先实例化一个对象，获取了a的实例属性和Student的类属性，然后调用了实例方法和类方法对两个属性重新赋值，最后再输出一次。 默认的类属性 默认的对象属性 修改的对象属性 修改的对象属性 ","date":"2017-01-01","objectID":"/posts/python-function/:0:0","tags":["Python"],"title":"Python 的实例方法、静态方法、类方法","uri":"/posts/python-function/"},{"categories":null,"content":"0X00 修改配置文件 MySQL/MariaDB默认并没有采用utf-8编码，所以我们要修改配置文件，以让其使用utf-8。 /etc/my.cnf就是配置文件，打开之后在[mysqld]下面加入两行，使其变成 [mysqld] character_set_server=utf8 init_connect='SET NAMES utf8' 修改好配置文件之后重启服务 0X01 修改数据库的字符集 在修改配置文件之后新建的数据库默认就是使用utf-8了，但是之前的还不是所以要修改一下。登录到数据库，在命令行界面修改数据库的字符集。 ALTER DATABASE `databases_name` COLLATE 'utf8_bin'; 再次重启数据库服务。这样再连接到数据库就解决掉汉字变问号的问题了 0X02 推荐两款软件 大家好多人都在用Navicat，但绝大多数人用的都是盗版软件，这里推荐大家用一些好用的开源软件来替代。 ","date":"2016-12-15","objectID":"/posts/mysql-zh-exception/:0:0","tags":["MySQL"],"title":"解决 Linux 下 MySQL/MariaDB 中文变问号 ？ 问题","uri":"/posts/mysql-zh-exception/"},{"categories":null,"content":"1 HeidiSQL 一款开源软件 可以连接MySQL/MariaDB/SQL Server 官方中文支持 下载地址：HeidiSQL ","date":"2016-12-15","objectID":"/posts/mysql-zh-exception/:0:1","tags":["MySQL"],"title":"解决 Linux 下 MySQL/MariaDB 中文变问号 ？ 问题","uri":"/posts/mysql-zh-exception/"},{"categories":null,"content":"2 MySQL Workbench 一款开源软件 MySQL官方开发 导出表关系图非常强大 下载地址：MySQL Workbench ","date":"2016-12-15","objectID":"/posts/mysql-zh-exception/:0:2","tags":["MySQL"],"title":"解决 Linux 下 MySQL/MariaDB 中文变问号 ？ 问题","uri":"/posts/mysql-zh-exception/"},{"categories":null,"content":"0X00 最优算法—不可能实现算法 最优算法听起来很棒，但是 实现起来是不可能的 。最优算法是：当发生缺页中断时，将最晚会用到的页换出。也就是说，有三个页，现在发生了缺页中断，第一个页在第201条指令的时候会用到，第二个页在第5001条指令的时候会用到，第三个页在第20000条指令的时候会用到，那么第三个页面就是最晚会被用到的，就将其换出。这样确实是最好的效率，但是真正实现不了的原因是：程序不可能知道自己在什么时候需要哪些内存，所以就不能找到最晚会被用到的页。因为要用未来的事情来判断所以我一般称之为未来算法。虽说实现不了，也不是说这个算法就没意义了。这个算法最大的意义就在于可以比较效率。效率越是接近最优算法的就越好，当一个算法已经能达到最优算法效率的101%时，就没必要累死累活的去优化效率了，可以去找一些别的瓶颈了。 0X01 最近未使用—NRU 如果说最优算法叫未来算法的话，那么这个最近未使用就可以叫做历史算法，这样就好理解了。当系统发生缺页中断的时候，在内存中找到最久没被用过的页，将其换出。有一种实现方法：给每一个页设置一个 R(read)位和M(modify)位 。当一个进程启动的时候将这个进程的所有页的RM位都设置为0。然后每访问一个页就将R位置为1，每修改一个页就将M置为1。系统每隔一段时间就将所有页的R置为0。那么这里就会出现四种页，其实这里只是一个表示，比如第1类。不可能出现一个没被访问就修改的页，但是第3类页经过一段时间之后将R置为0的话就是第1类了。 类别 | 访问 | 修改 | R | M -------------------------------------- 第0类：| 没有被访问 | 没有被修改 | 0 | 0 第1类：| 没有被访问 | 已经被修改 | 0 | 1 第2类：| 已经被访问 | 没有被修改 | 1 | 0 第3类：| 已经被访问 | 已经被修改 | 1 | 1 -------------------------------------- 现在内存中的每个页都是这0到4的其中一种。那么当发生缺页中断的时候，NRU算法就会从类别编号最小的一堆页中 随机 换出。 0X02 先进先出—队列置换算法 FIFO 这种算法相对容易实现，就像是数据结构中的 队列 一样。每次的新页放在队列尾部，当发生缺页中断时将队列头部的页踢掉，将新页放到队列尾部。这种算法有一个非常严重的问题就是会踢掉一些必要的页，比如操作系统核心功能。想想系统启动的时候，首先加入了10000个页以用来运行操作系统，但是发生缺页中断的时候就会将核心页换出去，然后因为核心页被换出去了就需要再换进来，有可能就造成了连续10000个页从队首换到了队尾，产生了20000个操作。 0X03 第二次机会—FIFO改进 因为FIFO会将所有队尾直接踢出去，第二次机会就是给了每个页面再一次机会。也就是说：每个页面还是有一个R位，然后有一个时间位用来记录装入时间。每当发生缺页中断的时候查看队首页的R值，如果R值为0那么就将其换出，否则就将其R值设置为0，并设置‘装入时间’（在哪个时钟的时候装入的），然后再从队首的下一个页开始判断。 0X04 时钟算法—CLOCK 因为二次机会算法还是基于单向链表的，所以会经常需要在链表中移动页面，虽然是在内存中操作但还是会浪费资源。这里就把单向链表改成了 环形链表 。当发生缺页中断的时候，检查指针指向的页面，如果页的R位是0则提出这个页面将新的页加到这个位置，并设置R位为1；否则就将当前的R位设置为0并将指针下移。所以时钟算法也可以理解成是第二次机会算法的改进版本。 图片来源：《现代操作系统》 Andrew S. Tanenbaum 0X05 最近最少使用置换算法—LRU 有这样一种情况：“在前面几条指令中频繁使用的页很可能在后面的及条指令中被使用”，所以说已经很久没有用过的页很可能在未来的一段时间内也不会被用到。所以可以在发生缺页中断的时候将最久未使用的页替换出去。为了实现LRU算法需要将所有页串成一个链表，链表的一端是最常使用的页，另一端则是最不常用的页，每次调用一个页的时候都要将整个链表更新，但移动整个链表是很慢的。 可以通过特殊硬件来实现LRU。 第一种方案：这里需要一个64位的计数器，计数器在每条指令执行完成之后自动加一，且每个页表项中需要需要足够容纳这个计数器。在每次访问内存的时候将当前计数器的值赋值给该页表项的对应区域。当发生缺页中断的时候找到每个页表项中该值最小的，这个页表项就是最近最少使用的。 需要注意的一点是： 这个计数器只有一个 而不是每个页表项一个；每个页表项里只是有一个可以容纳这个计数器的位置，也就是说要有一个64位的空间来保存数字。这里保存的数字不会随着指令的执行而自增，随着指令执行自增的就只有那个唯一的计数器。 第二种方案：假设某机器有n个页框，那么LRU硬件就是一个n * n的矩阵，初始化为零矩阵。当访问页框k时 先 将k行全部置1， 再 将k列全部置0。在任意时间二进制数值最小的行就是最近最少使用的，第二小的就是下一个最近最少使用的。 0X06 最不常用置换算法—NFU 因为LRU算法需要独立的硬件设备，然而大多数计算机并没有这种硬件，所以需要一个能用软件实现的解决方案。这种成为NFU的最不常用置换算法就是一种使用软件模拟LRU的实现。在NFU中针对每一个页设置一个计数器，每当发生缺页中断时刷新所有的页对应的计数器，先将每一个页的R值（R值用来标识该页是否用过，为0或1）加到计数器上，再将R置0。这个计数器基本可以反映某个页的使用频率。当发生缺页中断的时候就可以踢出计数器最小的页。 这个算法的一大问题就是：记忆力太强。比如说我开机的时候开机相关的页可能调用了10万次，开机之后其他的东西并没有这么高的使用率，但是因为这些页的计数器太大了，所以不会被轻易踢出去，就会导致有一批‘元老页’滞留在内存中浪费空间。 可以通过一个简单的小修改解决这个问题：首先在R值加到计数器之前先将计数器右移一位（二进制移位，最后一位抛掉），其次将R位加入到计数器的最左端，而不是最右端（简单的NFU是加入到最右端的）。经过这种修改的算法称之为 老化算法 。因为这种算法中新的操作会对计数器产生较大的影响，可以让以前的计数器迅速变老，所以称为老化算法。 ","date":"2016-12-14","objectID":"/posts/page-swap-simple/:0:0","tags":["OS"],"title":"简述几种简单的页面置换算法","uri":"/posts/page-swap-simple/"},{"categories":null,"content":"0X00 多线程 多线程是个提高程序运行效率的好办法，本来要顺序执行的程序现在可以并行执行，可想而知效率要提高很多。但是多线程也不是能提高所有程序的效率。程序的两个极端是‘CPU密集型’和‘I/O密集型’两种，多线程技术比较适用于后者，因为在串行结构中当你去读写磁盘或者网络通信的时候CPU是闲着的，毕竟网络比磁盘要慢几个数量级，磁盘比内存慢几个数量级，内存又比CPU慢几个数量级。多线程技术就可以同时执行，比如你的程序需要发送N个http数据包（10秒），还需要将文件从一个位置复制到另一个位置（20秒），然后还需要统计另一个文件中’hello,world’字符串的出现次数（4秒），现在一共是要用34秒。但是因为这些操作之间没有关联，所以可以写成多线程程序，几乎只需要20秒就完成了。这是针对I/O密集型的，如果是CPU密集型的就不行了。比如我的程序要计算1000的阶乘（10秒），还要计算100000的累加（5秒），那么即使程序是并行的，还是会要用15秒，甚至更多。因为当程序使用CPU的时候CPU是通过轮转来执行的，IO密集型的程序可以在IO的同时用CPU计算，但是这里的CPU密集型就只能先执行一会儿线程1再执行一会儿线程2。所以就需要15秒，甚至会更多，因为CPU在切换的时候需要耗时。解决CPU密集型程序的多线程问题就是CPU的事情了，比如Intel的超线程技术，可以在同一个核心上真正的并行两个线程，所以称之为‘双核四线程’或者‘四核八线程’，我们这里具体的先不谈，谈我也不知道。 0X01 Python骗人 说了这么多多线程的好处，但是其实Python不支持真正意义上的多线程编程。在Python中有一个叫做GIL的东西，中文是 全局解释器锁 ，这东西控制了Python，让Python只能同时运行一个线程。相当于说真正意义上的多线程是由CPU来控制的，Python中的多线程由GIL控制。如果有一个CPU密集型程序，用C语言写的，运行在一个四核处理器上，采用多线程技术的话最多可以获得4倍的效率提升，但是如果用Python写的话并不会有提高，甚至会变慢，因为线程切换的问题。所以Python多线程相对更加适合写I/O密集型程序，再说了真正的对效率要求很高的CPU密集型程序都用C/C++去了。 0X02 第一个多线程 Python中多线程的库一般用thread和threading这两个，thread不推荐新手和一般人使用，threading模块就相当够用了。 有一个程序，如下。两个循环，分别休眠3秒和5秒，串行执行的话需要8秒。 #!/usr/bin/env python # coding=utf-8 import time def sleep_3(): time.sleep(3) def sleep_5(): time.sleep(5) if __name__ == '__main__': start_time = time.time() print 'start sleep 3' sleep_3() print 'start sleep 5' sleep_5() end_time = time.time() print str(end_time - start_time) + ' s' 输出是这样的 start sleep 3 start sleep 5 8.00100016594 s 然后我们对它进行修改，使其变成多线程程序，虽然改动没有几行。首先引入了threading的库，然后实例化一个threading.Thread对象，将一个函数传进构造方法就行了。然后调用Thread的start方法开始一个线程。join()方法可以等待该线程结束，就像我下面用的，如果我不加那两个等待线程结束的代码，那么就会直接执行输出时间的语句，这样一来统计的时间就不对了。 #!/usr/bin/env python # coding=utf-8 import time import threading # 引入threading def sleep_3(): time.sleep(3) def sleep_5(): time.sleep(5) if __name__ == '__main__': start_time = time.time() print 'start sleep 3' thread_1 = threading.Thread(target=sleep_3) # 实例化一个线程对象，使线程执行这个函数 thread_1.start() # 启动这个线程 print 'start sleep 5' thread_2 = threading.Thread(target=sleep_5) # 实例化一个线程对象，使线程执行这个函数 thread_2.start() # 启动这个线程 thread_1.join() # 等待thread_1结束 thread_2.join() # 等待thread_2结束 end_time = time.time() print str(end_time - start_time) + ' s' 执行结果是这样的 start sleep 3 start sleep 5 5.00099992752 s 0X03 daemon 守护线程 在我们理解中守护线程应该是很重要的，类比于Linux中的守护进程。但是在threading.Thread中偏偏不是。 如果把一个线程设置为守护线程，就表示这个线程是不重要的，进程退出的时候不需要等待这个线程执行完成。 ———《Python核心编程 第三版》 在Thread对象中默认所有线程都是非守护线程，这里有两个例子说明区别。这段代码执行的时候就没指定my_thread的daemon属性，所以默认为非守护，所以进程等待他结束。最后就可以看到100个hello,world #!/usr/bin/env python # coding=utf-8 import threading def hello_world(): for i in range(100): print 'hello,world' if __name__ == '__main__': my_thread = threading.Thread(target=hello_world) my_thread.start() 这里设置了my_thread为守护线程，所以进程直接就退出了，并没有等待他的结束，所以我们看不到100个hello,world只有几个而已。甚至还会抛出一个异常告诉我们有线程没结束。 #!/usr/bin/env python # coding=utf-8 import threading def hello_world(): for i in range(100): print 'hello,world' if __name__ == '__main__': my_thread = threading.Thread(target=hello_world) my_thread.daemon = True # 设置了标志位True my_thread.start() 0X04 传个参数 之前的代码都是直接执行一段代码，没有过参数的传递，那么怎么传递参数呢？其实还是很简单的。threading.Thread(target=hello_world, args=('hello,', 'world'))就可以了。args后面跟的是一个元组，如果没有参数可以不写，如果有参数就直接在元组里按顺序添加就行了。 #!/usr/bin/env python # coding=utf-8 import threading def hello_world(str_1, str_2): for i in range(10): print str_1 + str_2 if __name__ == '__main__': my_thread = threading.Thread(target=hello_world, args=('hello,', 'world')) # 这里传递参数 my_thread.start() 0X05 再来个多线程 threading有三种创建Thread对象的方式，但是一般只会用到两种，一种是上面0X02说的传个函数进去，另一种就是这里说的继承threading.Thread。在这儿我们自己定义了两个类，类里重写了run()方法，也就是调用start()之后执行的代码，开启线程就和之前开启是一样的。之前的方式更面向过程，这个更面向对象。 #!/usr/bin/env python # coding=utf-8 import threading class MyThreadHello(threading.Thread): def run(self): for i in range(100): print 'hello' class MyThreadWorld(threading.Thread): def run(self): for i in range(100): print 'world' if __name__ == '__main__': thread_hello = MyThreadHello() thread_world = MyThreadWorld() thread_hello.start() thread_world.start() ","date":"2016-12-12","objectID":"/posts/python-threading/:0:0","tags":["Python","Thread"],"title":"Python 使用 threading 实现多线程","uri":"/posts/python-threading/"},{"categories":null,"content":"0x00 MariaDB的身世 自从MySQL被Oracle收购之后，社区就一直担心MySQL可能会被闭源或者一些其他的原因导致MySQL的支持出现问题。所以现在好多发行版本默认的数据库都从MySQL转移到了Mariadb。而且社区也开始大力支持Mariadb，再加上Mariadb的使用和API和MySQL完全一样，所以这里选择使用Mariadb而不是MySQL。 MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可。开发这个分支的原因之一是：甲骨文公司收购了MySQL后，有将MySQL闭源的潜在风险，因此社区采用分支的方式来避开这个风险。 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。在存储引擎方面，10.0.9版起使用XtraDB（名称代号为Aria）来代替MySQL的InnoDB。 MariaDB由MySQL的创始人麦克尔·维德纽斯主导开发，他早前曾以10亿美元的价格，将自己创建的公司MySQL AB卖给了SUN，此后，随着SUN被甲骨文收购，MySQL的所有权也落入Oracle的手中。MariaDB名称来自麦克尔·维德纽斯的女儿玛丽亚（英语：Maria）的名字。——————————维基百科 0X01 安装Mariadb MariaDB是一组软件，如果只安装一部分的话后期扩展可能会出现问题，所以我们可以一次安装整个软件组 [root@iZ28jaak5nnZ ~]# yum groupinstall mariadb mariadb-client -y 安装需要一点时间，我们只需要等待安装结束。 0X02 打开Mariadb服务并配置防火墙 启动Mariadb服务。在CentOS7.x中推荐使用systemctl来配置服务的启动方式 systemctl start mariadb.service 配置防火墙，允许从MariaDB使用的3306端口监听，由于历史遗留问题，这里还是称之为MySql。 [root@iZ28jaak5nnZ ~]# firewall-cmd --add-service=mysql success [root@iZ28jaak5nnZ ~]# firewall-cmd --list-services dhcpv6-client mysql ssh 0X03 配置MariaDB的安全性 MariaDB提供了一个脚本来为新安装的MariaDB提升安全性。但是在使用这个脚本之前必须要先打开MariaDB服务。 [root@iZ28jaak5nnZ ~]# systemctl start mariadb [root@iZ28jaak5nnZ ~]# systemctl enable mariadb ln -s '/usr/lib/systemd/system/mariadb.service' '/etc/systemd/system/multi-user.target.wants/mariadb.service' 然后运行这个脚本，这个脚本会有几次提示： 询问当前密码，如果没设置密码就直接回车 设置root用户的密码 删除匿名用户(anonymous-user) 删除可以从外部登陆的root用户 删除test测试数据库 重载数据库 [root@iZ28jaak5nnZ ~]# mysql_secure_installation /usr/bin/mysql_secure_installation: line 379: find_mysql_client: command not found NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY! In order to log into MariaDB to secure it, we'll need the current password for the root user. If you've just installed MariaDB, and you haven't set the root password yet, the password will be blank, so you should just press enter here. Enter current password for root (enter for none): OK, successfully used password, moving on... Setting the root password ensures that nobody can log into the MariaDB root user without the proper authorisation. Set root password? [Y/n] y New password: Re-enter new password: Password updated successfully! Reloading privilege tables.. ... Success! By default, a MariaDB installation has an anonymous user, allowing anyone to log into MariaDB without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? [Y/n] y ... Success! Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? [Y/n] y ... Success! By default, MariaDB comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? [Y/n] y - Dropping test database... ... Success! - Removing privileges on test database... ... Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload privilege tables now? [Y/n] y ... Success! 0X04 登陆到MariaDB 配置好密码和接入点之后就可以登录到MariaDB了。使用mysql命令来登陆MariaDB。 [root@iZ28jaak5nnZ ~]# mysql -h localhost -u root -p Enter password: Welcome to the MariaDB monitor. Commands end with ; or \\g. Your MariaDB connection id is 12 Server version: 5.5.50-MariaDB MariaDB Server Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MariaDB [(none)]\u003e 参数：-h 指定主机， -u 指定用户， -p 指定密码 这三个参数都可以省略，当我们省略主机名的时候就默认登录到本地，当省略用户名的时候默认使用root，当省略密码的时候默认没有密码登陆 0X05 用户管理 在MariaDB中有用户的概念和权限的概念。用户名+密码+登陆地点，三个选项唯一确定一个用户，就比如同一个用户名shawn在10.13.1.2和在10.13.1.3的登陆密码可以是不同的，这在MariaDB里会分成两条来存储。 创建一个名为shawn的，从localhost登陆的，密码为test的用户 MariaDB [(none)]\u003e CREATE USER 'shawn'@'local","date":"2016-11-20","objectID":"/posts/linux-mysql-mariadb-simple/:0:0","tags":["MySQL","Database"],"title":"Linux 下 MariaDB/MySql 的安装配置、用户管理和备份","uri":"/posts/linux-mysql-mariadb-simple/"},{"categories":null,"content":"0X00 firewalld 守护进程 firewall-cmd命令需要firewalld进程处于运行状态。我们可以使用systemctl status/start/stop/restart firewalld来控制这个守护进程。firewalld进程为防火墙提供服务。 当我们修改了某些配置之后（尤其是配置文件的修改），firewall并不会立即生效。可以通过两种方式来激活最新配置systemctl restart firewalld和firewall-cmd --reload两种方式，前一种是重启firewalld服务，建议使用后一种“重载配置文件”。重载配置文件之后不会断掉正在连接的tcp会话，而重启服务则会断开tcp会话。 0X01 控制端口/服务 可以通过两种方式控制端口的开放，一种是指定端口号另一种是指定服务名。虽然开放http服务就是开放了80端口，但是还是不能通过端口号来关闭，也就是说通过指定服务名开放的就要通过指定服务名关闭；通过指定端口号开放的就要通过指定端口号关闭。还有一个要注意的就是指定端口的时候一定要指定是什么协议，tcp还是udp。知道这个之后以后就不用每次先关防火墙了，可以让防火墙真正的生效。 firewall-cmd --add-service=mysql # 开放mysql端口 firewall-cmd --remove-service=http # 阻止http端口 firewall-cmd --list-services # 查看开放的服务 firewall-cmd --add-port=3306/tcp # 开放通过tcp访问3306 firewall-cmd --remove-port=80tcp # 阻止通过tcp访问3306 firewall-cmd --add-port=233/udp # 开放通过udp访问233 firewall-cmd --list-ports # 查看开放的端口 0X02 伪装IP 防火墙可以实现伪装IP的功能，下面的端口转发就会用到这个功能。 firewall-cmd --query-masquerade # 检查是否允许伪装IP firewall-cmd --add-masquerade # 允许防火墙伪装IP firewall-cmd --remove-masquerade# 禁止防火墙伪装IP 0X03 端口转发 端口转发可以将指定地址访问指定的端口时，将流量转发至指定地址的指定端口。转发的目的如果不指定ip的话就默认为本机，如果指定了ip却没指定端口，则默认使用来源端口。 如果配置好端口转发之后不能用，可以检查下面两个问题： 比如我将80端口转发至8080端口，首先检查本地的80端口和目标的8080端口是否开放监听了 其次检查是否允许伪装IP，没允许的话要开启伪装IP # 将80端口的流量转发至8080 firewall-cmd --add-forward-port=port=80:proto=tcp:toport=8080 # 将80端口的流量转发至 firewall-cmd --add-forward-port=port=80:proto=tcp:toaddr=192.168.1.0.1192.168.0.1 # 将80端口的流量转发至192.168.0.1的8080端口 firewall-cmd --add-forward-port=port=80:proto=tcp:toaddr=192.168.0.1:toport=8080 当我们想把某个端口隐藏起来的时候，就可以在防火墙上阻止那个端口访问，然后再开一个不规则的端口，之后配置防火墙的端口转发，将流量转发过去。 端口转发还可以做流量分发，一个防火墙拖着好多台运行着不同服务的机器，然后用防火墙将不同端口的流量转发至不同机器。 ","date":"2016-11-17","objectID":"/posts/firewall-cmd-port-forward/:0:0","tags":["Firewall","Linux"],"title":"CentOS7 中使用 firewall-cmd 控制端口和端口转发","uri":"/posts/firewall-cmd-port-forward/"},{"categories":null,"content":"0X00 文件 ** ‘文件’是进程创建的逻辑单元。 ** —《现代操作系统（原书第三版）》 文件我们再熟悉不过了，电脑磁盘上存的都是文件。在Windows里和Unix系列系统里，表面看上去文件之间还是有一点点小的区别。比如，在Windows里主要还是以文件的后缀名来标识文件具体是什么类型的，图片还是视频；在Unix系列里文件的后缀名就没那么重要，主要后缀名是用来帮助人们识别文件类型的，操作系统并不很关心。 ** 真正的文件类型 ** 是文件的本质类型，不是我们常说的exe类型、doc类型、更不是什么图片类型和视频类型。在Windows下有常见的普通文件和目录。 没错，目录其实是文件的 。在Unix里，还有一些叫字符特殊文件和块特殊文件的。 0X01 文件的元数据 文件里最重要的东西肯定是文件内容了，但是文件存在磁盘里是还有一些其他的相关数据也被存进去了的，那些数据被称之为元数据 。想一下文件的相关信息，在Windows里右键一个文件选择属性或者在Linux里使用ls -l看到的文件的详细信息，这些几乎全部都是文件的元数据，都存到了磁盘中。常见的元数据有下面这些:创建者、拥有者、权限标志、文件大小、锁等等。比如说我们在Linux下输入ls -l能看到文件的大小、权限、所属者，这些都是文件的元数据。 0X02 MBR-主引导记录 经常装系统的话应该比较熟悉这个词‘主引导记录’，在以前的磁盘上常用的就是这种称为MBR的磁盘分区方式，其实现在还有好多在用MBR的，不过由于MBR的原理导致不支持2TB以上的磁盘且不支持4个以上的主分区，所以用的越来越少了，取而代之的是GPT。不过由于MBR比较简单，就先介绍一下MBR。 计算机在启动的时候，BIOS会读取MBR的分区表来找到引导分区并引导操作系统。可以启动的分区称为活动分区，必须要是活动的分区才可以引导系统启动；MBR的分区表只能容纳四个分区，如果需要更多的分区就需要创建扩展分区。可以在一个MBR的分区表中创建三个主分区，在最后一个位置创建一个扩展分区。实际上最后一个扩展分区是不能直接使用的，相当于扩展分区在磁盘上花了一块当另一个磁盘用、在扩展分区头部还有一个扩展分区的分区表，里面保存着逻辑分区的分区信息，且这个分区表的空间比较大，所以逻辑分区可以创建好多个。 0X03 文件存储 文件存储在磁盘中有好多种分配方案，这些方案各有利弊。随着存储介质、CPU等设备的发展和人们需求的变化，出现了下面这些比较好的方案。 ","date":"2016-11-16","objectID":"/posts/simple-fs/:0:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"连续分配 首先我们把磁盘想象成一个超长的条形存储设备，这样就比较好理解（然而实际上现在常见的磁盘是区分盘面、磁道、柱面、扇区的）。 早期的磁盘和现代的CD-ROM是使用这种连续分配方式存储数据的。连续分配，由字面可知是把文件连续的从头到尾得存到磁盘里，这种方式读写都非常快，但是却非常不适合日常使用。考虑下面这种情况，我有下面这些文件 [ A ][ B ][ C ][ D ][ E ][ F ]整个磁盘大小为6GB，每个文件都有1GB，刚刚好用完整个磁盘。但是当我删除了B和E两个文件的时候就会变成下面这样，空余两个1GB的位置出来，但是这两个空间不连续 [ A ][ ][ C ][ D ][ ][ F ]现在我有2GB的空间。系统需要维护一个空闲空间列表来让后来的文件放在这些空闲的地方，因为如果不维护这张表的话，当磁盘写满过一次就再也不能写入新的数据了。虽然我们维护了一张这样的表也并不能很让人满意，比如有一个1.5GB的文件想存到磁盘里，系统就会查找连续的空余空间，但是并没有一个连续的高达1.5GB的空间，所以并不能把文件存进去，显然这并不能让人满意。而且，这些还都是建立在一个前提之下的，就是说“存储文件之前必须知道文件的大小”，然而事实上很多时候是不知道的，比如我们打开了AE（一款渲染视频的软件）来制作一段视频特效，然而在生成视频的时候没有人知道这个文件最后是多大的，所以就并不适用于这种情况。但是这种分配方案就没有优点了吗？也不是的。比如我们需要将数据刻录到CD-ROM上，因为CD-ROM是只读设备，所以在第一次刻录之后就没有修改的可能了，那么我们就可以通过这种方案直接将已有的数据顺序刻录到光盘里，这样以后的读取就会变得很快了。然后针对磁盘有了下面的‘链表分配方案’ ","date":"2016-11-16","objectID":"/posts/simple-fs/:1:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"链表分配 使用链表分配方案时，目录下的每一个文件都只保留文件的头指针，每个文件都是一个链表，这样我们就可以顺着指针的指向把整个文件从文件系统中遍历出来。虽然链表分配方案成功的利用起来了空闲空间，但是还是有下面两个比较严重的问题： 每次想要访问文件的第n个节点时候，都要从文件头开始访问，有n-1次磁盘的访问是无效的，所以这种方案对随机读取非常慢； 因为每个磁盘块的大小都是2的n次幂，保存的大小也就是2的n次幂，但是因为文件头被指针占去了一定的字节，就导致实际存储的文件并不是2的n次幂。虽然这个问题并不是致命的，但是确实会让系统变慢，也会让面向系统的编程变得困难很多。 内存链表解决了链表分配的一些问题。 ","date":"2016-11-16","objectID":"/posts/simple-fs/:2:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"内存链表分配 内存链表分配是将磁盘里所有文件的所有块都做成链表，依旧是每个文件一个链表。但是这次将链表整个存放到内存中，这样在随机访问的时候因为链表全都在内存中就会非常快。但是由于要对每一个文件建立存储，且存放在内存中，所以这种文件系统并不适合用于小文件大磁盘。对于一个200GB的磁盘，里面充满了1KB的块，那么根据系统优化之后这张表需要600~800MB的内存，然而现在动辄TB级的磁盘，则非常不适用。这种内存链表分配方案中维护的表称之为‘文件分配表’英文也就是我们熟悉的‘File Allocation Table—–FAT’ 为了克服内存链表分配的内存占用大的问题，有了i-Node方案。 ","date":"2016-11-16","objectID":"/posts/simple-fs/:3:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"i-Node i-Node方案在磁盘头部预留一段空间用来存放i-Node，这里的i-Node是一种数据结构，里面包含了文件的一些元数据和文件所有块的相关信息，所以根据一个i-Node就可以找到着整个文件。因为每个i-Node的预留空间都是固定的，如果文件太大太分散就会导致一个i-Node并不能存储完所有信息，那么i-Node中最后一段就保存了另一个i-Node的地址，然后在另一个i-Node中继续保存信息。因为i-Node是保存在磁盘里的，所以不会影响到内存，只有当文件真正打开的时候才会将数据加载到内存。所以内存占用是核同时打开的文件数量相关的。在Linux中我们使用ls -l -i就可以看到每个文件的i-Node编号。现在的大多数文件系统都采用这种方案了，比如EXT、NTFS、XFS等。 0X04 文件共享 首先明确两点：1. 这里说的文件共享并不是说将一个文件通过网络传输给他人的那种文件共享； 2. 系统中的文件结构不是树状，而是图。（当Windows中我们给一个文件建立了一个快捷方式并放在了另一个目录里的时候，就形成了图解构） 这里的文件共享主要就是链接的问题，关于链接的内容可以在我博客里找到。Linux软连接/硬链接 理解Linux链接 每个文件会保存指向自己的链接数，当只想自己的链接数为0的时候，那么这块数据就抛弃掉了。 0X05 文件系统 ","date":"2016-11-16","objectID":"/posts/simple-fs/:4:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"日志结构文件系统 因为现在的CPU运算能力和磁盘容量、内存容量等都有了非常大的进步，所以在不实际访问磁盘只在高速缓存上就能访问到很多需要的数据，所以根据这种情况，就出现了日志结构文件系统(Log-structred File System)。这种文件系统将文件操作结构化成日志。在这种文件系统中每次将数据读写缓存到内存，然后定时定量地将数据从内存写入磁盘。 ","date":"2016-11-16","objectID":"/posts/simple-fs/:5:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"日志文件系统 日志文件系统比日志结构文件系统有更强的鲁棒性（Roubst 也就是健壮性）。在这种文件系统中进行文件操作时，先记录下要干什么，然后再开始操作。这样不管什么时候出了错误，都可以根据日志来恢复操作。比如在Unix中删除一个文件分成三个步骤：1.在目录中删除文件 2.释放i-Node到空间i-Node节点池 3.强磁盘块归还到操作系统。 如果完成了第一步，就死机了，那么由于日志的存在就可以在知道这个操作究竟要干什么，在恢复开机的时候就可以继续完成这次操作。当所有的任务项都完成了的时候就删除这个日志。 ","date":"2016-11-16","objectID":"/posts/simple-fs/:6:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"虚拟文件系统 在Windows里用的是多根目录的方式，也就是有多个根目录，比如C盘D盘E盘等，但是在Linux中我们使用的是单根目录形式，如果要同时使用几种文件系统比如：根目录使用XFS、/home使用ext4、/usr目录使用ext3、那么久需要使用一种叫做虚拟文件系统的技术。两个不同的文件系统之间需要连接的话需要使用VFS（虚拟文件系统）接口来将两个文件系统连起来。使用这种虚拟文件系统的技术就可以让同一个根目录下面挂载有不同文件系统的设备。 0X06 磁盘分块 已知文件在磁盘里是按照块存储的，那么每个块分配多大就成为了一个问题。因为在磁盘底层，每个文件占用的块都是整数，比如我一个块是1kb，那么我有一个2.5kb的文件也要占用3个块，甚至是1字节的文件也要占用1kb，每个块中剩余的部分是不能存储其他文件的。从这方面看来分块越大就越浪费空间，块越小磁盘空间利用率越高。那么我们把块都分成最小，这样就行了吗？显然没有这么简单。因为磁盘在读写数据的时候是按照块来的，所以分的块越大读写的速度越快，因为磁盘里的总块数少，块越小越慢。总结下来是这样：随着块大小的提升，磁盘读写效率会提高，但是空间利用率会降低。统计所得，分块大小为4kb最容易获得最佳性能。 0X07 缓冲区 现在的文件系统都支持缓冲区写入。缓冲区写入对应的另一种是‘同步写入’。缓冲区写入：程序生成的或者用户的数据首先写入到内存中，当达到一定时间或者一定量的时候一次性写入磁盘；同步写入：将程序和用户产生的数据实时写入磁盘里。下面对比一下这两种的优缺点 同步写入优点 ：数据实时同步，出现数据丢失的可能性很小 同步写入缺点 ：由于数据产生很慢，所以磁盘利用率不高，且长期占据磁盘 缓冲写入优点 ：数据首先写入比磁盘快得多的内存中，再统一写入磁盘只会短时间占用磁盘，且占用磁盘时利用率高 缓冲写入缺点 ：当数据在缓冲区没有写入磁盘时系统发生异常或者崩溃，数据非常容易丢失 这里有两段Python的代码，展示了缓冲区写入和同步写入的速度差异，首先是使用同步写入方式写入 #!/usr/bin/python #coding=utf-8 from time import ctime f = open('D:/list.txt', 'r+', 0) #不使用缓冲 start_time = ctime() #开始时间 for i in range(3000000): f.write('hello,world\\n') f.close() end_time = ctime() #结束时间 print start_time print end_time 运行结果是这样的，我们看到写入300W行’hello,world’用了8秒，最后生成的数据量是37.1MB Thu Nov 17 11:12:21 2016 Thu Nov 17 11:12:29 2016 然后使用Python默认大小的缓冲区试试： #!/usr/bin/python #coding=utf-8 from time import ctime f = open('D:/list.txt', 'r+', -1) #使用默认缓冲区大小 start_time = ctime() #开始时间 for i in range(3000000): f.write('hello,world\\n') f.close() end_time = ctime() #结束时间 print start_time print end_time 运行结果是这样的，我们看到这次写入相同的数据只用了1秒 Thu Nov 17 11:18:04 2016 Thu Nov 17 11:18:05 2016 0X08 坏块屏蔽 在磁盘这种物理结构里出现错误是比较正常的，尤其是机械磁盘磁臂在旋转的时候与磁道的摩擦会产生部分坏块。这些坏块上的数据会发生丢失或者错误，那么怎么屏蔽这些坏块呢？之前在那些分区软件里看到过坏块屏蔽，感觉非常高端，其实原理是很简单的。比如磁盘里有23块坏块，那么修复程序就创建一个文件，指定这个文件就存储在这23个坏块上，且对操作系统不可见，那么操作系统虽然知道这里有文件，但是不会去管他，这样就相当于屏蔽了磁盘里的坏块。 0X09 提升文件系统性能 高速缓存 高速缓存就是将即将需要的文件和经常使用的文件放在磁盘的高速缓存里，因为高速缓存的速度比磁盘要快得多，所以就可以通过这种方式来提高I/O效率。 在Unix里有一个系统调用sync，在Windows里有一个FlushFileBuffers，是用来将高速缓存里的数据同步写入到磁盘里的。在Unix系列系统中每隔30秒就执行一次sync将数据写入，在Windows中则是实时的。这两种方案并没有谁好谁坏之分，各有优劣。 0X0A 磁盘碎片整理 因为绝大多数的现代文件系统都采用了链表存储的方案，所以在使用磁盘一段时间之后文件都是分散的放在磁盘的各个角落的，这样的话读写文件就会变得比较慢，文件越零散读写就越慢。那么我们可以手动将磁盘进行整理，将分散的文件数据聚合到一起，当然也只能是尽量，因为某些数据是不能被移动的，比如页文件、休眠文件、日志。当零散的文件变成连续的文件的时候读写的效率就会有大幅度提升。但是由于各个操作系统采用的文件系统的内部实现不同，导致几乎只有Windows需要对磁盘进行手动整理。当然所谓的手动整理也是有软件支持的，不需要用户自己去操作磁盘。但是因为Windows的发展，也几乎不再需要手动进行整理了。 ","date":"2016-11-16","objectID":"/posts/simple-fs/:7:0","tags":["OS","FS"],"title":"文件系统简述","uri":"/posts/simple-fs/"},{"categories":null,"content":"0X00 Linux中的计划任务 我们使用Linux更多的时候是在服务器上，然而我们有的时候就需要让计算机在固定的某个时间做一些事情。比如我们就可能有有如下需求： 临时有事需要离开电脑，但是一个小时后需要备份某个目录里的文件 写了个爬虫去抓取某网站的新闻，每隔十分钟就去爬取一次 周期性的执行某脚本，但放在后台的话退出ssh就会被自动关掉 其实还有好多这种可能………… 在Linux中有两种常见的任务管理，一个是at也就是在某时做某事，另一个是crontab也就是周期性任务表。使用at可以方便地给Linux设置一个在什么时候做什么事的计划，用crontab可以方便地给Linux设置我要做某事，多久做一次。 0X01 使用at命令 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:0:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"检查atd服务是否开启 atd就是at命令的守护进程，系统默认是打开着的，但是也有可能被关掉，在RHEL系中可以使用systemctl status atd来查看服务是否已经开启，没有开启的话可以用systemctl restart atd来打开服务 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:1:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"创建一个计划任务 先创建一个在今天的21:09的任务，任务内容是输出hello,world重定向到/hello文件。然后到时间之后再检查这个文件是否出现了。当我们只指定时分的时候，默认是当天，如果已经过了的时间的话，会默认为次日。 # 一个即日的计划任务 [root@iZ28jaak5nnZ ~]# date Wed Nov 2 21:07:07 CST 2016 [root@iZ28jaak5nnZ ~]# at 21:09 at\u003e echo \"hello,world\" \u003e /hello at\u003e \u003cEOT\u003e job 5 at Wed Nov 2 21:09:00 2016 [root@iZ28jaak5nnZ ~]# date Wed Nov 2 21:09:10 CST 2016 [root@iZ28jaak5nnZ ~]# cat /hello hello,world 当我们输入at 21:09之后，就进入了at模式，我们在这里输入的命令就是之后将要执行的命令。当输入完命令之后按Ctrl + D就可以退出at模式，此时计划任务创建完毕，系统会提示你计划任务的执行时间。 下面还有几个例子 # 一个准确定时的计划任务 [root@iZ28jaak5nnZ ~]# at 00:00 2016-11-11 # 在2016光棍节零点输出一个'hey 单身狗' at\u003e echo \"hey single dog\" at\u003e \u003cEOT\u003e job 7 at Fri Nov 11 00:00:00 2016 # 在十分钟后执行 [root@iZ28jaak5nnZ ~]# at now+10min at\u003e echo 'hello single dog' at\u003e \u003cEOT\u003e job 9 at Wed Nov 2 21:26:00 2016 # 在一小时后执行 [root@iZ28jaak5nnZ ~]# at now+1hour at\u003e echo 'hey single dog' at\u003e \u003cEOT\u003e job 10 at Wed Nov 2 22:16:00 2016 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:2:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"查看已有的at 可以使用atq命令来查看存在的at计划任务，注意这里并不一定全都是用户自己创建的，也有的是系统创建的。通过atq查看到之后可以使用at -c 来查看某个计划任务的具体信息。 [root@iZ28jaak5nnZ ~]# at now+1hour at\u003e echo \"hello\" at\u003e \u003cEOT\u003e job 11 at Wed Nov 2 22:23:00 2016 [root@iZ28jaak5nnZ ~]# atq # 这里输出的第一列就是at的编号，下面查看详细信息就是根据编号查看的 7 Fri Nov 11 00:00:00 2016 a root 6 Thu Nov 3 03:00:00 2016 a root 10 Wed Nov 2 22:16:00 2016 a root 9 Wed Nov 2 21:26:00 2016 a root 11 Wed Nov 2 22:23:00 2016 a root 1 Wed Nov 2 21:52:00 2016 a root [root@iZ28jaak5nnZ ~]# at -c 11 #!/bin/sh # atrun uid=0 gid=0 # mail root 0 umask 22 XDG_SESSION_ID=669; export XDG_SESSION_ID ............................... # 这里省略了好多环境变量，重点在下面 XDG_RUNTIME_DIR=/run/user/0; export XDG_RUNTIME_DIR cd /root || { echo 'Execution directory inaccessible' \u003e\u00262 exit 1 } ${SHELL:-/bin/sh} \u003c\u003c 'marcinDELIMITER0e9efce8' # 这里是执行的命令 echo \"hello\" marcinDELIMITER0e9efce8 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:3:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"删除一个at 使用一个atrm命令可以指定at号删除特定的at计划任务。 [root@iZ28jaak5nnZ ~]# at now+1hour at\u003e echo 'hello' at\u003e \u003cEOT\u003e job 12 at Wed Nov 2 22:27:00 2016 [root@iZ28jaak5nnZ ~]# atq 7 Fri Nov 11 00:00:00 2016 a root 6 Thu Nov 3 03:00:00 2016 a root 10 Wed Nov 2 22:16:00 2016 a root 11 Wed Nov 2 22:23:00 2016 a root 12 Wed Nov 2 22:27:00 2016 a root 1 Wed Nov 2 21:52:00 2016 a root [root@iZ28jaak5nnZ ~]# atm 12 -bash: atm: command not found [root@iZ28jaak5nnZ ~]# atrm 12 [root@iZ28jaak5nnZ ~]# atq 7 Fri Nov 11 00:00:00 2016 a root 6 Thu Nov 3 03:00:00 2016 a root 10 Wed Nov 2 22:16:00 2016 a root 11 Wed Nov 2 22:23:00 2016 a root 1 Wed Nov 2 21:52:00 2016 a root [root@iZ28jaak5nnZ ~]# at now+1hour at\u003e echo 'hello' at\u003e \u003cEOT\u003e job 12 at Wed Nov 2 22:27:00 2016 [root@iZ28jaak5nnZ ~]# atq 7 Fri Nov 11 00:00:00 2016 a root 6 Thu Nov 3 03:00:00 2016 a root 10 Wed Nov 2 22:16:00 2016 a root 11 Wed Nov 2 22:23:00 2016 a root 12 Wed Nov 2 22:27:00 2016 a root 1 Wed Nov 2 21:52:00 2016 a root [root@iZ28jaak5nnZ ~]# atrm 12 [root@iZ28jaak5nnZ ~]# atq 7 Fri Nov 11 00:00:00 2016 a root 6 Thu Nov 3 03:00:00 2016 a root 10 Wed Nov 2 22:16:00 2016 a root 11 Wed Nov 2 22:23:00 2016 a root 1 Wed Nov 2 21:52:00 2016 a root 0X02 使用crontab命令 这里的配置分成六段 分—时—日—月—周—命令 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:4:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"创建周期任务 使用任何一个用户登陆到系统之后，就可以执行crontab -e就进入了vi的编辑器模式，然后我们来编辑这个文件就可以创建/修改周期任务了。 15 10 1 10 * echo 'hello' \u003e /tmp/hello # 在每个10月1号10点15分执行命令 15 10 1 * * echo 'hello' \u003e /tmp/hello # 在每个1号10点15分执行命令 15 10 * * * echo 'hello' \u003e /tmp/hello # 在每个10点15分执行命令 15 * * * * echo 'hello' \u003e /tmp/hello # 在每个15分执行命令 */3 * * * * echo 'hello' \u003e /tmp/hello # 每3分钟执行命令 退出保存之后就可以按照这个时间来执行命令了。 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:5:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"查看周期任务 使用crontab -l查看该用户的周期任务 [root@iZ28jaak5nnZ ~]# crontab -l 15 10 1 10 * echo 'hello' \u003e /tmp/hello 15 10 1 * * echo 'hello' \u003e /tmp/hello 15 10 * * * echo 'hello' \u003e /tmp/hello 15 * * * * echo 'hello' \u003e /tmp/hello */3 * * * * echo 'hello' \u003e /tmp/hello ","date":"2016-11-02","objectID":"/posts/linux-crontab/:6:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"删除周期任务 可以使用crontab -r删除当前用户所有的周期任务。 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:7:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"管理周期任务 每个用户都可以使用crontab -e来管理自己的周期任务，然而root用户可以使用crontab -u来管理其他用户的周期任务。只要加一个-u选项即可，参数后面接上要管理的用户就可以了。然后还是和上面的操作一样，只是多了一个这个参数而已。 ","date":"2016-11-02","objectID":"/posts/linux-crontab/:8:0","tags":["Linux","Crontab"],"title":"Linux 中计划任务和周期任务","uri":"/posts/linux-crontab/"},{"categories":null,"content":"0X00 ACL是什么 ACL的全称是Access Control List访问控制列表。在Linux中可以给文件设置权限，-rwx-rw-rw这样，但是这里并不能细分，只能分到用户、组、其他用户。如果我想给某个单独的用户设置权限的话是做不到的。所以有了ACL的出现。通过ACL可以给Linux下的文件提供详细的访问控制，比如我们在设置了基本的rwx权限之后，可以通过ACL在细分用户对文件的权限。 0X01 查看文件的ACL 使用getfacl命令可以查看文件的ACL和详细的权限设置。 [root@iZ28jaak5nnZ ~]# ls -l total 4 -rwxr-xr-x 1 root root 1714 Oct 28 22:24 hello.py [root@iZ28jaak5nnZ ~]# getfacl hello.py # file: hello.py # owner: root # group: root user::rwx group::r-x other::r-x 这里显示了文件名、所属用户、所属组、还有相对应的权限。 0X02 创建测试用户/组/文件 先创建测试用户、测试组、测试文件。创建了xiaoming和xiaohong两个用户，在china组，创建了一个jack用户在usa组。然后用root用户在/tmp/目录下创建了一个acltest目录，用来做测试，因为这个目录是任何人都可以访问的，但是由于是root用户创建的子目录，所以要给这个目录777的权限，让其他用户可以在里面测试。现在里面又创建了一些目录和文件，但是全部都是root用户的，文件权限是644，目录权限是755。 [root@iZ28jaak5nnZ ~]# groupadd china [root@iZ28jaak5nnZ ~]# useradd xiaoming -g china [root@iZ28jaak5nnZ ~]# useradd xiaohong -g china [root@iZ28jaak5nnZ ~]# groupadd usa [root@iZ28jaak5nnZ ~]# useradd jack -g usa [root@iZ28jaak5nnZ ~]# cd /tmp [root@iZ28jaak5nnZ tmp]# mkdir acltest [root@iZ28jaak5nnZ tmp]# chmod 777 acltest [root@iZ28jaak5nnZ tmp]# cd acltest/ [root@iZ28jaak5nnZ acltest]# touch file_{1,3} [root@iZ28jaak5nnZ acltest]# mkdir dir_{1,3} 0X03 设置文件的ACL 使用setfacl命令可以设置文件ACL。这个命令有下面这几个常用参数 ","date":"2016-11-02","objectID":"/posts/linux-acl/:0:0","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"setfacl 各个参数 所谓的后续ACL就是在默认ACL的基础上添加的新的规则。 ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:0","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"-m 设置后续ACL 对某一个文件/目录设置某一个用户的访问权限， u表示用户 冒号后面是用户名 再一个冒号后面是权限 最后接文件/目录 [root@iZ28jaak5nnZ acltest]# setfacl -m u:user_1:rwx file_1 对某一个文件/目录设置某一个用户组的访问权限，u表示组 冒号后面是组名 再一个冒号后面是权限 最后接文件/目录 [root@iZ28jaak5nnZ acltest]# setfacl -m g:group_1:rwx file_1 ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:1","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"-x 删除后续ACL 删除之前添加的ACL项，指定用户或者指定组都是可以的，语法和上面差不多。这里删除的是一条ACL数据，下面说的-b参数是删除所有的ACL数据 [root@iZ28jaak5nnZ acltest]# setfacl -x u:xiaoming file_1 [root@iZ28jaak5nnZ acltest]# getfacl file_1 # file: file_1 # owner: root # group: root user::rw- group::r-- mask::r-- other::r-- ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:2","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"-b 删除所有后续ACL 这里是删除之前创建的所有ACL，包括下面会说的默认ACL [root@iZ28jaak5nnZ acltest]# getfacl file_1 # file: file_1 # owner: root # group: root user::rw- user:xiaoming:rwx user:xiaohong:rw- group::r-- mask::rwx other::r-- [root@iZ28jaak5nnZ acltest]# setfacl -b file_1 [root@iZ28jaak5nnZ acltest]# getfacl file_1 # file: file_1 # owner: root # group: root user::rw- group::r-- other::r-- ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:3","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"-d 设置默认ACL 设置默认ACL只能为目录设置，为目录设置了ACL之后里面新建的目录和文件都是使用这个默认的ACL [root@iZ28jaak5nnZ acltest]# getfacl dir_1 # file: dir_1 # owner: root # group: root user::rwx group::r-x other::r-x [root@iZ28jaak5nnZ acltest]# setfacl -m d:u:jack:rwx dir_1 # 设置目录的默认ACL [root@iZ28jaak5nnZ acltest]# getfacl dir_1 # 我们可以看到现在出现了一段默认ACL # file: dir_1 # owner: root # group: root user::rwx group::r-x other::r-x default:user::rwx default:user:jack:rwx default:group::r-x default:mask::rwx default:other::r-x [root@iZ28jaak5nnZ acltest]# cd dir_1 [root@iZ28jaak5nnZ dir_1]# touch hello [root@iZ28jaak5nnZ dir_1]# getfacl hello # 新建的文件也使用这些默认设置 # file: hello # owner: root # group: root user::rw- user:jack:rwx #effective:rw- group::r-x #effective:r-- mask::rw- other::r-- [root@iZ28jaak5nnZ acltest]# setfacl -m u::rwx -d dir_3 # 设置为每个用户，也可以修改为用户组 [root@iZ28jaak5nnZ acltest]# getfacl dir_3 # file: dir_3 # owner: root # group: root user::rwx group::r-x other::r-x default:user::rwx default:group::r-x default:other::r-x ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:4","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"-k 删除默认ACL 这里可以删除之前设置的默认ACL，只限默认ACL [root@iZ28jaak5nnZ acltest]# getfacl dir_3 # 查看ACL，这里显示有默认的ACL # file: dir_3 # owner: root # group: root user::rwx group::r-x other::r-x default:user::rwx default:group::r-x default:other::r-x [root@iZ28jaak5nnZ acltest]# setfacl -k dir_3 # 删除dir_3的默认ACL [root@iZ28jaak5nnZ acltest]# getfacl dir_3 # file: dir_3 # owner: root # group: root user::rwx group::r-x other::r-x ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:5","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"-R 递归设置ACL 给某一个目录设置递归的ACL之后这个目录和这个目录里的文件和子目录全部都会应用这个ACL，也就是说是相当于应用到了这个目录下的所有文件和目录 # 首先创建一下测试用的目录结构 [root@iZ28jaak5nnZ acltest]# mkdir -p dir1/dir2/dir3 [root@iZ28jaak5nnZ acltest]# touch dir1/hello.c [root@iZ28jaak5nnZ acltest]# touch dir1/dir2/hey.c [root@iZ28jaak5nnZ acltest]# setfacl -m u:jack:r -R dir1 递归设置ACL [root@iZ28jaak5nnZ acltest]# getfacl dir1 # file: dir1 # owner: root # group: root user::rwx user:jack:r-- group::r-x mask::r-x other::r-x [root@iZ28jaak5nnZ acltest]# getfacl dir1/hello.c # file: dir1/hello.c # owner: root # group: root user::rw- user:jack:r-- group::r-- mask::r-- other::r-- ","date":"2016-11-02","objectID":"/posts/linux-acl/:1:6","tags":["Linux","ACL","Permission"],"title":"Linux 权限控制列表 ACL","uri":"/posts/linux-acl/"},{"categories":null,"content":"0X00 查看网络配置文件 在CentOS中网络是以配置文件的形式存在系统里的，在/etc/sysconfig/network-scripts/目录下，一般情况下网卡的配置文件都在这里了，以ifcfg-就是配置文件了，打开配置文件看一下。下面注释一下关键的配置项 TYPE=Ethernet # 网络类型 BOOTPROTO=static # 协议取值，常见的是static和dhcp IPADDR=10.13.7.33 # 给网卡ip赋值 NETMASK=255.255.255.0 # 给网卡子网掩码赋值 DEFROUTE=yes PEERDNS=yes PEERROUTES=yes IPV4_FAILURE_FATAL=no IPV6INIT=yes IPV6_AUTOCONF=yes IPV6_DEFROUTE=yes IPV6_PEERDNS=yes IPV6_PEERROUTES=yes IPV6_FAILURE_FATAL=no NAME=enp0s8 USERCTL=no # 是否允许非root用户控制 UUID=4c967913-c4c9-4961-ae03-de7865f144d0 # 网卡的唯一标识码 DEVICE=enp0s8 # 设备名 ONBOOT=no # 是否在开机时激活 但是一般不建议直接使用编辑器修改网络配置文件，因为这样容易出现一些语法错误和逻辑错误，所以建议使用命令行来管理配置网络，虽然本质上都是去修改配置文件。但是使用命令行去管理网络，命令都是确保配置没有问题才会写入到文件，所以会更加安全。包括下面介绍的ip和nmcli命令，都是通过修改配置文件来完成功能的。 0X01 ifconfig 命令 这个命令在CentOS7中已经不建议使用了，不过由于之前的版本都是在用这个命令，还是说一下。ifconfig是interface configuration的缩写，也就是接口配置。 ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:0:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"查看网络 直接输入这个命令就可以看到现在启动着的所有网络。也可以接上某个特定的网卡来查看单独的信息ifconfig enp0s3 [root@localhost ~]# ifconfig enp0s8: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 ether 08:00:27:79:c4:b1 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u003chost\u003e loop txqueuelen 0 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 如果想查看包括已经关闭了的网络时，使用ifconfig -a就可以了 最后的那个lo是回环网络，暂时不用管 ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:1:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"开关网络 ifconfig还可以开关网络，命令后面接interface name也就是网卡名，然后接上up/down就可以开关网络了 [root@localhost ~]# ifconfig enp0s8 down # 关闭网络 [root@localhost ~]# ifconfig lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u003chost\u003e loop txqueuelen 0 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@localhost ~]# ifconfig enp0s8 up # 打开网络 [root@localhost ~]# ifconfig enp0s8: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 123.123.123.2 netmask 255.255.255.128 broadcast 123.123.123.1 ether 08:00:27:79:c4:b1 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u003cUP,LOOPBACK,RUNNING\u003e mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u003chost\u003e loop txqueuelen 0 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:2:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"配置网络 ifconfig命令可以在不重启的情况下开关网络接口，修改IP、掩码、网关等信息。 [root@localhost ~]# ifconfig enp0s8 # 查看ep0s8的网卡信息 enp0s8: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 ether 08:00:27:79:c4:b1 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@localhost ~]# ifconfig enp0s8 123.233.233.123 # 修改ip为123.233.233.123 [root@localhost ~]# ifconfig enp0s8 enp0s8: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 123.233.233.123 netmask 255.0.0.0 broadcast 123.255.255.255 ether 08:00:27:79:c4:b1 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@localhost ~]# ifconfig enp0s8 netmask 255.255.255.0 # 修改子网掩码为255.255.255.0 [root@localhost ~]# ifconfig enp0s8 enp0s8: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 123.233.233.123 netmask 255.255.255.0 broadcast 123.233.233.255 ether 08:00:27:79:c4:b1 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@localhost ~]# ifconfig enp0s8 123.123.123.2 netmask 255.255.255.128 # 当然也可以把这些写成一行 [root@localhost ~]# ifconfig enp0s8 enp0s8: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 123.123.123.2 netmask 255.255.255.128 broadcast 123.123.123. ether 08:00:27:79:c4:b1 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 0X02 nmtui 简单的类图形管理工具 在终端中输入nmtui就可以打开一个类图形的界面，用这个界面可以更简单得管理配置网络，但是不能做比较细致的配置，而且使用比较简单，所以这里就不多做介绍了，可以自己在终端上打开看看。这个命令的一大优点是可以在ssh远程连接的时候使用，在Windows下的XShell等软件中都可以直接调出。 0X03 ip 命令 ip是现在推荐使用的命令，功能比较强大。 ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:3:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"ip命令管理设备开关 [root@localhost ~]# ip link show enp0s8 # 这个命令大致相当于 ifconfig enp0s8 查看这个网卡的信息 3: enp0s8: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 08:00:27:79:c4:b1 brd ff:ff:ff:ff:ff:ff [root@localhost ~]# ip link set dev enp0s8 up # 设置一个device，enp0s8，打开 [root@localhost ~]# ip link show enp0s8 3: enp0s8: \u003cNO-CARRIER,BROADCAST,MULTICAST,UP\u003e mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 08:00:27:79:c4:b1 brd ff:ff:ff:ff:ff:ff [root@localhost ~]# ip link set dev enp0s8 down # 设置 设备 网卡名 打开/关闭 [root@localhost ~]# ip link show enp0s8 3: enp0s8: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 08:00:27:79:c4:b1 brd ff:ff:ff:ff:ff:ff ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:4:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"ip命令修改网卡MAC地址 [root@localhost ~]# ip link set dev enp0s8 address 00:00:ff:bb:aa:22 # 修改网卡的物理地址，也就是MAC地址 [root@localhost ~]# ip link show enp0s8 3: enp0s8: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 00:00:ff:bb:aa:22 brd ff:ff:ff:ff:ff:ff 0X04 nmcli 管理网络 nmcli是network manager command line interface的简写，这个命令可以用来管理配置网络。 ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:5:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"查看网络接口状态 [root@localhost ~]# nmcli -p g ============================================================= NetworkManager status ============================================================= STATE CONNECTIVITY WIFI-HW WIFI WWAN-HW WWAN ------------------------------------------------------------- connected full enabled enabled enabled enabled ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:6:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"查看修改主机名 [root@localhost ~]# nmcli general hostname localhost.localdomain [root@localhost ~]# nmcli general hostname test [root@localhost ~]# nmcli general hostname test ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:7:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"查看网络设备 以前可以用ifconfig来查看网络设备，ip命令也可以查看。可以直接查看所有的，也可以指定某一个设备查看。 [root@localhost ~]# nmcli device show # 莎看所有设备 GENERAL.DEVICE: enp0s3 GENERAL.TYPE: ethernet GENERAL.HWADDR: 08:00:27:35:C7:CE GENERAL.MTU: 1500 ....................... # 输出太多，就不全放在这里了 IP6.ADDRESS[1]: ::1/128 IP6.GATEWAY: [root@localhost ~]# nmcli device show enp0s8 # 查看指定设备 GENERAL.DEVICE: enp0s8 GENERAL.TYPE: ethernet GENERAL.HWADDR: 08:00:27:79:C4:B1 GENERAL.MTU: 1500 GENERAL.STATE: 100 (connected) GENERAL.CONNECTION: enp0s8 GENERAL.CON-PATH: /org/freedesktop/NetworkManager/ActiveConnection/7 WIRED-PROPERTIES.CARRIER: off IP4.ADDRESS[1]: 123.123.123.2/25 IP4.GATEWAY: IP6.GATEWAY: ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:8:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"修改网卡配置 一个设备可以有多个连接，在CentOS7中网络是以连接管理的。虽然每个设备可以有多个连接，但是同时生效的只能有一个。我们可以使用nmcli connection查看连接 [root@localhost ~]# nmcli connection show NAME UUID TYPE DEVICE enp0s8 ae99f48d-5f20-4a9c-a487-c4ebafa3f92e 802-3-ethernet enp0s8 enp0s3 2edc4731-888c-4102-8ff5-236ea47eeedb 802-3-ethernet enp0s3 我们可以进行如下操作nmcli connection add/delete/edit也就是增删改三个操作。 每一个连接都有一个名字，我们可以根据名字索引来操作对应到的连接。我们先来删除掉之前配置的网络。 [root@localhost ~]# nmcli connection delete enp0s8 # 这样可以删掉之前的连接 Connection 'enp0s8' (ae99f48d-5f20-4a9c-a487-c4ebafa3f92e) successfully deleted. 然后添加一个新的连接，名字叫’test_conn’，接口是’enp0s8’，类型是’ethernet’也就是以太网，ip使用v4版本123.123.123.123，子网掩码是24位，ipv4的网关是123.123.123.1 [root@localhost ~]# nmcli connection add con-name 'test_conn' ifname enp0s8 type ethernet ip4 123.123.123.123/24 gw4 123.123.123.1 Connection 'test_conn' (05c7cd70-a48e-4a12-a0de-9d57724cf0d0) successfully added. [root@localhost ~]# nmcli connection show test_conn # 这行命令的输出太多了就不展示了。但是我们可以通过这行命令看到自己创建的连接，信息和自己填写的命令相对应。 修改一个连接可以使用nmcli connection modify，下面我们来测试一下修改一个连接 [root@localhost ~]# nmcli connection show test_conn | grep ipv4.dns # 使用grep搜索查看dns设置 ipv4.dns: ipv4.dns-search: [root@localhost ~]# nmcli connection modify test_conn ipv4.dns 8.8.8.8 # 修改ipv4的dns地址 [root@localhost ~]# nmcli connection show test_conn | grep ipv4.dns ipv4.dns: 8.8.8.8 ipv4.dns-search: ","date":"2016-10-28","objectID":"/posts/linux-ip-nmcli/:9:0","tags":["Linux","Network"],"title":"使用 nmcli 和 ip 命令配置 CentOS/RHEL 的网络","uri":"/posts/linux-ip-nmcli/"},{"categories":null,"content":"0X00 什么是异常 程序在运行出错的时候就会抛出异常，异常时在正确的代码里发生的，不是代码出现了错误。下面就是一个异常 #!/usr/bin/python #coding=utf-8 num_1 = 10 num_2 = 0 # 很明显这里是用一个数字去除以0 # 小学老师就说过0不能作为被除数 # 那么我们来看Python是如何处理这个问题的 num_3 = num_1 / num_2 print num_3 运行这个程序就会报出下面的错误，错误提示说在hello.py这个文件的第6行，出现了一个错误integer division or modulo by zero也就是说Python解释器发现你试图除以0或者试图用0取模。 Traceback (most recent call last): File \"./hello.py\", line 6, in \u003cmodule\u003e num_3 = num_1 / num_2 ZeroDivisionError: integer division or modulo by zero 这里提示的ZeroDivisionError就是一个异常，我们可以在后面捕获这个异常，然后进行一些处理。如果不捕获这个异常的话，程序运行到这里，异常就会直接抛出到用户界面，中断程序的运行。 0X01 自己放出一个异常 我们可以用raise抛出一个自己的异常，这样我们可以在调试程序的时候判断到底出了什么错误，通过抛出的异常信息就可以判断。 #!/usr/bin/python #coding=utf-8 name = raw_input('name: ') if name == '': # 姓名不允许为空 raise Exception('name is null') # 抛出一个自定义的Exception内容是name is null age = input('age : ') if age \u003c= 0: # 不允许年龄小于等于0 raise Exception('age too little') # 爆出一个自顶一个Excep内容是age too little print 'name is ' + name print 'age is ' + str(age) 上面这段代码只是简单地输入name和age两个变量，合法的话就输出出来。我们这里运行一下试试 [root@iZ28jaak5nnZ ~]# ./hello.py name: shawn #合法输入的话，就可以顺利输出 age : 20 name is shawn age is 20 [root@iZ28jaak5nnZ ~]# ./hello.py name: # 这里变量内容为空 Traceback (most recent call last): File \"./hello.py\", line 6, in \u003cmodule\u003e raise Exception('name is null') # 就是在我设置的地方抛出了异常 Exception: name is null # 异常内容和类型都是我所规定的 [root@iZ28jaak5nnZ ~]# ./hello.py name: shawn age : -1 Traceback (most recent call last): File \"./hello.py\", line 10, in \u003cmodule\u003e raise Exception('age too little') Exception: age too little 我们可以用这种方式在自己的代码中抛出异常，用来做中间值检测，防止中间的数据出现意外导致一些不可思议的后果。 0X02 捕获异常 我们在代码中不管是解释器自己抛出的异常还是你手动抛出的异常，都可以手动的捕获到这个异常，并做出相应的处理。这样就可以提高代码的健壮性。在Python使用try...except...else来捕获处理异常。 try: # 这里执行一些可能会抛出异常的代码 except (ExceptionA, ExceptionB, ExceptionC): # 一个except可以捕获好多个异常 # 当抛出ABC三种异常的时候，执行这里的代码，执行完之后跳出try...except并继续执行代码 except ExceptD: # 当抛出D异常的时候就会执行这里的代码，执行完后也跳出 except ExceptE, e: # 当抛出E异常的时候在这里处理，e就是这个异常对象，我们可以看e中的信息 print e # 输出e except: # 当抛出了一个上面两个except捕获不到的异常的时候，执行这里的操作 else: # 当没有异常抛出的时候执行这里的代码 finally: # 不管代码有没有抛出异常，都会执行这里的代码 下面有一个样例，还是除0异常的样例，当除数是0的时候就抛出异常并捕获，然后处理这个异常（提示并重新输入），直到没有除0异常才计算成功并退出程序 #!/usr/bin/python #coding=utf-8 while True: num_1 = input('num1: ') num_2 = input('num2: ') try: num_3 = num_1 / num_2 except ZeroDivisionError: print 'num_1 is 0 !!!' continue else: print num_3 exit() 下面有一个运行样例 [root@iZ28jaak5nnZ ~]# ./hello.py num1: 9 num2: 0 num_1 is 0 !!! num1: 2 num2: 0 num_1 is 0 !!! num1: 0 num2: 0 num_1 is 0 !!! num1: 4 num2: 2 2 当然我们也可以将异常处理完了之后继续抛出，只要你需要。下面的代码和上面的是完全一样的，就只有12行的地方从continue换成了raise，意思就是抛出异常 #!/usr/bin/python #coding=utf-8 while True: num_1 = input('num1: ') num_2 = input('num2: ') try: num_3 = num_1 / num_2 except ZeroDivisionError: print 'num_1 is 0 !!!' raise # 只有这里是不同的，从continue换成了没有参数的raise，就是把异常继续抛出 else: print num_3 exit() 运行的样例就是下面这样的，执行下去之后会执行except中的处理代码，但是由于raise的存在还是会抛出这个异常 [root@iZ28jaak5nnZ ~]# ./hello.py num1: 123 num2: 0 num_1 is 0 !!! # 这里就是except处的处理代码 Traceback (most recent call last): File \"./hello.py\", line 9, in \u003cmodule\u003e num_3 = num_1 / num_2 ZeroDivisionError: integer division or modulo by zero ","date":"2016-10-27","objectID":"/posts/python-exception-simple/:0:0","tags":["Python"],"title":"Python 异常处理 捕获异常","uri":"/posts/python-exception-simple/"},{"categories":null,"content":"0X00 安装OpenSSH 一般情况下我们的系统中都是自带SSH服务端和客户端的，万一没有的话就需要我们手动安装这个服务。 yum install -y openssh 然后重启OpenSSH服务 systemctl restart sshd 0X01 两行简单的配置 OpenSSH的配置文件在/etc/ssh/目录下，有两个配置文件，一个是针对服务端的一个是针对客户端的，我们只需要修改针对服务端的sshd_config即可。 配置文件里比较重要的两行是PermitRootLogin和PasswordAuthentication。 PermitRootLogin 当这个值为yes时，才允许root用户使用ssh登陆 PasswordAuthentication 当这个值为yes时，允许使用密码登陆，反之则拒绝密码登陆(只能使用密钥)。 PermitRootLogin yes PasswordAuthentication yes 这里的配置就允许使用root用户登陆，也允许输入密码登陆 0X02 私钥和公钥————非对称加密 在ssh中可以使用用户名密码的形式登陆，也可以使用密钥的形式登陆。 非对称加密 就是说加密和解密用的密码不同。非对称加密里有公钥 和私钥 ，使用公钥加密的数据只有使用私钥才能解开，虽然是使用公钥加密的，但是并不能通过公钥反向解密。这点和传统的对称加密区别比较大。 下面假设有这么一个场景：有一台服务器S和三个管理员A1、A2、A3。 S生成了自己的一对公钥和私钥，将公钥公开出去，这时候A1就能能看到这个公钥，所以都可以用这个公钥将发给S的数据加密。虽然A2和A3也看到了这个公钥，但是不能通过这个公钥将这个加密的数据解开。数据只有在S上通过对应的私钥才能解开。 公钥：一般是公开出去，并用于加密 私钥：保存在自己这里，用于解密 公钥和私钥是一对的，一个公钥和一个私钥两两对应 0X03 在SSH生成公钥和私钥 在Linux里SSH可以使用公钥和私钥来登陆系统，也就是前面我们说的那个PasswordAuthentication选项，如果禁止密码登陆的话就只能使用公钥和私钥登陆了。 ssh-keygen可以生成一对公钥和私钥。我们一般在自己用户的主目录里的.ssh目录里执行这个命令、执行完了之后会提示输入加密，这里是给公钥私钥加密，可以暂时不用管，一路回车就行了，直到看到一堆乱七八糟的图像，类似于这样就算好了。 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: 09:7c:28:a5:75:sr:ab:5c:82:43:17:81:f8:78:zs:1e root@buyongkan.zhelishi.gaiguode The key's randomart image is: +--[ RSA 2048]----+ | . .+o. | | . .=8o. | | oo+-+. | | . =.oo.. | | X + SB. | | o . o | | o + | | ) | | | +-----------------+ .ssh目录如果不存在的话，执行一下ssh localhost然后输入密码登陆以下本地，就会有了。生成完之后目录里会多出两个文件，id_rsa 和 id_rsa.pub 后面pub结尾的是public也就是公钥，我们可以打开看看是一堆看似乱码的东西。 0X04 使用公钥和私钥免密码登陆 如果我们有两台机器，一个叫Server一个叫Desktop，我想让Desktop可以免密码登陆到Server上，就可以用这个方法。 原理大概是这样的：在Desktop上生成一对公钥和私钥，然后将Desktop上的公钥追加到Server的.ssh目录下的authorize_keys里，这个文件就是用来保存可以免密码登录到自己机器上的那些用户的公钥的。 [root@iZ28jaak5nnZ .ssh]# ssh-keygen #生成一对公钥私钥 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: 30:05:7d:63:sf:10:aa:cb:e1:b7:84:48:54:5f:42:4d root@iZ28jaak5nnZ The key's randomart image is: +--[ RSA 2048]----+ |..o o EO+o... | | o = * n. +. | | + * N o . | | S + a | | . . S | |m s | | | | x | | | +-----------------+ [root@iZ28jaak5nnZ .ssh]# ssh-copy-id -i id_rsa.pub root@182.234.214.243 #使用ssh-copy-id来将自己的公钥发送到Server上去，会自动找到那个文件 /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@182.254.214.250's password: # 还没配置好所以要输密码 Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'root@182.254.214.250'\" and check to make sure that only the key(s) you wanted were added. [root@iZ28jaak5nnZ .ssh]# ssh root@182.254.214.250 # 登陆 Last login: Mon Oct 17 13:47:23 2016 from 43.13.56.7 ☁ ~ hostname # 成功登陆 blog.just666.cn ☁ ~ 0X05 使用私钥签名 公钥私钥对可以对数据加密，是用公钥加密私钥解密。也可以使用公钥私钥对进行数字签名。 当Server公开自己的公钥之后，大家都可以用这个公钥进行加密，然后传给Server，Server用私钥解密就能看到内容。 Server如果想加密一段数据给其他人的话，可以用自己的私钥加密，将密文发送给其他人，其他人就能用Server的公钥去解密 。因为除了Server意外，任何人都不知道Server的私钥，所以其他人可以确信这条消息是Server发出来的。这种行为称之为签名 。 注意一个问题：公钥加密的数据可以用私钥解开 且私钥加密的数据也可以用公钥解开 ","date":"2016-10-17","objectID":"/posts/openssh-key/:0:0","tags":["Linux","OpenSSH"],"title":"OpenSSH 配置 免密码登陆 公钥和私钥 私钥签名","uri":"/posts/openssh-key/"},{"categories":null,"content":"0X00 hello,world 从一本 The C Programming Language 开始，我们就开始了几十年的’hello,world’之路。从那以后，机会所有的教程都从输出一句’hello,world’开始，这次也不例外。 #!/bin/bash echo \"hello,world\" 这里的第一行是注释，这个注释是很特殊的，他会告诉系统我们使用哪个解释器来运行下面的代码，这里我们用的是/bin/bash，当然Python的代码就要加上#!/usr/bin/python。 第二行就是输出一句’hello,world’。echo就是输出语句。 [root@mail shell]# chmod +x test.sh [root@mail shell]# ./test.sh hello,world ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:0:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"运行脚本 执行之前要给脚本一个x权限，也就是执行权限。然后直接运行就行了。还有一种运行方式是/bin/bash test.sh 这样就是执行bash这个命令，将test.sh作为参数传进去，这样就可以不必写第一行的解释器声明。但是建议使用第一种方式执行脚本。 0X01 使用变量 既然是编程，那一定会有变量。Shell编程里的变量和C、Java不同，我们不需要声明一个变量就能直接赋值，想下面这样。 #!/bin/bash str=\"hello,world\" echo $str 这里需要注意一点，我们在写一些代码的时候，可能习惯了像这样使用操作符str = \"hello,world\",也就是在操作符两端加上空格。但是在Shell编程里这样做是被禁止的，加了空格就会导致语法错误。所以Shell编程里的空格限制是很严格的。 给一个新的变量赋值的时候我们可以直接写变量名，但是我们调用这个变量的时候要给变量名前面加上一个$符号，就像上面我写的那样。当然最好写成下面这种形式${str}因为这样会更加清晰的显示出变量名。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:1:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"只读变量 在Shell中有一种变量叫‘只读变量’，顾名思义，这种变量的值不会被改变，是固定的，我们这样来声明一个只读变量readonly str。只读变量之前也是可以随意更改的，只是在后面给它加上了一个只读属性而已，就像下面这样。 #!/bin/bash str=\"hello,world\" readonly str str=\"hey,world\" echo str 就会报错：’./hello.sh: line 5: str: readonly variable' ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:2:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"删除变量 当我们不再使用一个变量的时候，可以把这个变量删除掉 #!/bin/bash str=\"hello,world\" unset str echo str 这里什么都不会输出，因为并没有str这个变量。在Shell中输出一个并不存在的变量不会有提示。 0X02 执行一行命令 既然是Shell编程，那么执行命令是最重要的事情了。所以在Shell编程里执行命令也是非常简单的，直接把要执行的命令写到这里就行了。 #!/bin/bash lscpu #这个命令是查看CPU相关信息的 0X03 字符串 字符串可以用单引号包起来，也可以用双引号包起来。单引号包起来的字符串会原封不动，会忽略转义字符和变量；双引号包起来的字符串会识别转义字符和变量。 #!/bin/bash a=\"hello\" str1='$a, world' #单引号字符串 str2=\"$a, world\" #双引号字符串 echo $str1 echo $str2 输出是这样的 [root@mail shell]# ./hello.sh $a, wrld #可以看到这里没有识别到变量 hello, world #这里是识别到了变量的 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:3:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"字符串拼接 Shell里拼接字符串的语法非常简陋，直接把两个字符串变量写在一起就行了。 #!/bin/bash str1=\"hello,\" str2=\"world\" echo $str1$str2 #就这么简单粗暴 这样就可以输出一个’hello,world’了 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:4:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"获取长度 在Shell编程里获取字符串长度不是通过一个len方法或者.length属性获取，而是通过下面这种并不直观的方式获取长度。 #!/bin/bash str=\"hello,world\" str_len=${#str} echo $str_len ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:5:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"部分截取 在Shell编程里我们可以截取一个字符串中的某一段，只需要两个参数，一个来指定开始位置，一个来指定结束位置。 #!/bin/bash str=\"hello,world\" echo ${str:1:4} 这样可以截取str字符串从1到4的部分。是一个闭区间，从0开始计数。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:6:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"查找位置 我们经常会需要从一段字符串里找到某个字符出现的位置，可以通过下面的方法来查找。 #!/bin/bash str=\"hello,world\" index=`expr index \"$str\" lo` #查找l或者o这个字符首次出现在字符串的哪个位置 0X04 搞个数组 既然是编程，那么当然要有数组这个最基本的数据结构了。但是Shell只支持一维数组，并不支持二维和多维数组。 #!/bin/bash array_str=(\"hello\" \"hey\" \"nihao\") echo ${array_str[2]} #定义的时候用的是小括号，调用的时候是大括号 echo ${array_str[@]} #这里的一个@表示数组里的所有内容 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:7:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"获取数组的长度 获取数组长度的方式和获取字符串长度的方式差不太多。 #!/bin/bash array_str=(\"hello\" \"hey\" \"nihao\") length=${#array_str[@]} #获取数组长度 echo $length length=${#array_str[2]} #获取数组中某个元素的长度 echo $length 0X05 别忘了注释 编程的时候给关键代码加上注释是一个非常好的习惯。Shell编程里只支持单行注释，不支持多行注释。单行注释是这样的 #!/bin/bash str=\"hello\" #str=\"world\" echo $str 这样输出的结果是’hello’而不是’world’因为哪一行被注释掉了，并不会执行。 那么我们需要多行注释怎么办呢？其实也不是不可以，我们可以用一个诡异的方式来实现多行注释：把需要注释掉的代码改写成一个函数，只要我们在后面不去调用这个函数，那不就和被注释掉是一样的效果了嘛。关于函数的问题下面会说的。 0X06 Shell参数 我们的Shell脚本经常是需要传入参数进来的，那么应该怎么传进来呢？我们有下面一段代码： #!/bin/bash echo \"1. $1\" echo \"2. $2\" echo \"3. $3\" echo \"all is $#\" echo \"pid is $$\" 我们运行一下这段代码，并传入两个参数 [root@mail shell]# ./hello.sh hello world 1. hello 2. world 3. all is 2 pid is 22017 $1 $2 $3 这些参数表示：传入的第几个参数 $# 表示传入的参数总数 $$ 表示这个脚本运行的PID （进程号） 0X07 运算符 运算符示例 #!/bin/bash val=`expr 1 + 1` #注意空格问题 echo $val 最常见的 + - * / = == 全都有的，不过注意的一点是，运算符两边一定要加空格，一定。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:8:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"bool布尔运算 ! 非 -o 或 -a 且 -eq 是‘判断是否相等，相等则true’ #!/bin/bash a=1 b=2 if [ $a -eq $b -o $a == 1 ] then echo \"ok\" else echo \"not ok\" fi 这段代码输出的是’ok’因为虽然$a和$b并不相等，但是后面的-o表示或，或后面的$a == 1成立了，所以还是输出了ok ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:9:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"逻辑运算 \u0026\u0026 and || or 下面这段Demo代码和上面的效果几乎是一样的 #!/bin/bash a=1 b=2 if [[ $a -eq $b || $a == 1 ]] #这里比上面多了一组中括号 then echo \"ok\" else echo \"not ok\" fi ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:10:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"字符串运算符 = 这里不是赋值，是判断两个字符串是否相等 != 这里是判断不相等 -n 检测字符串长度，不为0则返回true -z 检测字符串长度，为0则返回true str 检测字符串是否为空，这里的str表示一切字符串或者变量 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:11:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"运算符总结： ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"算术运算符 运算符 说明 举例 + 加法 `expr $a + $b` 结果为 30。 - 减法 `expr $a - $b` 结果为 -10。 * 乘法 `expr $a \\* $b` 结果为 200。 / 除法 `expr $b / $a` 结果为 2。 % 取余 `expr $b % $a` 结果为 0。 = 赋值 a=$b 将把变量 b 的值赋给 a。 == 相等。用于比较两个数字，相同则返回 true。 [ $a == $b ] 返回 false。 != 不相等。用于比较两个数字，不相同则返回 true。[ $a != $b ] 返回 true。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:1","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"关系运算符 运算符 说明 举例 -eq 检测两个数是否相等，相等返回 true。 [ $a -eq $b ] 返回 false。 -ne 检测两个数是否相等，不相等返回 true。[ $a -ne $b ] 返回 true。 -gt 检测左边的数是否大于右边的，如果是，则返回 true。[ $a -gt $b ] 返回 false。 -lt 检测左边的数是否小于右边的，如果是，则返回 true。[ $a -lt $b ] 返回 true。 -ge 检测左边的数是否大等于右边的，如果是，则返回 true。[ $a -ge $b ] 返回 false。 -le 检测左边的数是否小于等于右边的，如果是，则返回 true。[ $a -le $b ] 返回 true。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:2","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"布尔运算符 运算符 说明 举例 ! 非运算，表达式为 true 则返回 false，否则返回 true。[ ! false ] 返回 true。 -o 或运算，有一个表达式为 true 则返回 true。 [ $a -lt 20 -o $b -gt 100 ] 返回 true。 -a 与运算，两个表达式都为 true 才返回 true。 [ $a -lt 20 -a $b -gt 100 ] 返回 false。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:3","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"逻辑运算符 运算符 说明 举例 \u0026\u0026 逻辑的 AND [[ $a -lt 100 \u0026\u0026 $b -gt 100 ]] 返回 false || 逻辑的 OR [[ $a -lt 100 || $b -gt 100 ]] 返回 true ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:4","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"字符串运算符 运算符 说明 举例 = 检测两个字符串是否相等，相等返回 true。 [ $a = $b ] 返回 false。 != 检测两个字符串是否相等，不相等返回 true。[ $a != $b ] 返回 true。 -z 检测字符串长度是否为0，为0返回 true。 [ -z $a ] 返回 false。 -n 检测字符串长度是否为0，不为0返回 true。 [ -n $a ] 返回 true。 str 检测字符串是否为空，不为空返回 true。 [ $a ] 返回 true。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:5","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"文件测试运算符 操作符 说明 举例 -b file 检测文件是否是块设备文件，如果是，则返回 true。 [ -b $file ] 返回 false。 -c file 检测文件是否是字符设备文件，如果是，则返回 true。 [ -c $file ] 返回 false。 -d file 检测文件是否是目录，如果是，则返回 true。 [ -d $file ] 返回 false。 -f file 检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。[ -f $file ] 返回 true。 -g file 检测文件是否设置了 SGID 位，如果是，则返回 true。 [ -g $file ] 返回 false。 -k file 检测文件是否设置了粘着位(Sticky Bit)，如果是，则返回 true。[ -k $file ] 返回 false。 -p file 检测文件是否是具名管道，如果是，则返回 true。 [ -p $file ] 返回 false。 -u file 检测文件是否设置了 SUID 位，如果是，则返回 true。 [ -u $file ] 返回 false。 -r file 检测文件是否可读，如果是，则返回 true。 [ -r $file ] 返回 true。 -w file 检测文件是否可写，如果是，则返回 true。 [ -w $file ] 返回 true。 -x file 检测文件是否可执行，如果是，则返回 true。 [ -x $file ] 返回 true。 -s file 检测文件是否为空（文件大小是否大于0），不为空返回 true。[ -s $file ] 返回 true。 -e file 检测文件（包括目录）是否存在，如果是，则返回 true。[ -e $file ] 返回 true。 0X08 条件判断语句 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:12:6","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"if-else Shell 编程里的if语句和平时接触的编程语言有点区别 if后面要加一个中括号，中括号之间是条件，而且中括号前后都要有一个空格； if调节写好之后要写一个then表示接下来执行什么 then结束之后如果没有别的条件了的话就接一个if的逆字符串fi来表示if语句结束 如果还有其他条件那就用elif，然后也是一个中括号里写条件，then后面接要执行的语句 最后的else可写可不写，表示上面的条件没达成的话要执行的语句。 if语句的最后一定是一个fi，表示if语句的结束 #!/bin/bash if [ $1 == \"1\" ] then echo \"input is 1\" elif [ $1 == \"2\" ] then echo \"input is 2\" elif [ $1 == \"3\" ] then echo \"input is 3\" else echo \"input is big\" fi 运行起来是这样的，比如我运行的参数是./hello.sh 2 ，那么就会输出'2' ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:13:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"case 跟if一起的一般还有一个switch-case语句，但是Shell里没有switch这个关键字，但是功能是一样的。case $1 in表示的是$1这个变量去对比下面的选项。下面的每一个选项都要加一个回括号，然后写上要执行的语句，再加上两个连续的分号，表示这一段结束，最后的* )表示匹配其他全部没有匹配到的可能。 #!/bin/bash case $1 in \"1\" ) echo \"word is 1\" ;; \"2\" ) echo \"word is 2\" ;; * ) echo \"word is other\" ;; esac 0X09 几种循环 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:14:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"for 在Shell编程里有我们非常熟悉的for循环，但是语法个其他编程语言还是有一定的出入。 在Shell里用for循环的话，要给每一段循环体加上do...done，表示这之间的代码是循环体，注意for的一行后面没有冒号，像Python的话就会有个冒号。 #!/bin/bash for a in \"hello,wrld\" #循环输出字符串中的每个字符 do echo $a done for a in 1 2 3 4 5 #循环输出列表中的每一个数据 do echo $a done in关键字前面是一个a后面是一堆数据，我们可以理解成把a从后面的一堆数据上走一遍，也就是说相当于Python代码中这样 #!/usr/bin/python for i in ['123', '234', '345', '456']: print i ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:15:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"while #!/bin/bash while [[ $1 == \"2\" ]] do echo \"hello,world\" done 这段代码如果我是这样运行./hello.sh 2 那么因为$1是2所以就会进入死循环，否则就什么都不会输出。 while循环就是说，如果while后面的表达式成立，那么就执行一次循环。因为我这段代码没有改变$1的值，所以才会出现要么无输出要么死循环的情况。我们改成下面这种情况就好了，这意思是当我运行这个脚本，就执行10次。 #!/bin/bash a=0 while [[ $a -lt 10 ]] #这里的 -lt 的意思是 '当左边的值小于右边的值就返回true' do a=$a+1 echo \"hello,world\" done ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:16:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"until until循环就相当于C和Java中的do-while循环，表示一个‘直到’的效果。下面就是说，执行循环体里的内容，一直到$a的变量大于右边的数值的时候才停止，所以才输出了11次’hello,world' #!/bin/bash a=0 until [[ $a -gt 10 ]]; do a=$a+1 echo \"hello,world\" done 0X0A 写个函数 Shell里的函数有两种定义方式，效果是一样的。 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:17:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"方案1 方案一是直接写函数名，后面接一个括号，然后大括号里写上函数体就好了。 myFunction(){ echo \"hello,world\" } ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:18:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"方案2 方案二是function name()这样定义，先声明这是一个函数，然后写上函数名，最后也是一对小括号，大括号里写函数体。 function name(){ echo \"hello,world\" } Shell的函数也是有返回值的，不过返回值的类型很少不像Java中可以返回数字字符串甚至返回对象，在Shell中只能返回数字，还只能是0~255的。变量只有8个二进制位，就算你返回了一个255以上的数字也会返回出溢出之后的数字的。返回值是不可以赋值的，想使用函数的返回值的话要用$?，这个符号代表上一次调用的函数的返回值。 #!/bin/bash function my(){ return $1 } my 1 echo $? my 10 echo $? my 256 echo $? my -2 echo $? my 123.123 echo $? 这里前两个可以正常输出，256因为溢出了所以输出不正常，负数和浮点数不支持。 0X0B test测试 test测试可以测试各种数据，包括数字、字符串、文件。使用方法如下 #!/bin/bash if test -e /etc/passwd #判断是否存在这个文件 then echo \"ok\" #存在则输出ok else echo \"no\" #不存在则输出no fi ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:19:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"针对数字的测试 -eq 等于则为真 -ne 不等于则为真 -gt 大于则为真 -ge 大于等于则为真 -lt 小于则为真 -le 小于等于则为真 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:20:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"针对字符串的测试 = 等于则为真 != 不相等则为真 -z 字符串 字符串的长度为零则为真 -n 字符串 字符串的长度不为零则为真 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:21:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"针对文件的测试 -e 文件名 如果文件存在则为真 -r 文件名 如果文件存在且可读则为真 -w 文件名 如果文件存在且可写则为真 -x 文件名 如果文件存在且可执行则为真 -s 文件名 如果文件存在且至少有一个字符则为真 -d 文件名 如果文件存在且为目录则为真 -f 文件名 如果文件存在且为普通文件则为真 -c 文件名 如果文件存在且为字符型特殊文件则为真 -b 文件名 如果文件存在且为块特殊文件则为真 ","date":"2016-10-02","objectID":"/posts/shell-programming-beginner/:22:0","tags":["Shell","Linux"],"title":"Shell编程入门笔记  新手教程","uri":"/posts/shell-programming-beginner/"},{"categories":null,"content":"0X00 创建一个类 Python也是一个和C++、Java一样的面向对象编程语言，所以Python里也有类和对象。 class Person: #这是一个类 def sayHello(self): #这是一个方法 print 'hello,world' def setName(self, inputName): self.name = inputName def getName(self): return self.getName() 在类中创建的方法使用def关键字定义，每个方法有一个或以上的参数，selft就是实例化的对象自己。需要返回值就return一个，不需要就可以不写return Python的类和Java的类还是有点区别，Java的类里主要写的是属性和方法，Python里不写属性，因为Java的变量需要定义而Python的变量并不需要定义，最多也就是在前面个各个属性一个变量名并赋初值 0X01 实例化一个对象 类是一个很抽象的概念，可以由类实例化好多个对象出来。Java中我们习惯说成 new一个对象，而Python中并不需要new xiaoming = Person() #实例化了一个类 xiaoming.setName('xiaoming') #调用一个方法 print xiaoming.getName() xiaoming.sayHello() 此段代码接着上面的类声明 0X02 Python的私有 Java和其他好多面向对象编程语言中会有一个private关键字，将属性和方法约束为私有的。然而Python并不能直接支持private，间接地支持也不是真正的private。在Python中的private由一种诡异的方式模拟，在方法前加上连续的两个下划线 class Person: def __hello(self): return 'hello,world' def sayHello(self): print self.__hello() xiaoming = Person() xiaoming.sayHello() xiaoming.__hello() 运行起来就是这样的，前面的xiaoming.sayHello()因为是可以直接调用的，然后在方法里调用了私有的__hello()方法，所以可以正常执行；后面的xiaoming.__hello()因为不能直接调用，所以报错了。 hello,world Traceback (most recent call last): File \"./hello.py\", line 12, in \u003cmodule\u003e xiaoming.__hello() AttributeError: Person instance has no attribute '__hello' 其实在方法前面加了两个下划线并没有真的把方法改了个名字而已，改成了_Class__name的类型，一个下划线+类名+两个下划线+方法名，当我们知道了这个问题之后就可以这样调用‘私有’方法了，但是既然我们都将其设为了’私有’就不要这么用了。 \u003e\u003e\u003e xiaoming = Person() \u003e\u003e\u003e print xiaoming._Person__hello() #不要这样做，虽然可行 hello,world 0X03 继承 面向对象编程的特性之一：继承。Python中的继承和Java的语法差异还是挺大的，像下面这样可以声明两个类，让 一个类继承自另一个 #!/usr/bin/python class Person: def sayHello(self): print \"hello, I'm person\" class Jack: def sayHello(self): print \"helo, I'm Jack\" person = Person() jack = Jack() person.sayHello() jack.sayHello() 输出是这样的 hello, I'm person helo, I'm Jack 这也体现了面向对象的覆盖的思想 0X04 多继承 一个类可以继承自另一个类，也可以继承自其他好多个类。 class Student: #一个父类 def learn(self): print \"i'can learn\" class Coder: #另一个父类 def programming(self): print \"I'can programming\" class Jack(Student, Coder): #继承自两个雷的子类 pass jack = Jack() #可以调用两个父类中的方法 jack.learn() jack.programming() 书写多继承的时候要注意一个问题，如果某类继承自两个类，且那两个类有相同的方法，那么就会造成覆盖（重写） 是这样一个情况，如果我上面的Student和Coder类都有一个名为eat的方法，但是Student类里的eat方法是输出‘student can eat’但是‘Coder’类中eat方法是输出’coder can eat’，那么在写子类的时候就要格外小心 如果子类这样写class Jack(Student, Coder)那么这个子类的ear方法就会是’Coder‘’中的方法，输出‘Coder can eat’，如果这样写class Jack(Coder, Student)的话，输出就是’Student can eat‘’ 0X05 构造方法 我们实例化一个对象的时候是这样做的xiaoming = Person()其内部是调用了Person类的构造方法并返回给了xiaoming这个变量。在Java中在java中构造方法是一个没有返回类型的与类名同名的方法，但是在Python中所有构造方法都叫__init__()。自己不手动写构造方法的话Python就会自动生成一个，这点和Java相同。当然我们也可以自己手写构造方法 class Person: def __init__(self): print 'new a person' xiaoming = Person() 会输出一个new a person 这就能证明确实在实例化对象的时候回先调用这个类的构造方法。构造方法可以加参数，就像下面这样 class Person: def __init__(self, say='new a person'): print say 如果我实例化对象的时候这么写xiaoming = Person()那么就会输出一个’new a person’的字符串，因为我没传入参数，所以就使用了默认值，但是如果我这样实例化对象xiaohong = Person('xiaohong')就会输出一个’xiaohong’的字符串，因为我给构造方法传入参数了。 在Python中还有一个叫析构方法的__del__但是我们最好不要去碰它，因为Python里有像Java类似的自动垃圾回收，所以几乎不会需要我们自己去析构一个对象，但是如果真的有需要，也可以像C++一样手动析构 0X06 方法的重写 如果有一个类A，A中有一个sayHello的方法，然后有一个B类继承了A类，那么自然就也有了sayHello的方法，但是如果我们给B类单独设定sayHello方法会怎么样呢 class Person: def sayHello(self): print \"i'm Person\" class Man(Person): def sayHello(self): print \"i'm Man\" xiaoming = Man() xiaoming.sayHello() 输出的是\"i’m Man\"而不是\"i’m Person\"。这就是方法的重写，子类中写了一个和父类中相同名的方法，就会把已经继承过来的方法重写掉 ","date":"2016-09-14","objectID":"/posts/python-oop/:0:0","tags":["Python"],"title":"Python 之面向对象","uri":"/posts/python-oop/"},{"categories":null,"content":"0X00 什么是字典 字典，顾名思义就是通过一个条件可以找到相应的值，字典由Key-Value组成。像是下面这样创建一个字典 字典中的数据是没有顺序的，不像列表一样有顺序，在字典中是没有固定顺序的 \u003e\u003e\u003e a = {'name':'xiaoming', 'sex':'F', 'age':22} #直接创建一个字典 \u003e\u003e\u003e print a {'age': 22, 'name': 'xiaoming', 'sex': 'F'} \u003e\u003e\u003e print a['name'] xiaoming \u003e\u003e\u003e b = dict(name='xiaogang', sex='M', age=23) #通过dict函数创建一个字典 \u003e\u003e\u003e print b['name'] xiaogang 下文说的Key就是键， Value就是值 Key-Value 就是键值对，一个键对应着一个值 Key的值是可以随意改变的，但是Key的类型是固定的不能改变 如果为一个不存在的键赋值，那么会自动添加这个K-V 0X01 字典操作 ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:0:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"len 测量长度 测量这个字典中有多少 \u003e\u003e\u003e d = {'username':'admin', 'password':'123456') \u003e\u003e\u003e print len(d) 2 ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:1:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"d[k] 调用字典 根据已知的Key来查找Key所对应的Value \u003e\u003e\u003e d = {'username':'admin', 'password':'123456'} \u003e\u003e\u003e print d['username'] admin ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:2:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"d[k] = v 字典赋值 为某个特定的Key赋值，如果这个Key在字典中不存在则创建这个Key \u003e\u003e\u003e d = {'username':'admin', 'password':'123456'} \u003e\u003e\u003e d['password'] = '2336666' \u003e\u003e\u003e print d['password'] 2336666 ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:3:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"del d[k] 删除Key-Value 删除相应的Key和Value \u003e\u003e\u003e d = {'username':'admin', 'password':'123456'} \u003e\u003e\u003e del d['username'] \u003e\u003e\u003e print d {'password': '123456'} \u003e\u003e\u003e del d['username'] #删除一个不存在的K-V会抛出异常 Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e KeyError: 'username' ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:4:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"k in d 成员检查 检查某字典中是否存在某Key，成员检查时用的是Key而不是Value \u003e\u003e\u003e d = {'username':'admin', 'password':'123456'} \u003e\u003e\u003e 'username' in d True \u003e\u003e\u003e 'phonenumber' in d False 0X02 字典的递归 字典中是Key-Value，然而字典的Value是可以是字典的，也就是Key-Value的Value是字典，也就是递归。 这样就可以建立一个递归的字典，字典里的字典可以是一层一层递归包括下去 \u003e\u003e\u003e phones = { 'xiaoming':{ 'num':'123', 'addr':'hebei' }, 'xiaohua':{ 'num':'456', 'addr':'sichuan' } } \u003e\u003e\u003e print phones {'xiaoming': {'num': '123', 'addr': 'hebei'}, 'xiaohua': {'num': '456', 'addr': 'sichuan'}} \u003e\u003e\u003e print phones['xiaoming'] {'num': '123', 'addr': 'hebei'} \u003e\u003e\u003e print phones['xiaoming']['addr'] hebei 0X03 字典方法 字典有好多方法可以调用，对字典进行操作 ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:5:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"clear 清除 clear可以清除字典所有项，这项操作是直接操作原来的字典而不是修改字典然后返回新的字典 下面展示一下这个方法的用处 A \u003e\u003e\u003e x = {} \u003e\u003e\u003e y = x \u003e\u003e\u003e x['key'] = 'value' \u003e\u003e\u003e y {'key': 'value'} \u003e\u003e\u003e x = {} \u003e\u003e\u003e y {'key': 'value'} B \u003e\u003e\u003e x = {} \u003e\u003e\u003e y = x \u003e\u003e\u003e x['key'] = 'value' \u003e\u003e\u003e y {'key': 'value'} \u003e\u003e\u003e x.clear() \u003e\u003e\u003e y {} 对比A和B这两种情况，就大概知道什么时候clear方法可以发挥用处了 ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:6:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"copy 复制 可以复制一个全新的字典出来，并返回这个新的字典。字典的复制分为浅度复制和深度复制 浅度复制 \u003e\u003e\u003e x = {'username':'admin', 'machines':['foo', 'bar', 'baz']} \u003e\u003e\u003e y = x.copy() \u003e\u003e\u003e y['username'] = 'mlh' \u003e\u003e\u003e y['machines'].remove('bar') \u003e\u003e\u003e y {'username': 'mlh', 'machines': ['foo', 'baz']} \u003e\u003e\u003e x {'username': 'admin', 'machines': ['foo', 'baz']} 深度复制 \u003e\u003e\u003e from copy import deepcopy \u003e\u003e\u003e d = {} \u003e\u003e\u003e d['names'] = ['Alfred', 'Bertrand'] \u003e\u003e\u003e c = d.copy() \u003e\u003e\u003e dc = deepcopy(d) \u003e\u003e\u003e d['names'].append('Clive') \u003e\u003e\u003e c {'names': ['Alfred', 'Bertrand', 'Clive']} \u003e\u003e\u003e dc {'names': ['Alfred', 'Bertrand']} ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:7:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"fromkeys 空字典 使用给定的键来建立一个没有值的字典，也可以给一个默认的值让所有键的值都是这个默认值 \u003e\u003e\u003e dict.fromkeys(['name', 'age']) #一个纯空的字典 {'name': None, 'age':None} \u003e\u003e\u003e dict.fromkeys(['name', 'age'], 'unknow') #给键创建一个默认值 {'name':'unknow', 'age':'unknow'} ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:8:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"get 获取 比较宽松的获取数据，以前用d[k]的方式调用一个值的话，如果这个键不存在就会抛出异常，用get获取就不会这样 \u003e\u003e\u003e d = {} \u003e\u003e\u003e print d.get('name') #获取一个不存在的数据，不会抛出异常，而显示None None ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:9:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"has_key 判断键 判断字典中是否有这个键，返回True和False \u003e\u003e\u003e d = {'name':'admin', 'password':'123456'} \u003e\u003e\u003e d.has_key('name') True \u003e\u003e\u003e d.has_key('hehe') False ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:10:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"items iteritems 返回字典 items 可以将整个字典转化成列表并返回 iteritems 可以将整个字典转化成迭代器返回 \u003e\u003e\u003e d = {'username':'admin', 'password':'123'} \u003e\u003e\u003e d.items() [('username', 'admin'), ('password', '123')] \u003e\u003e\u003e d.iteritems() \u003cbuilt-in method iteritems of dict object at 0x7f62ac08e6e0\u003e \u003e\u003e\u003e list(d.iteritems()) [('username', 'admin'), ('password', '123')] ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:11:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"keys iterkeys 返回键 keys 以列表的方式返回整个字典中所有的key iterkeys 以迭代器的方式返回整个字典中所有的key \u003e\u003e\u003e d = {'username':'admin', 'password':'123'} \u003e\u003e\u003e d.keys() ['username', 'password'] \u003e\u003e\u003e d.iterkeys() \u003cdictionary-keyiterator object at 0x7f62ac0972b8\u003e ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:12:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"pop 出栈 因为字典中是没有顺序的，所以出栈的时候必须自己指定一个Key才能弹出这个K-V，如果了解“栈”这个数据结构的话就能非常清晰这个方法。使用pop弹出一个数据的时候回在原字典中删除这个数据并返回这个数据。 \u003e\u003e\u003e d = {'username':'admin', 'password':'123'} \u003e\u003e\u003e d.pop('password') '123' \u003e\u003e\u003e d {'username': 'admin'} ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:13:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"popitem 随机出栈 随机从字典中弹出一组K-V \u003e\u003e\u003e d = {'one':'1', 'two':'2', 'three':'3', 'four':'4', 'five':'5', 'six':'6'} \u003e\u003e\u003e d.popitem() ('six', '6') \u003e\u003e\u003e d.popitem() ('three', '3') \u003e\u003e\u003e d.popitem() ('two', '2') \u003e\u003e\u003e d.popitem() ('four', '4') \u003e\u003e\u003e d.popitem() ('five', '5') \u003e\u003e\u003e d.popitem() ('one', '1') ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:14:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"setdefault 设置默认 给字典中某个Key设定一个默认的Value，当这个Key没有Value的时候就默认为那个默认的Value，如果有数据则默认Value不会生效 \u003e\u003e\u003e d = {} \u003e\u003e\u003e d.setdefault('name', 'N/A') 'N/A' \u003e\u003e\u003e d {'name': 'N/A'} \u003e\u003e\u003e d['name'] = 'admin' \u003e\u003e\u003e d.setdefault('name', 'N/A') 'admin' \u003e\u003e\u003e d {'name': 'admin'} ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:15:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"update 更新字典 可以用一个新的字典去更新旧的字典，新旧字典中Key重合的部分以新字典为准，旧字典中有的Key且新字典中没有的话则不变，旧字典中没有的Key且新字典中有的话则添加这个K-V \u003e\u003e\u003e a = {'username':'admin', 'password':'123456'} \u003e\u003e\u003e b = {'password':'2336666', 'sex':'F'} \u003e\u003e\u003e a.update(b) \u003e\u003e\u003e a {'username': 'admin', 'password': '2336666', 'sex': 'F'} ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:16:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"values intervalues values 返回字典中所有的值组成的列表 intervalues 返回字典中所有的值相应的迭代器 ","date":"2016-09-13","objectID":"/posts/python-dict-simple/:17:0","tags":["Python"],"title":"Python 字典","uri":"/posts/python-dict-simple/"},{"categories":null,"content":"0X00 列表和元组 Python里有个东西叫做序列 ，可以想象成一堆数据。可以简单的通过序列实现数组、链表、栈和队列等数据结构。 序列有几种，常见的是列表和元组。 0X01 序列分片 我们可以从序列中截取一部分，这种操作被称为分片 分片的时候我们可以选择起始点和结束点，还能选择步长，甚至乃能倒序 分片使用:分隔开参数，一般情况下有两个参数，截取第一个参数到第二个参数，左开右闭 如果参数是负数的话，则表示倒数第几个 但是可以接受第三个参数，第三个参数表示步长。如果第二个参数是2那么就是接一跳一。 如果参数为空则表示极限。 具体可以看下面的代码 \u003e\u003e\u003e username = 'hello,world' \u003e\u003e\u003e print username[4:8] #截取从4到8，左开右闭 o,wo \u003e\u003e\u003e print username[4:-2] #截取4到倒数第4的参数，如果想要包括最后一个是不能用-1的，要用下面的方式 o,wor \u003e\u003e\u003e print username[2:] #截取包括最后一个的话不能用-1，因为-1是最后一个，然后区间是左开右闭，所有右边留空就表示极限了 llo,world \u003e\u003e\u003e print username[:] #两头取极限，就是完整的序列 hello,world \u003e\u003e\u003e print username[1:8:2] #演示步长，此处步长为2 el,o \u003e\u003e\u003e print username[8:0:-1] #当步长为-1的时候，就是从后向前的 row,olle 0X02 序列拼接 序列拼接就和Java里的字符串拼接差不多，可以单纯的用一个加号连在一起。当然Python比Java方便的一点就是，不只是字符串，什么东西只要是在序列里就能用序列拼接到一起。 Python中用加号的方式把序列拼接在一起是返回一个新的序列 而不是直接修改其中的一个序列。 \u003e\u003e\u003e username = 'hello' \u003e\u003e\u003e password = 'world' \u003e\u003e\u003e print username + ',' + password hello,world 序列不只能做加法，还能做乘法。序列乘n之后返回一个重复了n次的序列 \u003e\u003e\u003e username = 'hello,world' \u003e\u003e\u003e print username * 3 hello,worldhello,worldhello,world 0X03 空序列 空序列是空的，而不是值为0。也许现在不知道这东西干嘛用，等到时候用到了就豁然开朗了 username = [None] * 10 #这样就生成了一个长度为10的空序列 0X04 成员判断 成员判断就是判断一个元素是不是存在于一个序列里 这里返回的是布尔值，True或者False \u003e\u003e\u003e username = 'hello,world' \u003e\u003e\u003e ',' in username #判断元素是不是在序列里 True \u003e\u003e\u003e 'hello' in username #判断序列是不是在序列里 True \u003e\u003e\u003e username = ['hello', 'world'] \u003e\u003e\u003e 'hello' in username True \u003e\u003e\u003e 'hel' in username False 0X05 长度\u0026统计 可以统计一个序列的长度，还能计算出序列所有元素的最大和最小 具体的排序方法可以去网上找找或者自己尝试一下，针对每种类型的排序方式是不一样的 \u003e\u003e\u003e username = 'hello,world' #获取长度 \u003e\u003e\u003e print len(username) 12 \u003e\u003e\u003e number = [1, 2, 3, 4, 5, 6, 7, 8, 9] \u003e\u003e\u003e print max(number) #统计最大 9 \u003e\u003e\u003e print min(number) #统计平均 1 0X06 列表赋值 对列表的赋值和对其他编程语言里的数组赋值几乎是一样的 \u003e\u003e\u003e username = [0, 1, 2, 3, 4, 5] \u003e\u003e\u003e username[3] = 33 \u003e\u003e\u003e username[5] = 55 \u003e\u003e\u003e print username [0, 1, 2, 33, 4, 55] 0X07 列表删除数据 删除列表里的数据也非常易于理解 \u003e\u003e\u003e username = ['h', 'e', 'l', 'l', 'o', ',', 'w', 'o', 'r', 'l', 'd'] \u003e\u003e\u003e del username[5] #删除索引为5的元素，也就是第6个 \u003e\u003e\u003e print username ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd'] #处在原来5的位置的逗号不在了 0X08 列表分片赋值 分片赋值相当于把以前的部分数据盖上，写上新的数据 \u003e\u003e\u003e username = list('hello,world') \u003e\u003e\u003e username[1:5] = list('++++++') \u003e\u003e\u003e username[2:2] = list('------') \u003e\u003e\u003e print username #数据添加成功 ['h', '+', '-', '-', '-', '-', '-', '-', '+', '+', '+', '+', '+', ',', 'w', 'o', 'r', 'l', 'd'] \u003e\u003e\u003e username[3:5] = [] #理论上可以通过这种方式去删除列表中的数据，不过非常不建议这么做，没人愿意看这种代码，包括几天之后的你自己 0X09 列表常用方法 ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:0:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"append 和 extend append()方法是 向列表中添加一个元素 ，而extend()则是扩展原有列表。这两个方法都是修改之前的列表，而不是返回一个新的列表。 #!/usr/bin/python # coding=utf-8 if __name__ == '__main__': a = [1, 2, 3, 4, 5, 6, 7, 8, 9] b = ['a', 'b', 'c', 'd', 'e', 'f'] # a.append(b) a.extend(b) print a 上面的代码中，留下append()方法后运行结果如下，可以看到是向原来的列表中加入了一个元素 [1, 2, 3, 4, 5, 6, 7, 8, 9, ['a', 'b', 'c', 'd', 'e', 'f']] 留下extend()方法后运行结果如下，可以看到是将列表b中的元素扩展到了列表a中 [1, 2, 3, 4, 5, 6, 7, 8, 9, 'a', 'b', 'c', 'd', 'e', 'f'] ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:1:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"count 统计数据 计数，统计一个列表在另一个列表里出现了多少次 \u003e\u003e\u003e username = 'hello,world' \u003e\u003e\u003e username.count('l') 3 \u003e\u003e\u003e username = ['hello', 'hello', 'world', 'test'] \u003e\u003e\u003e username.count('hello') 2 ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:2:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"index 索引查找 查找第一个匹配的位置，并返回索引位置。如果返回0则是在0的位置上找到了，而不是没找到。没找到的话会直接抛出异常 \u003e\u003e\u003e username = 'hello,world' \u003e\u003e\u003e username.index('l') #返回位置 2 \u003e\u003e\u003e username.index('h') #返回0的位置 0 \u003e\u003e\u003e username.index('x') #找不到，抛出异常了 Traceback (most recent call last): File \"\u003cstdin\u003e\", line 1, in \u003cmodule\u003e ValueError: substring not found ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:3:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"insert 插入数据-准 向列表中插入数据，可选参数有插入位置和插入内容 \u003e\u003e\u003e username = [1, 2, 3, 4, 5, 6, 7] \u003e\u003e\u003e username.insert(3, 666) \u003e\u003e\u003e print username #向3的位置上插入666 [1, 2, 3, 666, 4, 5, 6, 7] ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:4:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"pop 弹出数据-出栈 将列表中的最后一个数据弹出来，返回且删除它。 如果知道数据结构中的栈的话，就明白了，可以比喻成 出栈 \u003e\u003e\u003e username = [1, 2, 3, 4, 5, 6, 7, 8] \u003e\u003e\u003e username.pop() #出栈 8 \u003e\u003e\u003e username.pop(2) #选择删除 3 ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:5:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"remove 匹配删除 移除匹配到的第一项 \u003e\u003e\u003e username = [1, 2, 3, 4, 5, 6] \u003e\u003e\u003e username.remove(2) \u003e\u003e\u003e print username [1, 3, 4, 5, 6] ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:6:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"sort 排序方法 可以通过Python内置算法排序，甚至还可以自定义参数 \u003e\u003e\u003e username = [1, 5, 2, 5, 65, 23, 54675, 8, 34, 5568, 345] \u003e\u003e\u003e username.sort() \u003e\u003e\u003e print username [1, 2, 5, 5, 8, 23, 34, 65, 345, 5568, 54675] ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:7:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"sorted 排序函数 类似sort，不过这个是返回一个新的列表 \u003e\u003e\u003e username = [1, 5, 2, 234, 3465, 234, 4657, 5, 65, 23, 54675, 8, 34, 5568, 345] \u003e\u003e\u003e sorted(username) [1, 2, 5, 5, 8, 23, 34, 65, 234, 234, 345, 3465, 4657, 5568, 54675] ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:8:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"list(reversed(x)) 反向排序 逆向 \u003e\u003e\u003e username = [1, 5, 2, 234, 3465, 234, 4657, 5, 65, 23, 54675, 8, 34, 5568, 345] \u003e\u003e\u003e username = sorted(username) \u003e\u003e\u003e print username [1, 2, 5, 5, 8, 23, 34, 65, 234, 234, 345, 3465, 4657, 5568, 54675] \u003e\u003e\u003e list(reversed(username)) [54675, 5568, 4657, 3465, 345, 234, 234, 65, 34, 23, 8, 5, 5, 2, 1] 0X0A 元组简介 元组一般用括号表示 元组和列表相比，列表可以修改而元组不能修改 \u003e\u003e\u003e username = ('h', 'e', 'l', 'l', 'o', ',', 'w', 'o', 'r', 'l', 'd') \u003e\u003e\u003e password = ('x', ) #创建一个只包含一个数据的元组 ","date":"2016-09-07","objectID":"/posts/python-list-tumple/:9:0","tags":["Python"],"title":"Python 之序列：列表、元组","uri":"/posts/python-list-tumple/"},{"categories":null,"content":"0X00 如何定义一个字符串 ** Python不需要定义 ** 其实就是这样的。。在Python中的字符串通常这么写 \u003e\u003e\u003e str1 = 'hello,world' \u003e\u003e\u003e str2 = \"It's work\" \u003e\u003e\u003e str3 = \"\"\" Jack:\"oh shit\" \"\"\" str1 的声明方式是最普通的声明方式 str2 的声明方式可以在字符串中存在单引号‘ str3 的声明方式可以在字符串中存在双引号 “ 还能在字符串中换行 0X01 拼接字符串 \u003e\u003e\u003e str1 = \"hello\" \u003e\u003e\u003e str2 = \",\" \u003e\u003e\u003e str3 = \"world\" \u003e\u003e\u003e print str1 + str2 + str3 hello,world 注意： 连接的时候加号左右都要是字符串，如果是字符串加数字就不行了。除非把数字转成字符串格式 0X02 输入字符串 标准输入就是直接把你输入的东西写到代码里，甚至可以用变量名 原始输入就是直接输入字符串，纯字符串 具体情况可以从下面的Demo中看到效果 \u003e\u003e\u003e hello = \"hello,world\" \u003e\u003e\u003e str1 = input(\"what's your name:\") #获取标准输入 hello \u003e\u003e\u003e str2 = raw_input(\"what's your name:\") #获取原始输入 \"hello\" \u003e\u003e\u003e print str1 hello,world \u003e\u003e\u003e print str2 hello 0X03 字符串格式化 学过C的能迅速的理解Python里的字符串格式化，没学过C的可以快速的理解Python里的字符串格式化 +_+ \u003e\u003e\u003e from string import Template \u003e\u003e\u003e text = Template(\"1---$a 2---$b 3---$c 4---$$\") # $a $b $c 都是字符串占位符，先写好后赋值 \u003e\u003e\u003e text.substitute(a=\"hello,\", b=\"world\", c=\"wow\") '1---hello, 2---world 3---wow 4---$' # %s 是字符串占位符，将后面的字符串加到前面 \u003e\u003e\u003e print \"hello, %s\" % (\"world\") hello, world # %15s 是将字符串向前扩充打15位 \u003e\u003e\u003e print \"hello, %15s\" % (\"world\") hello, world \u003e\u003e\u003e print \"%15s, world\" % (\"hello\") hello, world # %-15s 是将字符串向后扩充到15位 \u003e\u003e\u003e print \"%-15s, world\" % (\"hello\") hello , world 0X04 字符串处理函数 ","date":"2016-09-05","objectID":"/posts/python-string/:0:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"find 字符串查找 从一个字符串中查找另一个字符串，返回最左端索引，找不到就返回 -1 \u003e\u003e\u003e str = \"hello,world\" \u003e\u003e\u003e str.find(\",\") 5 \u003e\u003e\u003e str = \"hello,hello,world,world\" \u003e\u003e\u003e str.find(\",\", 8, 12) 11 如果find函数返回了0，并不是没找到，而是在0的位置找到了。毕竟程序员的世界从来都是从0开始数数的 ","date":"2016-09-05","objectID":"/posts/python-string/:1:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"join 连接 连接序列中的元素 \u003e\u003e\u003e str1 = ['hello', 'world'] \u003e\u003e\u003e str2 = \"---\".join(str1) \u003e\u003e\u003e print str2 hello---world 注意这里是 str.join(list) 而不是 list.join(str) ","date":"2016-09-05","objectID":"/posts/python-string/:2:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"lower 我要小写 返回字符串的全小写版 \u003e\u003e\u003e str1 = \"HELLO,WORLD\" \u003e\u003e\u003e str2 = str1.lower() \u003e\u003e\u003e print str2 hello,world ","date":"2016-09-05","objectID":"/posts/python-string/:3:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"replace 查找并替换 查找并替换全部 \u003e\u003e\u003e str1 = \"my world, my house, my phone\" \u003e\u003e\u003e str2 = str1.replace(\"my\", \"your\") #我就这么把所有东西过户给了你+_+ \u003e\u003e\u003e print str2 your world, your house, your phone ","date":"2016-09-05","objectID":"/posts/python-string/:4:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"split 分割 分割字符串 \u003e\u003e\u003e str1 = \"hello,world\" \u003e\u003e\u003e str2 = str1.split(\",\") \u003e\u003e\u003e print str2 ['hello', 'world'] #返回了一个列表 ","date":"2016-09-05","objectID":"/posts/python-string/:5:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"strip 清理字符串 去除两侧的东西 strip默认去除两侧的空格，当然也可以加参数 \u003e\u003e\u003e str1 = \" hello,world \" \u003e\u003e\u003e str2 = str1.strip() #默认去除空格 \u003e\u003e\u003e print str2 hello,world \u003e\u003e\u003e str1 = \"aaahello,worldaaa\" \u003e\u003e\u003e str2 = str1.srtip(\"a\") #加了参数就删除两侧的参数里的内容 \u003e\u003e\u003e print str2 hello,world ","date":"2016-09-05","objectID":"/posts/python-string/:6:0","tags":["Python"],"title":"Python 之字符串","uri":"/posts/python-string/"},{"categories":null,"content":"0X00 准备工作 1.一台海外或者香港的服务器/虚拟主机（后面统称VPS），要有独立IP 2.VPS的带宽和流量不能太小 3.一个连接VPS的软件，LInux/Mac可以用终端，Windows用户可以用XShell或者putty 4.VPS要使用Linux系统，Debian/Ubuntu/CentOS都行 开工之前最好有Linux适用基础 没有VPS的推荐一个购买地址，便宜好用banwagong 这个网站不是官网，但是起到了类似中文官网的作用，可以按照里面的推荐和教程去购买适合自己的VPS VPS买回来不止可以干这个、配置高一点的话还可以搭建一个独立博客和一些其他的服务 0X01 简述工作原理 ** 不通过伟大防火墙时 ** 我们访问某网站，流量从我们的机器一路跑到网站服务器，然后服务器响应数据再一路跑回来。 现在 ** 有了伟大的防火墙 ** 不让我们和某些网站交流了，我们可以搭一个 ** 梯子 ** ，让流量通过梯子。其实用 ** 镜子 ** 比喻会更好一点。 ** 有了镜子 ** 之后，我们的流量一路跑到镜子那里，镜子替我们将流量一路跑到网站服务器，然后网站服务器将数据一路发送到镜子，镜子再转发给我们。 所以造成下面几个问题： 1.你终端（电脑、手机等设备）产生的数据流量（代理流量）都要从梯子那里经过，所以梯子也要走一份流量。 2.你的网速同时取决于 你的速度、VPS的速度、网站服务器的速度 3.你的延迟同时取决于 你到VPS的延迟，VPS到网站服务器的延迟 0X02 安装软件 Debian/Ubuntu sudo apt-get update #更新系统 sudo apt-get install python-pip #安装Python-pip sudo pip install shadowsocks #安装shadowsocks CentOS sudo yum update #更新系统 sudo yum install python-setuptools \u0026\u0026 easy_install pip sudo pip install shadowsocks #安装shadowsocks 0X03 修改配置文件 配置文件默认不存在，我们直接创建一个就行vim /etc/shadowsocks.json 这里配置文件使用Json解析，看起来很清晰，便于识别修改 { \"server\":\"my_server_ip\", \"server_port\":8388, \"local_address\": \"127.0.0.1\", \"local_port\":1080, \"password\":\"mypassword\", \"timeout\":300, \"method\":\"aes-256-cfb\", \"fast_open\": false } server 修改成你VPS的外网ip server_port 是服务端用的端口，没有特殊需要就不用改了 local_address 本地地址，使用默认的127.0.0.1就行 local_port 客户机端口，使用默认的1080就行 password 设置密码 timeout 超时时间，使用默认即可 method 加密算法，使用默认aes-256-cfb即可，改用rc4-md5也行，不过客户端也要跟着改 fast_open 默认即可 0X04 如何开启关闭服务 这样开启服务ssserver -c /etc/shadowsocks.json -d start 这样关闭服务ssserver -c /etc/shadowsocks.json -d stop 这样重启服务关了再开就是重启+_+ ","date":"2016-07-23","objectID":"/posts/shadowsocks-build/:0:0","tags":["Proxy"],"title":"Shadowsocks 如何科学上网 搭梯子 简明教程","uri":"/posts/shadowsocks-build/"},{"categories":null,"content":"0X04 下载客户端 点击下载WIndows环境下的Shadowsocks客户端 点击下载Android环境下的Shadowsocks客户端 IOS版本的客户端在AppStore里有，不过要收费。也有免费的解决方案，因为不用IOS所以不清楚，自己去找找吧。 0X05 优化速度 注意： 1.前提是你的VPS限制流量但不限制带宽且你有足够的流量 2.所谓速度优化只针对大文件下载和在线视频有明显效果 3.速度优化之后会双倍流量发送，所以只有流量充足的用户适用 Debian/Ubuntu: wget --no-check-certificate https://raw.githubusercontent.com/tennfy/debian_netspeeder_tennfy/master/debian_netspeeder_tennfy.sh chmod a+x debian_netspeeder_tennfy.sh bash debian_netspeeder_tennfy.sh CentOS: wget --no-check-certificate https://gist.github.com/LazyZhu/dc3f2f84c336a08fd6a5/raw/d8aa4bcf955409e28a262ccf52921a65fe49da99/net_speeder_lazyinstall.sh sudo sh net_speeder_lazyinstall.sh 启动加速 nohup /usr/local/net_speeder/net_speeder venet0 \"ip\" \u003e/dev/null 2\u003e\u00261 \u0026 ","date":"2016-07-23","objectID":"/posts/shadowsocks-build/:0:1","tags":["Proxy"],"title":"Shadowsocks 如何科学上网 搭梯子 简明教程","uri":"/posts/shadowsocks-build/"},{"categories":null,"content":"0X00 简介 最近经常要在代码中使用到BASE64编码和MD5，所以把笔记贴在这里方便自己查找。 在配置postfix邮件服务器的时候发现，收到的邮件正文都是使用BASE64编码过的，所以才了解了一下这种编码。 MD5不算加密算法，但是可以用作摘要计算。 0X01 BASE64编码 Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于2的6次方等于64，所以每6个比特为一个单元，对应某个可打印字符。三个字节有24个比特，对应于4个Base64单元，即3个字节需要用4个可打印字符来表示。它可用来作为电子邮件的传输编码。在Base64中的可打印字符包括字母A-Z、a-z、数字0-9，这样共有62个字符，此外两个可打印符号在不同的系统中而不同。一些如uuencode的其他编码方法，和之后binhex的版本使用不同的64字符集来代表6个二进制数字，但是它们不叫Base64。 ———–维基百科 代码需要 import sun.misc.BASE64Encoder; public static String encodeing(String str){ byte[] b = null; String s = null; try{ b = str.getBytes(\"utf-8\"); }catch (Exception e){ e.printStackTrace(); } if (b != null){ s = new BASE64Encoder().encode(b); } return s; } 0X02 BASE64解码 代码需要import sun.misc.BASE64Decoder; public static String decoding(String str){ byte[] b = null; String result = null; if (str != null){ BASE64Decoder decoder = new BASE64Decoder(); try{ b = decoder.decodeBuffer(str); result = new String(b, \"utf-8\"); }catch (Exception e){ e.printStackTrace(); } } return result; } 0X03 MD5 MD5消息摘要算法（英语：MD5 Message-Digest Algorithm），一种被广泛使用的密码散列函数，可以产生出一个128位（16字节）的散列值（hash value），用于确保信息传输完整一致。MD5由罗纳德·李维斯特设计，于1992年公开，用以替换MD4算法。这套算法的程序在 RFC 1321 中被加以规范。 将数据（如一段文字）运算变为另一固定长度值，是散列算法的基础原理。 1996年后被证实存在弱点，可以被加以破解，对于需要高度安全性的数据，专家一般建议改用其他算法，如SHA-1。2004年，证实MD5算法无法防止碰撞，因此无法适用于安全性认证，如SSL公开密钥认证或是数字签名等用途。 ———–维基百科 虽说MD5已经被证明不安全，不过用作实验性的登陆验证还是没有问题的。（其实好多好多网站的密码都是MD5的，不信可以去社工库里看看） 代码需要import java.security.MessageDigest; public static String getMd5(String text){ try{ MessageDigest md = MessageDigest.getInstance(\"MD5\"); md.update(text.getBytes()); byte b[] = md.digest(); int i; StringBuffer buf = new StringBuffer(\"\"); for (int offset = 0; offset \u003c b.length; offset++){ i = b[offset]; if (i \u003c 0){ i += 256; } if (i \u003c 16){ buf.append(\"0\"); } buf.append(Integer.toHexString(i)); } return buf.toString(); // 32位 //return buf.toString().substring(8, 24); // 16位 }catch (Exception e){ e.printStackTrace(); return null; } } ","date":"2016-07-17","objectID":"/posts/java-md5-base64/:0:0","tags":["Encode"],"title":"Java 使用 MD5 和 BASE64","uri":"/posts/java-md5-base64/"},{"categories":null,"content":"0X00 NFS简介 NFS的全称是Net-File-System也就是网络文件系统。这和Samba与FTP不同，FTP的主要用途是用来上传和下载文件，Samba的主要功能是共享文件，而NFS的主要功能是用作文件系统。也就是说和NTFS、FAT32、EXT4等是类似的性质。我们可以将这个NFS当做一个磁盘分区挂载到自己的操作系统上，像操作自己的分区一样，甚至可以从NFS启动操作系统。 实验环境：两台虚拟机CentOS7.x 同处在一个内网环境下 0X01 安装NFS软件和服务 # 安装软件 yum install rpcbind yum install nfs-utils 0X02 创建测试目录并修改权限 # 创建测试用的目录 mkdir /home/share # 创建测试用的文件(让文件里有内容，方便后来判断是否搭建成功) ls / \u003e /home/share/test1 ls /etc/ \u003e /home/share/test2 # 创建挂载点、以后就把NFS挂载到这里 mkdir /home/test # 将这个测试目录设置为777的权限 chmod 777 /home/share 0X03 修改配置文件 配置文件是/etc/exports 使用文本编辑器打开配置文件并进行修改 # 添加如下配置 192.168.123.132是客户端IP /home/share/ 192.168.123.132(rw, sync) *(ro) /home/share/表示NFS的路径 192.168.123.132(rw, sync)表示192.168.123.132访问此NFS时使用后面的配置、具有rw权限（读写）、sync同步模式，表示内存中的数据实时写入磁盘 *(ro)表示所有IP访问时使用后面的配置、ro表示read only只读 每个路径下面可以接好多个访问项，就是192.168.123.132(rw, sync)或者*(ro)，使用空格分开 0X04 启动服务并检查NFS配置 # 启动服务 systemctl start portmap systemctl start nfs # 在客户端检查 192.168.123.123是服务端 showmount -e 192.168.123.123 # 如果输出成如下这样就是正确了 Export List for 192.168.123.123: /home/share * 0X05 挂载和卸载 # 挂载 mount -t nfs 192.168.123.123:/home /home/test # 卸载 umount /home/test ","date":"2016-06-12","objectID":"/posts/nfs-simple/:0:0","tags":["Linux","NFS","FS"],"title":"NFS 网络文件系统 安装 配置 挂载 卸载","uri":"/posts/nfs-simple/"},{"categories":null,"content":"SWAP分区是Linux的交换分区。交换分区实际存在于磁盘中，不过Linux系统可以将它当作内存使用，当物理真实内存不足的时候交换分区就可以和真实内存进行数据交换。简单地说就是从磁盘里拿出一块空间当作内存的储备区。虽说磁盘被拿来当作内存使用，但是速度还是磁盘的速度。可以想象下面一种日常生活的场景： 当你工作的时候一定是把桌洞里的东西拿出来放到桌面上，并且手里拿着一部分东西在操作，然后桌面和桌洞里的东西在不断的交换你手里的东西和桌面上的东西也一直在交换。 可以这么想：你从桌洞里拿出来了一本《5年高考3年模拟——理综》放到桌面上，然后拿起笔开始刷题。后来你又从桌洞里拿出来了一本《5年高考3年模拟——数学》放到桌面上，准备两本一起写（就是这么屌），写一题理综写一题数学。再后来你又想同时写语文，但是你发现你的桌面上已经没有了那么大的空间，就只能把语文放到桌洞里，每次拿出来语文就要把数学或是理综放到桌洞中。 计算机实际上是这样。从磁盘中打开了一个程序（一本书），操作系统就（另一个人）把这个程序运行起来放到内存中（放到桌面上），CPU将（部分）程序加载到寄存器中（拿起笔开始做题）。然后又运行了一个程序，CPU就在这两个程序之间切换（做完一题数学就换一题理综）。再运行一个程序，发现内存不足（桌面不够用了）就将内存和磁盘中的交换分区开始交换（把语文放到桌洞里，每次拿出来语文就要把数学或是理综放到桌洞中） 0X00 创建一个分区 || 创建一块文件 要想创建swap分区可以有两种分配空间的方式，一种是直接从磁盘中分出来一块用来当做swap分区，然后格式化为swap格式 # 对磁盘sdb进行分区 fdisk /dev/sdb 在提示符下输入n 然后会提示p 或 e的选项，P就是主分区，只能创建4个， e就是逻辑分区，不限个数。 我们创建一个逻辑分区就行。然后会提示新分区的起始扇区，使用默认的就好，直接回车 接下来是终止扇区，这里支持直接输入扇区号和输入大小两种方式，我们普遍直接输入分区大小 输入 +512M 按下回车，就是创建一个512M的分区。现在fdisk又回到了开始的提示符，如果之前的操作有错误可以直接输入q退出，如果没有问题就可以输入w保存并退出。 根据你的命令，就创建了一个名为sdb*的文件，这个文件就是分区文件。 另一种方式就是创建一个文件，然后将这块文件格式化为swap格式 # 创建一个512M的数据类型文件 dd if=/dev/zero of=swapfile bs=1024 count=523288 # if -\u003e input_file输入文件 of -\u003e output_file输出文件 bs -\u003e block_size块大小 count -\u003e 计数 解释一下特殊文件/dev/zero /dev/zero 这个文件放在Linux存放设备的目录下，如果以他为输出源，输出的全部都是二进制0 cat /dev/zero \u003e test 这个命令会生成一个test文件并不断的向该文件中输入二进制0 0X01 格式化\u0026\u0026激活\u0026\u0026挂载swap分区 # 格式化刚才的文件 # 根据创建的方式选择命令 mkswap /dev/sdb* mkswap swapfile # 激活swap分区 相等于挂载 # 根据创建的方式选择命令 swapon /dev/sdb* swapon swapfile 0X02 设置自动挂载 大家都是懒人，谁想每次开机都手动挂载一次分区呢？所以我们可以一劳永逸，将挂载设为自动 # 打开配置文件 vim /etc/fstab # 添加这样一行 # 根据创建的方式选择命令 /dev/sdb* swap swap defaults 0 0 /home/swapfile swap swap defaults 0 0 下面解释这个配置文件的每一列 第一列是数据块文件的位置 第二列是挂载点 第三列是分区类型 第四列是挂载参数 通常默认 第五列是备份选项 0代表不备份 1代表备份 通常为0 第六列是自检顺序 0代表不自检 1和2代表自检 如果是根分区要设为1，其他分区只能是2 通常为0 ","date":"2016-05-28","objectID":"/posts/linux-swap/:0:0","tags":["Linux","Swap","OS"],"title":"Linux 交换分区 swap 虚拟内存 理解虚拟内存","uri":"/posts/linux-swap/"},{"categories":null,"content":"我的另一篇 LVM 博客，可供参考 我的另一篇 LVM 博客，可供参考 我的另一篇 LVM 博客，可供参考 0X00 LVM是什么，有什么用 LVM的全称是Logical Volume Manager（逻辑卷管理）。是Linux下的一种磁盘分区管理机制，方便给分区（逻辑分区）扩容和压缩。最简单的可以理解成原始的磁盘分区管理是单纯的给每个独立的磁盘进行分区，然后对每个分区进行管理，这样的话每次扩容和压缩空间都会很麻烦。LVM就相当于把所有磁盘的分区都揉到一起，揉成一个大磁盘或者说是大分区，然后从大的中分出小的，这样的话扩容和压缩都会变得方便。 版权声明：图片来自Linux.cn 0X01 基础术语解释 PV 是 Physical Volume 物理卷 -–也就是真实的磁盘分区 VG 是 Volume Group 卷组 -–也就是好多PV组成的一个组 LV 是 Logical Volume 逻辑卷 -–就是从VG中分出来的分区 PE 是 Physical Extent 物理区域 -–是PV中最小的存储单元 LE 是 Logical Extent 逻辑区域 -–是LV中做小的存储单元 0X02 测试环境 V-Box 中的 CentOS 7.x 64bit 有两块或者以上数量的虚拟磁盘 磁盘大小在1GB以上 我这里/dev/sdb和/dev/sdc是刚刚添加的磁盘 root用户的~/lvm-mount用来挂载逻辑卷 使用root登陆(单纯的因为每次sudo太麻烦) 0X03 准备分区 使用fdisk为磁盘分区 不会使用fdisk的可以直接按着我说的敲 还是建议学LVM之前掌握最基础的fdisk分区和格式化 fdisk /dev/sdb # 使用fdisk给/dev/sdb分区 按n 回车 新建一个分区 按p 回车 选择新建分区为主分区 按 回车 选择默认分区号 按 回车 默认选择开始位置 输入 +100M 回车 选择使用100M为新分区的大小 输入 t 回车 设置分区类型 按 回车 默认选择刚才创建的分区 输入 8e 设置刚才创建的分区为 LVM 类型 重复上面的步骤，给/dev/sdb分出来三个区 0X04 创建物理卷 PV 创建物理卷的时候，可以大小不同，也可以是不同磁盘的分区，只要是 8e 类型的分区都是可以创建到物理卷中的，这里只是为了做示范 # 创建 pvcreate /dev/sdb1 pvcreate /dev/sdb2 pvcreate /dev/ddb3 # 检查 pvdisplay # 删除 (这步不要跟着做) pvremove /dev/sdb1 0X05 准备卷组 VG 创建一个包括/dev/sdb1 /dev/sdb2 /dev/sdb3 物理卷的卷组 命名为 volme-group1 # 创建 vgcreate volume-group1 /dev/sdb1 /dev/sdb2 /dev/sdb3 # 检查 vgdisplay # 删除 (这步不要跟着做) vgremove volume-group1 0X06 创建逻辑卷 LV 创建逻辑卷的时候要指定名称、大小和所属VG # 创建 lvcreate -L 100M -n LV1 volume-group1 # 检查 lvdisplay # 格式化 格式化成ext4类型 mkfs.ext4 /dev/volume-group1/LV1 # 挂载 mkdir ~/lvm-mount #设置一个挂载点 mount /dev/volume-group1/LV1 ~/lvm-mount # 挂载 # 删除 lvremove /dev/volume-group1/LV1 0X07 扩展LVM逻辑卷 调整逻辑卷大小是LVM最重要最有用的功能。 比如之前创建的100MB的分区不够用了，所以我们需要扩展一下那个分区的大小。虽然LVM很强大，但是扩展的时候还是需要卸载LV # 卸载LV umount ~/lvm-mount/ # 调整大小 lvresize -L 200M /dev/volume-group1/LV1 # 检查磁盘错误（非必须） e2fsck -f /dev/volume-group1/LV1 # 扩展文件系统 resize2fs /dev/volume-group1/LV1 # 验证 lvdisplay 0X08 压缩LVM逻辑卷 比如你发现有一个分区给了很大，但是完全用不到，那么就可以压缩它的空间，把空余的空间用在有用的地方。 # 同样，先卸载 umount /dev/volume-group1/LV1 # 检查错误 e2fsck -f /dev/volume-group1/LV1 # 更新文件系统信息 resize2fs /dev/volume-group1/LV1 100M # 压缩空间 lvresize -L 100M /dev/volume-group1/LV1 这里会弹出警告，告诉你这项操作可能会导致数据丢失，当然，一般是没有问题的 0X09 扩展卷组 有一天服务器的磁盘塞满了，你就新买了一块3TB的硬盘插到了电脑上，那么如何让这个3TB和之前的空间一起工作呢？我们可以把这个磁盘分区然后也放到之前的VG（卷组）中，这样通过之前的扩容功能就可以让新的3TB运用到系统中了。 # 先给新磁盘分区（参考0X03步骤） fdisk /dev/sdc # 然后创建PV(物理卷) pvcreate /dev/sdc1 # 将新PV添加到VG vgextend volume-group1 /dev/sdc1 # 验证一下 vgdisplay ","date":"2016-05-17","objectID":"/posts/linux-lvm-simple/:0:0","tags":["Linux","LVM"],"title":"Linux 的 LVM 逻辑卷管理 分区 划分 重划","uri":"/posts/linux-lvm-simple/"},{"categories":null,"content":"DHCP介绍 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:0:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"功能简介 DHCP是一个基于UDP的工作在应用层的协议，用来自动分配IP地址。 应用实例：一个办公室有十个人，每个人每天上下班都要带着自己的笔记本，所以每次都要手动配置IP地址，这样简直就不是计算机该干的事情是吧。。所以DHCP应运而生，它能够根据服务端的配置给连接到网络的客户机自动分配IP地址。 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:1:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"提供的服务 1.提供IP地址和子网掩码 2.提供IP地址对应的网络地址和广播地址 3.默认网关地址 4.DNS服务器地址 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:2:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"通俗的解释 你们寝室里有六个人（对应到客户机），每个人都需要用床（对应到IP地址）睡觉，所以每个人回到寝室都会需要一张床。这样的话每个人每次回到寝室的时候都需要宿管（对应到网络管理员）都需要给他分配一个床位，这样就很麻烦。 现在有一个“动态床位分配系统”（对应到DHCP服务器），你们每个人回到寝室的时候都会收到一张纸条，纸条上写了你可以使用哪个床位，这就简单多了。 再然后可能小明（对应到一个特定的客户机）有洁癖，他需要一个固定的床位（对应到一个需要固定IP的设备，比如打印机或者提供某些服务的服务器）。所以“动态床位分配系统”可以添加一条规则“叫小明的人来了就分配给他三号床位”，就解决了这个问题。 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:3:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"租约时间 DHCP分配给客户的IP是以租约 形式分配的。当客户接入到网络中，DHCP便会分配一个IP给客户机，当租约时间到的时候如果客户机还在使用这个IP那么就可以续约，继续使用当前IP而不是从新分配一个。 在CentOS 7.x 下搭建DHCP服务 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:4:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"0X00 安装DHCP服务 使用yum、rpm、源码等方式进行安装 yum install dhcpd ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:5:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"0X01 配置服务器网络服务 首先我们要将本地的网络配置成静态地址，并重启网络服务 vim /etc/sysconfig/network-scripts/enp0s3 这里的enp0s3是我的网卡，你需要将这里修改成你的网卡，另外在CentOS 7.x 以前的版本中，使用的是ethx的命名方式。 修改如下选项 BOOTPROTO=static #之前很有可能是dhcp，现在我们修改它为静态 ONBOOT=yes #以前可能是no，改为yes、就是打开网络服务的时候启动这个网卡 IPADDR=192.168.233.1 #配置文件中可能没有这个，没有的话就自己添加这行 # 这里的IP地址可以自定义，不过最后一位最好是1，这样便于识别 然后重启网络服务，如果配置文件没有错误的话，就可以正常启动了 systemctl restart network.service 这时候我们检查一下IP是不是已经变成我们设置的静态IP了 ifconfig enp0s3 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:6:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"0X02 配置DHCP服务 配置文件在这里 vim /etc/dhcp/dhcpd.conf 打开之后会有三行注释，我们初次学习配置的时候可以先不管它，只有服务真的跑起来并生效了我们才会进一步学习是吧。 在配置文件最后添加如下内容 #设置DHCP于DNS服务器的动态信息更新模式。初学时完全可以不理这个选项，但是全局设置中一定要有这个选项，否则DHCP服务不能成功启动。 ddns-update-style interim; #下面开始分配子网，网段是192.168.233.0 子网掩码是 255.255.255.0 #不能理解网段和子网掩码的可以去网上查一下。简单的说网段规定了分配IP的段（分配哪一段IP给客户机），子网掩码规定了网段的大小 subnet 192.168.233.0 netmask 255.255.255.0 { range 192.168.233.100 192.168.233.199; #分配给客户机的IP从192.168.233.100开始到192.168.233.199 option routers 192.168.233.2; #设置网关 default-lease-time 600; #默认租约时间 max-lease-time 7200; #最大租约时间 } ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:7:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"0X03 给特定客户分配特定地址 上面也说过，如果同一网络内有需要固定IP的设备，我们也可以通过DHCP来给他分配固定的IP。（就比如说那个有洁癖的同学） 首先要说一下DHCP识别主机的方式： DHCP通过接入客户的网卡的MAC地址来判断客户，所以如果你的机器有两块网卡，并且两块网卡同时接入了网络，那么DHCP就会认为有两个设备接入了网络，并给你的机器分配两个IP MAC地址并不是“苹果地址”。。。 MAC地址是“物理地址”，每块网卡在出厂的时候都会有一个全球独一无二的MAC地址，MAC地址是一个48位2进制的数字，通常表达为六段两位十六进制。 下面我们继续修改DHCP配置文件，在刚才的配置后面追加下面的内容 host Client_C { #有一个主机，叫Client_C hardware ethernet 08:00:27:5e:04:27; #MAC地址是08:...:27的网卡 fixed-address 192.168.233.123; #分配给它192.168.233.123的IP } 这样配置的话，不管何时，只要这个CLient_C接入到了这个网络中，那么它获取的IP就是固定的这一个，并不会变 然后重启一下DHCP服务就好了 systemctl restart dhcpd.service 好了，至此DHCP的基础配置就搞定了 ","date":"2016-05-12","objectID":"/posts/linux-dhcp-server/:8:0","tags":["Linux","DHCP"],"title":"Linux 配置 DHCP 服务器 简明教程","uri":"/posts/linux-dhcp-server/"},{"categories":null,"content":"0X00 什么是链接文件 Linux中的链接文件就相当于是Windows中的快捷方式，通过链接文件可以访问到链接指向的源文件。但是Linux下的链接文件和Windows中的快捷方式还是有一定的区别。Linux中有两种链接文件硬链接 和软连接 也称为符号链接 在介绍链接文件之前先要介绍一下Linux文件系统中的inode inode是Linux文件系统中文件的唯一定位器，每一个文件都有一个inode，也是唯一的，每一个inode唯一对应一个文件。 还要熟悉一下指针的问题，如果学习过C/C++的话理解起来会很轻松 计算机中所有的数据都是保存在磁盘里的，使用的时候会读取到内存中，而磁盘和内存都是一块一块的地方，我们用地址来找到数据的具体存放位置。保存了文件或者数据存放地址的物体（通常是变量）就称为指针。 我们在Linux的操作界面看到的文件，都是一个指针，他们指向磁盘的某个具体位置。这样我们每次点开文件的时候，系统就可以在磁盘对应的位置打开我们需要的文件。 0X01 硬链接 硬链接就是指向磁盘中具体位置的指针。如果我们创建一个文件，那么就会在磁盘中分配一块位置用来存储这个文件，并且创建一个指针方便我们找到这个文件。然后我们创建一个这个文件的硬链接，就相当于又创建了一个指向磁盘中存放文件的位置的指针 ，所以就算我们删除了之前创建的文件（其实就是一个指针），文件本身也不会被删除，因为还是有一个指针是指向文件存储位置的，所以我们还是可以通过后来创建的硬链接来访问到原来的文件。 0X02 软链接——符号链接 **软连接可以大致理解成指向指针的指针。**如果我们创建一个文件，并且创建了这个文件的一个软连接，那么如果删除了原来的文件，那这个文件就真的找不到了。因为软连接只是指向之前（指向具体文件位置）的指针，所以那个软连接就指向一片空白区域了，以前存在的文件也找不到了。就好像我们在Windows中卸载了一个软件，但是桌面上的快捷方式还可能存在，现在我们打开那个快捷方式是不能访问到软件的，因为已经被我们删除了。 0X03 操作演示 首先我这里有一个hehe.tar.gz的文件，我们查看一下它的inode是33515290。然后创建一个硬链接名为heihei.tar.gz，再查看这两个文件的inode，发现两个文件的inode是一样的。再创建一个软连接名为haha.tar.gz，继续查看文件inode。可以看到系统给haha.tar.gz分配了一个新的inode，并且ls的时候有个箭头显示了指向什么文件。 ls -i 可以显示文件的inode 0X04 软硬链接对比 ","date":"2016-05-11","objectID":"/posts/linux-link/:0:0","tags":["Linux"],"title":"Linux 软链接(符号链接)/硬链接 理解Linux链接","uri":"/posts/linux-link/"},{"categories":null,"content":"硬链接： 不分配新的inode 不可以在不同的文件系统之间链接 只有root才能创建目录的硬链接 只能源于存在的文件 ","date":"2016-05-11","objectID":"/posts/linux-link/:1:0","tags":["Linux"],"title":"Linux 软链接(符号链接)/硬链接 理解Linux链接","uri":"/posts/linux-link/"},{"categories":null,"content":"软链接： 分配新的inode 不受文件系统的限制 指向源文件位置的标识 可以链向不存在的文件 ","date":"2016-05-11","objectID":"/posts/linux-link/:2:0","tags":["Linux"],"title":"Linux 软链接(符号链接)/硬链接 理解Linux链接","uri":"/posts/linux-link/"},{"categories":null,"content":"如果我们只有一台服务器，应该怎么实现让这台服务器同时处理PHP和JSP的请求？ 这里的解决方案是通过Apache的虚拟主机（vhost）来进行端口转发。 Apache会通过访问服务器的域名将请求转发至不同的端口或者不同的服务器。 0X00 前提\u0026目的 前提： 拥有一个域名，并有两个A解析，同时解析到这台服务器的IP 分别拥有一个JSP和PHP的页面（网站） 目的： 使用php.test.com访问的时候解析到PHP的网站上 使用jsp.test.com访问的时候解析到JSP的网站上 操作系统： Centos 7.x 如果是之前的版本或是其他系统可能出现不同的情况 0X01 安装httpd (Apache) 安装并启动服务 yum install httpd systemctl start httpd.service 0X02 安装PHP yum install php 0X03 安装JDK用来配合JSP yum install java-1.8.0-openjdk 0X04 安装tomcat用于解析JSP页面 yum install tomcat tomcat-webapps tomcat-admin-webapps systemctl start tomcat.service 0X05 配置httpd用于同时支持PHP和JSP 打开配置文件 vim /etc/httpd/conf/httpd.conf 在配置文件的最前端添加如下内容 NameVirtualHost *:80 \u003cVirtualHost *:80\u003e ServerName php.test.com #指定一个域名 DocumentRoot /var/www/html #PHP网站的位置 ErrorLog logs/php.test.com-error.log #日志位置 CustomLog logs/php.test.com-access.log common #日志位置 \u003c/VirtualHost\u003e \u003cVirtualHost *:80\u003e ServerName jsp.test.com #指定另一个域名 DocumentRoot /var/lib/tomcat/webapps/ROOT #JSP网站的位置 ErrorLog logs/jsp.test.com-error.log #日志位置 CustomLog logs/jsp.test.com-access.log common #日志位置 ProxyPass / http://127.0.0.1:8080/ #转发位置 ProxyPassReverse / http://127.0.0.1:8080/ #转发位置 \u003c/VirtualHost\u003e 0X06 最后 systemctl restart httpd.service systemctl restart tomcat.service 现在就可以使用php.test.com 和 jsp.test.com分别访问到PHP和JSP的页面了 ","date":"2016-05-08","objectID":"/posts/apachepei-zhi-xu-ni-zhu-ji-virtualhost-duo-zhan-dian/:0:0","tags":["Apache","VirtualHost"],"title":"Apache配置虚拟主机 VirtualHost 多站点","uri":"/posts/apachepei-zhi-xu-ni-zhu-ji-virtualhost-duo-zhan-dian/"},{"categories":null,"content":"首先需要安装好MySQL/Mariadb的服务端和客户端，并且能连接到服务端 命令中的大写字母是SQL的关键字，小写字母是自己的相关属性和数据 0X00 连接到数据库 使用mysql连接到127.0.0.1并用root用户登陆，密码等待输入 mysql -h 127.0.0.1 -u root -p 0X01 创建数据库 创建一个名为school的数据库 CREATE DATABASE school; 0X02 建立一个表 建立一个名为student的表 索引： 10个字符长度的name 不能为空 11个字符长度的number 不能为空 int类型的age 不能为空 use school; 使用school这个数据库 CREATE TABLE student( name VARCHAR(10) NOT NULL, number VARCHAR(11) NOT NULL, age INT NOT NULL, PRIMARY KEY (number) ); 0X03 查询数据库和表 SHOW DATABASES; 查看所有数据库 SHOW BALES; 查看正在使用的数据库中的表 0X04 插入数据 INSERT INTO student VALUES('lilei','666',15); 插入新的数据，按照顺序写 INSERT INTO student (name)VALUES('hanmeimei'); 自定义顺序写入 0X05 查询数据 SELECT name FROM student; 查询student表中的所有name SELECT name FROM student WHERE number=0002; 查询student表中number为0002的name SELECT name FROM student WHERE age BETWEEN 20 and 30; 查询student表中age在20到30之间的name 0X06 更新数据 UPDATE student SET name='xiaohei' WHERE number='0002'; 把所有number为0002的name更新为xiaohei 0X07 删除数据 DELETE FROM student WHERE number='0002' 删除所有number为0002的数据 ","date":"2016-03-15","objectID":"/posts/mysql-super-simple/:0:0","tags":["MySQL","Database"],"title":"Mariadb/MySQL 增删查改 数据库操作 建表 建数据库","uri":"/posts/mysql-super-simple/"},{"categories":null,"content":"0X00抽象类\u0026接口简介 抽象类 abstract 抽象修饰符——抽象就是为了让子类集成的，并不能直接实现一个对象 抽象类中所有抽象方法都要在子类中实现 拥有抽象方法的类必须声明为抽象类 抽象类可以有非抽象的方法 接口 interface 接口修饰符——接口是为了让类实现的 变量默认是public static final并且不能改变 方法默认是public abstract并且不能改变 接口不实现方法 0X01抽象类和接口的区别 抽象类可以实现方法细节，接口不能 抽象类的变量可以是各种类型的，接口不能 抽象类可以有静态代码块和静态方法，接口不能 一个类可以实现多个接口，而只能继承自一个抽象类 继承可以理解成“是不是”，接口可以理解成“有没有” 0X02举个例子 有一个接口CanFly public interface CanFly { public abstract void fly(); } 有一个抽象类Bird public abstract class Bird { int age; void eat(){ System.out.println(\"I can eat insect~\"); } } 有一个Sparrow类继承自Bird public class Sparrow extends Bird implements CanFly{ public void fly(){ System.out.println(\"I can fly\"); } } 有一个抽象类Airplane public abstract class Airplane { double price;//价格 void Crash(){ //坠毁 System.out.println(\"This airplane is crashed!\"); } } 有一个Jian_10类继承自Airplane public class Jian_10 extends Airplane implements CanFly{ public void fly(){ System.out.println(\"I can fly\"); } } 有一个包含主方法的类来测试 public class Main { public static void main(String[] args) { Jian_10 A_0 = new Jian_10();//实例化A_0号战机 Sparrow xiaoMing = new Sparrow();//没错，这只麻雀叫小明 //我们都能飞 A_0.fly(); xiaoMing.fly(); //小明吃饭了 xiaoMing.eat(); //战机坠毁了 A_0.Crash(); } } 运行结果是这样的 I can fly I can fly I can eat insect~ This airplane is crashed! 0X03粗略解释 大概是这么回事： Airplane和Bird是两个抽象类，Jian_10和Sparrow分别继承自他们，所以子类可以直接调用父类的方法。且Jian_10和Sparrow还有接口CanFly 。然后Jian_10和Sparrow实现了接口CanFly中声明的fly方法（必须实现）。 如果以后想要修改Airplane和Bird两个父类的方法的时候，比如我不想让Bird吃东西了或者Airplane不会坠毁了，就只需要修改Airplane和Bird中相应的方法。 一个类只能继承自一个类\u0026抽象类，但是可以实现多个接口 比如，Airplane和Bird有很多相同的方法，但是实现不尽相同，我们就可以把这些方法放到一个接口中。 ","date":"2015-11-21","objectID":"/posts/java-abstract-class-interface/:0:0","tags":["Java","Abstract"],"title":"Java 抽象类和接口 理解抽象类和接口","uri":"/posts/java-abstract-class-interface/"},{"categories":null,"content":"0X00 编译环境 Ubuntu 14.04 + GNU/gcc 如果要在Windows下编译的话，* 可能 * 需要注释掉16-18的编译预处理，还 * 可能 * 要注释掉getch()的函数声明及定义，最后在加上conio.h的头文件。 0X01 遇到的问题 Windows中能使用getch()函数，这个函数是以输入流的方式输入。（简单地说就是按下去一个按键就能有反应，而不用点击回车）。但是在Linux环境下没有这个函数也没有connio.h的头文件。但是每次按一下还要按回车还是挺逆天的。不过我在网上找到了替代品（感谢 幽鬼 ） http://my.oschina.net/yougui/blog/111345 0X02 代码实现 /******************************************************************* * Project name : push the boxs * Create date : 2015.10.17 * Last modify date : 2015.10.19 * Auther name : mouse_ts * E-mail address : michaelhaozi@hotmail.com * Description : this is game, you control a boy push the boxs * to the destination. but you can't push the stone and two boxs. * if you'r box touch the wall , you can't pull it. * ****************************************************************/ #include \u003cstdio.h\u003e #include \u003cstring.h\u003e #include \u003cmath.h\u003e #include \u003cstdlib.h\u003e #include \u003ctermios.h\u003e//using getch() #include \u003cunistd.h\u003e #include \u003cassert.h\u003e //this is constant #define TRUE 1 #define FALSE 0 #define MAX 10 #define WALL 6 #define PLAYER 7 #define BOX 8 #define BLANK 5 #define DES 9 #define W 'w' #define A 'a' #define S 's' #define D 'd' //this is game map int map[MAX][MAX]; int ok = 0; //player struct player { int x; int y; }player; //boxs struct box { int x; int y; }box_1, box_2, box_3; //des struct des { int x; int y; }des_1, des_2, des_3; //statement function void initMap(); //init the map void initPlayer(); //init the player void initBox(); //init the boxs void initDes(); //init the des void printMap(); //print the map void setMap(); //set the player, boxs, des char getch(); //getch() void goUP(); //go up void goDown(); //go down void goLeft(); //go left void goRight(); //go right int computingSuccess();//computing how many box seccessd int main() { char ch; system(\"clear\"); //init the game initMap(); initPlayer(); initBox(); setMap(); printMap(); //control the boy while (ch = getch()) { switch(ch)//where is the boy move { case W: goUP(); break; case A: goLeft(); break; case S: goDown(); break; case D: goRight(); break; defualt: printf (\"You should press w, a, s, d to control the boy to move\\n\"); } setMap(); system(\"clear\"); printMap(); if (computingSuccess() == 3) break; else continue; } system(\"clear\"); printf (\"\\n\\n\\n\\n\\n\\n\\n You win the game!\\n\"); getch(); system(\"clear\"); return 0; } //getch() by.YouGui http://my.oschina.net/yougui/blog/111345 char getch() { int c = 0; struct termios org_opts, new_opts; int res = 0; res = tcgetattr(STDIN_FILENO, \u0026org_opts); assert(res == 0); memcpy(\u0026new_opts, \u0026org_opts, sizeof(new_opts)); new_opts.c_lflag \u0026= ~(ICANON | ECHO | ECHOE | ECHOK | ECHONL | ECHOPRT | ECHOKE | ICRNL); tcsetattr(STDIN_FILENO, TCSANOW, \u0026new_opts); c = getchar(); res = tcsetattr(STDIN_FILENO, TCSANOW, \u0026org_opts); assert(res == 0); return c; } //init this map void initMap() { int i, j; for (i = 0; i \u003c MAX; i++) { for (j = 0; j \u003c MAX; j++) { map[i][j] = WALL; } } for (i = 2; i \u003c 8; i++) { map[i][2] = BLANK; map[i][3] = BLANK; map[i][5] = BLANK; map[i][6] = BLANK; map[i][7] = BLANK; } map[5][4] = BLANK; initDes(); } //print map void printMap() { printf (\"This is a game !\\n\"); int i, j; for (i = 0; i \u003c MAX; i++) { for (j = 0; j \u003c MAX; j++) { if (map[i][j] == WALL) printf (\" # \"); else if (map[i][j] == BOX) printf (\" @ \"); else if (map[i][j] == PLAYER) printf (\" X \"); else if (map[i][j] == BLANK) printf (\" \"); else if (map[i][j] == DES) printf (\" O \"); } printf (\"\\n\"); } } //init the player void initPlayer() { player.x = 2; player.y = 2; } //init the boxs void initBox() { box_1.x = 3; box_1.y = 6; box_2.x = 4; box_2.y = 3; box_3.x = 6; box_3.y = 3; } //init the des void initDes() { des_1.x = 5; des_1.y = 7; des_2.x = 6; des_2.y = 7; des_3.x = 7; des_3.y = 7; } //set map void setMap() { int i, j; //set blank for (i = 2; i \u003c 8; i++) { map[i][2] = BLANK; map[i][3] = BLANK; map[i][5] = BLANK; map[i][6] = BLANK; map[i][7] = BLANK; } map[5][4] = BLANK; //set des map[des_1.x][des_1.y] = DES; map[des_2.x][des_2.y] = DES; map[des_3.x][","date":"2015-10-19","objectID":"/posts/c-sokoban/:0:0","tags":["C"],"title":"C语言 推箱子 gcc编译通过","uri":"/posts/c-sokoban/"},{"categories":null,"content":"实现简单日历 读取当前时间并显示本月日历 输出格式化 #!/usr/bin/python2 #It's my first Python program import time #get date 获取当前日期 year = time.strftime('%Y', time.localtime(time.time())) year = (int)(year) month = time.strftime('%m', time.localtime(time.time())) day = time.strftime('%d', time.localtime(time.time())) day = (int)(day) week = time.strftime('%w', time.localtime(time.time())) week = (int)(week) for i in range(0, day - 1): week = week - 1 if week == -1: week = 6 #judge leap 判断是否是闰年 if year % 4 == 0 and year % 400 != 0 or year % 400 == 0 : isLeap = True else : isLeap = False #all Day这个月一共有多少天 if month == '01' or month == '03' or month == '05' or month == '07' or month == '08' or month == '10' or month == '12' : allDay = 31 elif month == '02' and isLeap : allDay = 29 elif month == '02' and not isLeap : allDay = 28 else : allDay = 30 #print CAL 输出这个月的日历 print ' ' + str(year) + ' ' + str(month) print '' print 'Sun Mon Tue Wed Thu Fri Sat' for i in range(week) : #print space 输出空白部分 print ' ', for i in range(1, allDay + 1) : #print everday 输出日期 x = str(i) print '%-4s' % x, week = week + 1 if week == 7 : week = 0 print raw_input() ","date":"2015-07-06","objectID":"/posts/python-calander/:0:0","tags":null,"title":"Python命令行日历 获取时间 格式化输出","uri":"/posts/python-calander/"},{"categories":null,"content":"0X00 Linux文件目录结构 我们在Windows环境下的文件目录结构大概是几个盘符，C盘D盘E盘…但是在Linux下是树状图（其实Windows内部也是树状图）。Linux下一个主要目录是/根目录，其他所有目录都在根目录下，其他的磁盘或者分区也都挂载在根目录下。 挂载：比如我的/media/shawn/new_disk是一个空的目录，那我就可以把一个分区挂载到这个目录下，就相当于你挂了一个瓶子在某个树杈上，这样你在瓶子（分区）装的文件就只占用你的瓶子（分区）空间，并不会占据树上的空间。 |------------- bin 是binary的简写，保存可执行文件，此处的都二进制文件所有用户都有执行权限 |------------- boot 引导目录，负责引导启动系统。目录中的vmlinux是内核文件 |------------- dev 是Device的简写，保存设备文件。Uinux哲学中“一切皆文件”，设备被抽象成文件 |------------- home 用户的主目录，每个用户默认在此有一个自己的主目录。root用户的主目录在根目录下`/root` |------------- lib 保存库文件 |------------- media 挂载驱动器（U盘或者磁盘分区等），并不是所有发行版本都有这个文件夹 |------------- mnt 和media的功能几乎完全一致，该目录所有发行版本都有 / ------------- opt 是option的简写，用来安装各种大型软件 / ------ \\ |------------- proc 是process的简写，不是一个真实存在的目录，可以查看目录的信息判断。用来保存系统实时信息，各个进程的和内存信息 |------------- sbin 是super binary的简写，保存可执行文件，此处的文件是只有超级管理员可以使用的 |------------- selinux 保存SELinux的相关内容（SELinux是美国国安局开发的安全系统，开源并整合到了Linux内核中） |------------- sys 保存系统底层信息，硬件信息 |------------- temp 临时目录，系统会自动删除清理 |------------- usr 应用软件的默认安装位置 |------------- var 是variable的简写，用来保存经常变动的文件比如日志信息和邮件等 0X01 Proc文件介绍 cpuconf CPU的实时信息 memconf 内存的实时信息 iomem I/O信息 interrupts 中断信息 uptime 启动时间 acpi 电源信息 凌乱的数字 可以理解成每个目录都是一个进程，目录里是进程的相关信息 ","date":"2015-06-09","objectID":"/posts/linux-dir-tree/:0:0","tags":["Linux"],"title":"Linux 目录结构 理解Linux目录树","uri":"/posts/linux-dir-tree/"},{"categories":null,"content":"最近在网上看到有的说法里是没有bool类型的，不过以前在书上好像看到过相关的介绍，就特意找出来了那本书《C Primer Plus》，确定了C语言里确实存在bool类型。C语言是在C99标准中添加的bool类型。 bool类型是以英国数学家 * George Boole * 命名的，是他开发了用线性代数表示并解决逻辑问题的系统。 在C语言中我们使用 _Bool 来定义bool类型的变量 下面定义了一个_Bool类型的变量，并把(1 == 3)的计算值赋值给test #include \u003cstdio.h\u003e int main () { _Bool test; test = (1 == 3); return 0; } 下面能证明bool类型变量的特点 只有0和1两个值 只有0赋值给bool类型时，bool才为0 #include \u003cstdio.h\u003e int main () { _Bool test; int i; for (i = -10; i \u003c 10; i++) { test = i; if (test) printf (\"true\\n\"); else printf (\"false\\n\"); } return 0; } 最后我们证明一下bool类型比int类型占的内存要少 #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e int main () { int myint; _Bool mybool; int memint; int membool; memint = sizeof(myint); membool = sizeof(mybool); printf (\"int = %d\\n\", memint); printf (\"_Bool = %d\\n\", membool); return 0; } ","date":"2015-03-17","objectID":"/posts/boolean-in-c/:0:0","tags":["C"],"title":"C语言中的 Bool 类型","uri":"/posts/boolean-in-c/"}]